<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Behaviour | Open Neuroscience</title>
    <link>https://open-neuroscience.com/tags/behaviour/</link>
      <atom:link href="https://open-neuroscience.com/tags/behaviour/index.xml" rel="self" type="application/rss+xml" />
    <description>Behaviour</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>CC BY SA 4.0</copyright><lastBuildDate>Sun, 04 Oct 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://open-neuroscience.com/media/openneuroscience_logo_dark.svg</url>
      <title>Behaviour</title>
      <link>https://open-neuroscience.com/tags/behaviour/</link>
    </image>
    
    <item>
      <title>Mouse VR</title>
      <link>https://open-neuroscience.com/post/mouse_vr/</link>
      <pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/mouse_vr/</guid>
      <description>&lt;p&gt;Harvey Lab miniaturized mouse VR rig for head-fixed virtual navigation and decision-making tasks.&lt;/p&gt;
&lt;p&gt;The VR setup is comprised of several independent assemblies:&lt;/p&gt;
&lt;p&gt;The screen assembly: a laser projector projects onto a parabolic screen surrounding the mouse. This is the basis for the visual virtual reality.&lt;/p&gt;
&lt;p&gt;Ball cup assembly: an air-supported 8&amp;quot; styrofoam ball that the mouse can run on, with associated ball cup, sensors, and electronics&lt;/p&gt;
&lt;p&gt;Reward delivery system and lick sensor: lick spout, liquid reward reservoir, solenoid, and associated electronics&lt;/p&gt;
&lt;p&gt;Enclosure: A box surrounding the behavioral setup.&lt;/p&gt;
&lt;p&gt;Each of these components is independent of the others: i.e. just the screen could be used in combination with a different treadmill and reward delivery system. The electronics for the ball sensors, reward delivery, and lick detection are all mounted on the same PCB. If only one or two of these functions are needed, you do not need to populate the entire PCB.&lt;/p&gt;
&lt;p&gt;The screen assembly is designed to be small enough to be mounted within a standard 19&amp;quot; server rack, which could easily fit 3 rigs stacked vertically (or two + monitor and keyboard station).&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Noah Pettit; Matthias Minderer; Selmaan Chettih; Charlotte Arlt; Jim Bohnslav; Pavel Gorelick; Ofer Mazor; Christopher Harvey&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/HarveyLab/mouseVR&#34;&gt;https://github.com/HarveyLab/mouseVR&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Noah Pettit&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>FastTrack</title>
      <link>https://open-neuroscience.com/post/fasttrack/</link>
      <pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/fasttrack/</guid>
      <description>&lt;p&gt;FastTrack is an open-source cross-platform tracking software. Easy to install and easy to use, it can track a large variety of systems from active particles to animals, with a known or unknown number of objects. It can process movies from any quality on low-end to high-end computers.&lt;/p&gt;
&lt;p&gt;Two main features are implemented in the software:
- A fast and automatic tracking algorithm that can detect and track objects, conserving the objects&#39; identities across the video recording.
- A manual tool to review the tracking where errors can be corrected rapidly and easily to achieve 100% accuracy with a minimum of efforts.&lt;/p&gt;
&lt;p&gt;FastTrack do not require coding abilities to be used. A developer documentation is available for users who want to embed FastTrack tracking algorithm directly inside their projects.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Benjamin Gallois&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/FastTrackOrg/FastTrack&#34;&gt;https://github.com/FastTrackOrg/FastTrack&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.fasttrack.sh/UserManual/docs/assets/example_vid.webm&#34;&gt;http://www.fasttrack.sh/UserManual/docs/assets/example_vid.webm&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Benjamin Gallois&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>PiDose</title>
      <link>https://open-neuroscience.com/post/pidose/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/pidose/</guid>
      <description>&lt;p&gt;PiDose is an open-source tool for scientists performing drug administration experiments with mice. It allows for automated daily oral dosing of mice over long time periods (weeks to months) without the need for experimenter interaction and handling. To accomplish this, a small 3D-printed chamber is mounted adjacent to a regular mouse home-cage, with an opening in the cage to allow animals to freely access the chamber.&lt;/p&gt;
&lt;p&gt;The chamber is supported by a load cell, and does not contact the cage but sits directly next to the entrance opening. Prior to treatment, mice have a small RFID capsule implanted subcutaneously, and when they enter the chamber they are detected by an RFID reader. While the mouse is in the chamber, readings are taken from the load cell in order to determine the mouse&amp;rsquo;s bodyweight. At the opposite end of the chamber from the entrance, a nose-poke port accesses a spout which dispenses drops from two separate liquid reservoirs. This spout is wired to a capacitive touch sensor controller in order to detect licks, and delivers liquid drops in response to licking.&lt;/p&gt;
&lt;p&gt;Each day, an average weight is calculated for each mouse and a drug dosage is determined based on this. When a mouse licks at the spout it dispenses either regular drinking water or a drop of drug solution depending on if they have received their daily dosage or not. All components are controlled by a Python script running on a Raspberry Pi.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Cameron Woodard; Wissam Nasrallah; Bahram Samiei; Tim Murphy; Lynn Raymond&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://osf.io/rpyfm/&#34;&gt;https://osf.io/rpyfm/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Cameron Woodard&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>pyControl</title>
      <link>https://open-neuroscience.com/post/pycontrol/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/pycontrol/</guid>
      <description>&lt;p&gt;pyControl is a system of open source hardware and software for controlling behavioural experiments, built around the Micropython microcontroller.&lt;/p&gt;
&lt;p&gt;pyControl makes it easy to program complex behavioural tasks using a clean, intuitive, and flexible syntax for specifying tasks as state machines. User created task definition files, written in Python, run directly on the microcontroller, supported by pyControl framework code. This gives users the power and simplicity of Python for specifying task behaviour, while allowing advanced users low-level access to the microcontroller hardware.&lt;/p&gt;
&lt;p&gt;pyControl hardware consists of a breakout board and a set of devices such as nose-pokes, audio boards, LED drivers, rotary encoders and stepper motor controllers that are connected to the breakout board to create behavioural setups.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Thomas Akam&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://pycontrol.readthedocs.io&#34;&gt;https://pycontrol.readthedocs.io&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Thomas Akam&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>SLEAP</title>
      <link>https://open-neuroscience.com/post/sleap/</link>
      <pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/sleap/</guid>
      <description>&lt;p&gt;SLEAP (Social LEAP Estimates Animal Poses) is a multi-animal pose tracker based on deep learning. It is the successor of LEAP (Pereira et al., Nature Methods, 2019) and was designed to deal with the problem of tracking body landmarks of multiple freely interacting animals.&lt;/p&gt;
&lt;p&gt;Using deep learning, SLEAP trains neural network models from few user annotations to enable highly accurate body part localization, grouping and tracking. It supports multiple neural network architectures, including pretrained state-of-the-art models and lightweight customizable architectures. SLEAP has been used successfully to track mice, fruit flies, bees and other species of animals under a variety of experimental and imaging conditions.&lt;/p&gt;
&lt;p&gt;The software was designed to make it easy for users with no experience with deep learning through a fully featured GUI, as well as providing a rich functionality for advanced users seeking to develop a custom solution for their project. Tutorials and guides are available on our website (&lt;a href=&#34;https://sleap.ai&#34;&gt;https://sleap.ai&lt;/a&gt;) detailing steps for easy installation (Windows/Mac/Linux), labeling a new project, training on the locally or on the cloud, and tracking new data.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Talmo Pereira; Joshua Shaevitz; Mala Murthy&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://sleap.ai&#34;&gt;https://sleap.ai&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zwCf1pGnBUw&#34;&gt;https://www.youtube.com/watch?v=zwCf1pGnBUw&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Talmo Pereira&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>PiVR</title>
      <link>https://open-neuroscience.com/post/pivr/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/pivr/</guid>
      <description>&lt;p&gt;PiVR is a system that allows experimenters to immerse small animals into virtual realities. The system tracks the position of the animal and presents light stimulation according to predefined rules, thus creating a virtual landscape in which the animal can behave. By using optogenetics, we have used PiVR to present fruit fly larvae with virtual olfactory realities, adult fruit flies with a virtual gustatory reality and zebrafish larvae with a virtual light gradient.&lt;/p&gt;
&lt;p&gt;PiVR operates at high temporal resolution (70Hz) with low latencies (&amp;lt;30 milliseconds) while being affordable (&amp;lt;US$500) and easy to build (&amp;lt;6 hours). Through extensive documentation (&lt;a href=&#34;http://www.PiVR.org&#34;&gt;www.PiVR.org&lt;/a&gt;), this tool was designed to be accessible to a wide public, from high school students to professional researchers studying systems neuroscience in academia.&lt;/p&gt;
&lt;p&gt;The project is open source (BSD-3) and the documented code written in the freely available programming language Python. We hope that PiVR will be adapted by advanced users for their particular needs, for example to create closed-loop experiments involving other sensory modalities (e.g., sound/vibration) through the use of PWM controllable devices. We envision PiVR to be used as the central module when creating virtual realities for a variety of sensory modalities. This ‘PiVR module’ takes care of detecting the animal and presenting the appropriate PWM signal that is then picked up by the PWM controllable device installed by the user, for example to produce a sound whenever an animal enters a pre-defined region.&lt;/p&gt;
&lt;p&gt;In short, PiVR is a powerful and affordable experimental platform allowing experimenters to create a wide array of virtual reality experiments. Our hope is that PiVR will be adapted by several labs to democratize closed-loop experiments and, by standardizing image quality and the animal detection algorithm, increase reproducibility.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;David Tadres; Matthieu Louis&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.PiVR.org&#34;&gt;http://www.PiVR.org&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=w5tIG6B6FWo&#34;&gt;https://www.youtube.com/watch?v=w5tIG6B6FWo&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
David Tadres&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Simple Behavioral Analysis (SimBA)</title>
      <link>https://open-neuroscience.com/post/simple_behavioral_analysis_simba/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/simple_behavioral_analysis_simba/</guid>
      <description>&lt;p&gt;Several excellent computational frameworks exist that enable high-throughput and consistent tracking of freely moving unmarked animals. SimBA introduce and distribute a plug-and play pipeline that enables users to use these pose-estimation approaches in combination with behavioral annotation for the generation of supervised machine-learning behavioral predictive classifiers.&lt;/p&gt;
&lt;p&gt;SimBA was developed for the analysis of complex social behaviors, but includes the flexibility for users to generate predictive classifiers across other behavioral modalities with minimal effort and no specialized computational background.&lt;/p&gt;
&lt;p&gt;SimBA has a variety of extended functions for large scale batch video pre-processing, generating descriptive statistics from movement features, and interactive modules for user-defined regions of interest and visualizing classification probabilities and movement patterns.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Simon Nilsson: Jia Jie Chhong; Sophia Hwang; Nastacia Goodwin; Sam A Golden&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/sgoldenlab/simba&#34;&gt;https://github.com/sgoldenlab/simba&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Frq6mMcaHBc&amp;amp;list=PLi5Vwf0hhy1R6NDQJ3U28MOUJPfl2YWYl&amp;amp;index=2&amp;amp;t=0s&#34;&gt;https://www.youtube.com/watch?v=Frq6mMcaHBc&amp;amp;list=PLi5Vwf0hhy1R6NDQJ3U28MOUJPfl2YWYl&amp;amp;index=2&amp;amp;t=0s&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Simon Nilsson&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Stytra</title>
      <link>https://open-neuroscience.com/post/stytra/</link>
      <pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/stytra/</guid>
      <description>&lt;p&gt;Stytra, a flexible, open-source software package, written in Python and designed to cover all the general requirements involved in larval zebrafish behavioral experiments.&lt;/p&gt;
&lt;p&gt;It provides timed stimulus presentation, interfacing with external devices and simultaneous real-time tracking of behavioral parameters such as position, orientation, tail and eye motion in both freely-swimming and head-restrained preparations.&lt;/p&gt;
&lt;p&gt;Stytra logs all recorded quantities, metadata, and code version in standardized formats to allow full provenance tracking, from data acquisition through analysis to publication.&lt;/p&gt;
&lt;p&gt;The package is modular and expandable for different experimental protocols and setups. We also provide complete documentation with examples for extending the package to new stimuli and hardware, as well as a schema and parts list for behavioural setups.&lt;/p&gt;
&lt;p&gt;The software can be used in the context of calcium imaging experiments by interfacing with other acquisition devices.&lt;/p&gt;
&lt;p&gt;Our aims are to enable more laboratories to easily implement behavioral experiments, as well as to provide a platform for sharing stimulus protocols that permits easy reproduction of experiments and straightforward validation.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Vilim Stih; Luigi Petrucco&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/portugueslab/stytra&#34;&gt;https://github.com/portugueslab/stytra&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Anonymous&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>DeepLabCut</title>
      <link>https://open-neuroscience.com/post/deeplabcut/</link>
      <pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/deeplabcut/</guid>
      <description>&lt;p&gt;DeepLabCut™ is an efficient method for 3D markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results (i.e. you can match human labeling accuracy) with minimal training data (typically 50-200 frames). We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors.&lt;/p&gt;
&lt;p&gt;The package is open source, fast, robust, and can be used to compute 3D pose estimates. Please see the original paper and the latest work below.  This package is collaboratively developed by the Mathis Group &amp;amp; Mathis Lab at EPFL/Harvard.&lt;/p&gt;
&lt;p&gt;The code is freely available and easy to install in a few clicks with Anaconda (and pypi). Please see instructions on deeplabcut.org. We also provide a very easy to use GUI interface, and a step-by-step user guide!&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Mackenzie Mathis, Alexander Mathis &amp;amp; contributors&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://deeplabcut.org/&#34;&gt;http://deeplabcut.org/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC2HEbWpC_1v6i9RnDMy-dfA&#34;&gt;https://www.youtube.com/channel/UC2HEbWpC_1v6i9RnDMy-dfA&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Mackenzie Mathis&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Bonsai</title>
      <link>https://open-neuroscience.com/post/bonsai/</link>
      <pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/bonsai/</guid>
      <description>&lt;p&gt;Bonsai is a high-performance, easy to use, and flexible visual programming language for designing closed-loop neuroscience experiments combining physiology and behaviour data.&lt;/p&gt;
&lt;p&gt;Bonsai has allowed scientists with no previous programming experience to quickly develop their own experimental rigs and is also being increasingly used as a platform to integrate new open-source hardware and software from the experimental neuroscience community.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Gonçalo Lopes&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://bonsai-rx.org/&#34;&gt;https://bonsai-rx.org/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Gonçalo Lopes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Ethoscopes</title>
      <link>https://open-neuroscience.com/post/ethoscopes/</link>
      <pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/ethoscopes/</guid>
      <description>&lt;p&gt;Ethoscopes are machines for high-throughput analysis of behavior in Drosophila and other animals.&lt;/p&gt;
&lt;p&gt;Ethoscopes provide a software and hardware solution that is reproducible and easily scalable.&lt;/p&gt;
&lt;p&gt;They perform, in real-time, tracking and profiling of behavior by using a supervised machine learning algorithm, are able to deliver behaviorally triggered stimuli to flies in a feedback-loop mode, and are highly customizable and open source.&lt;/p&gt;
&lt;p&gt;Ethoscopes can be built easily by using 3D printing technology and rely on Raspberry Pi microcomputers and Arduino boards to provide affordable and flexible hardware.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Quentin Geissmann; Luis Garcia; Giorgio Gilestro&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://lab.gilest.ro/ethoscope&#34;&gt;http://lab.gilest.ro/ethoscope&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Giorgio Gilestro&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Bonvision</title>
      <link>https://open-neuroscience.com/post/bonvision/</link>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/bonvision/</guid>
      <description>&lt;p&gt;BonVision is an open-source closed-loop visual environment generator developed by the Saleem Lab and Solomon Lab at the UCL Institute of Behavioural Neuroscience in collaboration with NeuroGEARS.&lt;/p&gt;
&lt;p&gt;BonVision’s key features include:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Naturally closed-loop system based on reactive coding of the Bonsai framework
Handles 2D and 3D stimuli with equal ease
Visual environment generated independent of display configuration
Graphical programming language of the Bonsai framework
Can be used for Augmented Reality, Virtual Reality or 2D visual stimuli
Does not require the observer to be in a fixed position
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Bonvision&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://bonvision.github.io&#34;&gt;http://bonvision.github.io&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>DeepLabStream</title>
      <link>https://open-neuroscience.com/post/deeplabstream/</link>
      <pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/deeplabstream/</guid>
      <description>&lt;p&gt;DeepLabStream is a python based multi-purpose tool that enables the realtime tracking of animals and manipulation of experiments. Our toolbox is adapted from the previously published DeepLabCut (Mathis et al., 2018) and expands on its core capabilities. DeepLabStreams core feature is the real-time analysis using any type of camera-based video stream (incl. multiple streams). Building onto that, we designed a full experimental closed-loop toolkit. It enables running experimental protocols that are dependent on a constant stream of bodypart positions and feedback activation of several input/output devices. It&amp;rsquo;s capabilities range from simple region of interest (ROI) based triggers to headdirection or behavior dependent stimulation.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Schwarz Neurocon Lab&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/SchwarzNeuroconLab/DeepLabStream&#34;&gt;https://github.com/SchwarzNeuroconLab/DeepLabStream&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>FishCam</title>
      <link>https://open-neuroscience.com/post/fishcam/</link>
      <pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/fishcam/</guid>
      <description>&lt;p&gt;We describe the “FishCam”, a low-cost (500 USD) autonomous camera package to record videos and images underwater. The system is composed of easily accessible components and can be programmed to turn ON and OFF on customizable schedules. Its 8-megapixel camera module is capable of taking 3280 × 2464-pixel images and videos. An optional buzzer circuit inside the pressure housing allows synchronization of the video data from the FishCam with passive acoustic recorders. Ten FishCam deployments were performed along the east coast of Vancouver Island, British Columbia, Canada, from January to December 2019. Field tests demonstrate that the proposed system can record up to 212 h of video data over a period of at least 14 days. The FishCam data collected allowed us to identify fish species and observe species interactions and behaviors. The FishCam is an operational, easily-reproduced and inexpensive camera system that can help expand both the temporal and spatial coverage of underwater observations in ecological research. With its low cost and simple design, it has the potential to be integrated into educational and citizen science projects, and to facilitate learning the basics of electronics and programming.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Xavier Mouy&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2468067220300195&#34;&gt;https://www.sciencedirect.com/science/article/pii/S2468067220300195&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Deep Cinac</title>
      <link>https://open-neuroscience.com/post/deep_cinac/</link>
      <pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/deep_cinac/</guid>
      <description>&lt;p&gt;Two-photon calcium imaging is now widely used to infer neuronal dynamics from changes in fluorescence of an indicator. However, state of the art computational tools are not optimized for the reliable detection of fluorescence transients from highly synchronous neurons located in densely packed regions such as the CA1 pyramidal layer of the hippocampus during early postnatal  stages  of  development.  Indeed,the  latest  analytical  tools  often  lack  proper benchmark  measurements.  To  meet  this  challenge,  we  first  developed  a  graphical  user interface allowing for a precise manual detection of all calcium transients from imaged neurons based on the visualization of the calcium imaging movie. Then, we analyzed the movies using a convolutional neural network with an attention process and a bidirectional long-short term memory network. This method is able to reach human performance and offers a better F1 score (harmonic mean of sensitivity and precision) than CaImAn to infer neural activity in the developingCA1 without any user intervention. It also enables automatically identifying activity originating from GABAergic neurons. Overall, DeepCINAC offers a simple, fast and flexible open-source toolbox for processing a wide variety of calcium imaging datasets while providing the tools to evaluate its performance.&lt;/p&gt;
&lt;p&gt;See full text at &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/803726v2.full.pdf&#34;&gt;https://www.biorxiv.org/content/10.1101/803726v2.full.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Julien Denis&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://gitlab.com/cossartlab/deepcinac&#34;&gt;https://gitlab.com/cossartlab/deepcinac&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Head-Mounted Mesoscope</title>
      <link>https://open-neuroscience.com/post/head-mounted_mesoscope/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/head-mounted_mesoscope/</guid>
      <description>&lt;p&gt;The  advent  of  genetically encoded  calcium  indicators,  along  with  surgical  preparations  such  as thinned skulls or refractive index matched skulls, have enabled mesoscale cortical activity imaging in head-fixed mice. Such imaging studies have revealed complex patterns of coordinated activity across  the cortex during  spontaneous  behaviors,  goal-directed  behavior,  locomotion,  motor learning,and perceptual decision making. However, neural activity during unrestrained behavior significantly  differs from neural  activity in head-fixed  animals. Whole-cortex  imaging  in  freely behaving  mice  will  enable  the  study  of  neural  activity  in  a  larger,  more  complex  repertoire  of behaviors  not  possible  in  head-fixed  animals. Here  we present  the “Mesoscope,”  a  wide-field miniaturized, head-mounted fluorescence microscope compatible with transparent polymer skulls recently developed by our group. With afield of view of 8 mm x 10 mm and weighing less than 4 g, the Mesoscope can image most of the mouse dorsal cortex with resolution ranging from 39 to 56μm. Stroboscopic illumination with blue and green LEDs allows fort he measurement of both fluorescence changes  due  to calcium  activity  and  reflectance  signals to  capture hemodynamic changes. We have used the Mesoscope to successfully record mesoscale calcium activity across the dorsal cortex during sensory-evoked stimuli, open field behaviors, and social interactions.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Biosensing and Biorobotics Lab&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2020.05.25.114892v1.full.pdf&#34;&gt;https://www.biorxiv.org/content/10.1101/2020.05.25.114892v1.full.pdf&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Open Source Syringe Pump Controller</title>
      <link>https://open-neuroscience.com/post/open_source_syringe_pump_controller/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/open_source_syringe_pump_controller/</guid>
      <description>&lt;p&gt;Syringe pumps are a necessary piece of laboratory equipment that are used for fluid delivery in behavioral neuroscience laboratories. Many experiments provide rodents and primates with fluid rewards such as juice, water, or liquid sucrose. Current commercialized syringe pumps are not customizable and do not have the ability to deliver multiple volumes of fluid based on different inputs to the pump. Additionally, many syringe pumps are expensive and cannot be used in experiments with paired neurophysiological recordings due to electrical noise. We developed an open source syringe pump controller using commonly available parts. The controller adjusts the acceleration and speed of the motor to deliver three different volumes of fluid reward within one common time epoch. This syringe pump controller is cost effective and has been successfully implemented in rodent behavioral experiments with paired neurophysiological recordings in the rat frontal cortex while rats lick for different volumes of liquid sucrose rewards. Our syringe pump controller will enable new experiments to address the potential confound of temporal information in studies of reward signaling by fluid magnitude.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Laubach Lab&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/LaubachLab/OpenSourceSyringePump&#34;&gt;https://github.com/LaubachLab/OpenSourceSyringePump&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Automated Operant Conditioning</title>
      <link>https://open-neuroscience.com/post/automated_operant_conditioning/</link>
      <pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/automated_operant_conditioning/</guid>
      <description>&lt;p&gt;Operant conditioning (OC) is a classical paradigm and a standard technique used in experimental psychology in which animals learn to perform an action to achieve a reward. By using this paradigm, it is possible to extract learning curves and measure accurately reaction times (RTs). Both these measurements are proxy of cognitive capabilities and can be used to evaluate the effectiveness of therapeutic interventions in mouse models of disease. Here, we describe a fully 3D printable device that is able to perform OC on freely moving mice, while performing real-time tracking of the animal position. We successfully trained six mice, showing stereotyped learning curves that are highly reproducible across mice and reaching &amp;gt;70% of accuracy after 2 d of conditioning. Different products for OC are commercially available, though most of them do not provide customizable features and are relatively expensive. This data demonstrate that this system is a valuable alternative to available state-of-the-art commercial devices, representing a good balance between performance, cost, and versatility in its use.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Raffaele Mazziotti, Giulia Sagona, Leonardo Lupori, Virginia Martini and Tommaso Pizzorusso&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/raffaelemazziotti/oc_chamber&#34;&gt;https://github.com/raffaelemazziotti/oc_chamber&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>VocalMat</title>
      <link>https://open-neuroscience.com/post/vocalmat/</link>
      <pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/vocalmat/</guid>
      <description>&lt;p&gt;Mice emit ultrasonic vocalizations (USV) to transmit socially-relevant information. To detect and classify these USVs, here we describe the development of VocalMat. VocalMat is a software that uses image-processing and differential geometry approaches to detect USVs in audio files, eliminating the need for user-defined parameter tuning. VocalMat also uses computational vision and machine learning methods to classify USVs into distinct categories. In a dataset of &amp;gt;4,000 USVs emitted by mice, VocalMat detected more than &amp;gt;98% of the USVs and accurately classified ≈86% of USVs when considering the most likely label out of 11 different USV types. We then used Diffusion Maps and Manifold Alignment to analyze the probability distribution of USV classification among different experimental groups, providing a robust method to quantify and qualify the vocal repertoire of mice. Thus, VocalMat allows accurate and highly quantitative analysis of USVs, opening the opportunity for detailed and high-throughput analysis of this behavior.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Antonio H. O. Fonseca, Gustavo M. Santana, Sergio Bampi, Marcelo O Dietrich&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.dietrich-lab.org/vocalmat&#34;&gt;https://www.dietrich-lab.org/vocalmat&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Autoreward 2</title>
      <link>https://open-neuroscience.com/post/autoreward2/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/autoreward2/</guid>
      <description>&lt;p&gt;The &lt;strong&gt;motivation&lt;/strong&gt; to start this project arises when we started to include a new behavioral paradigm in the lab, an alternation T-mace with return arms (like the one in Wood e_t al._ 2000). We wanted a clean performance, as well as a clean video record, so we consider necessary to interfere neither with the animal attention (mice, how they are!) nor the camera’s field of view. I decided then to give a try to the new hobby I was getting into, “Do-It-Yourself” (DIY) stuff.&lt;/p&gt;
&lt;p&gt;In my head, it was pictured very simple. At the end of the day, I just needed a) something to detect the animal passing by, b) something to deliver a drop of water and c) something to make it happen in a coordinated way. And that’s what Autoreward2 is, no more, no less.&lt;/p&gt;
&lt;p&gt;Well perhaps it is a bit more. &lt;strong&gt;So far, the project can&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Detect&lt;/strong&gt; when the animal reaches the end of any of the two arms.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deliver&lt;/strong&gt; a small drop of fluid through the corresponding licking port (easy to make it happen in the opposite, if wanted).&lt;/li&gt;
&lt;li&gt;Give visual cues to the experimenter, indicating which arm has been reached.&lt;/li&gt;
&lt;li&gt;Allow to &lt;strong&gt;select&lt;/strong&gt; different modes of working for different working protocols: ‘Waiting for selection’, ‘Habituation’, ‘Training’, ‘Experimental’ and “Filling and cleaning” modes (and is ready to include more!).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To achieve it, I decided for very &lt;strong&gt;simple approach&lt;/strong&gt;. A couple of cheap infrared emitters are continuously read by an UNO R3 board. Breaking any of the beams triggers the signal to open the corresponding solenoid valve, connected to the fluid tank. That lets the liquid flow by gravity for around 75 milliseconds, resulting in a single drop at the tip of the licking port.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;img src=&#34;./featured2.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;There is a delay after each detection, to avoid repetitive delivery if animals don’t leave the area. A couple LEDs mounted in the bare-board (out of animal sight) light up when the process is triggered, one for each side. They also work as indicators for the ‘Waiting for selection’ mode, when they are continuously on, meanwhile no option is choose or the ‘return to waiting mode action’ is pressed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The selection&lt;/strong&gt; is made through a 4×4 membrane keypad. Right now, only options 1 to 4 are programmed, making up to 12 more programs available! When any section is made, the in-built LED blinks the corresponding times and the system is ready to work. At any moment, pressing any key makes the system reset to the waiting mode. As easy as that.&lt;/p&gt;
&lt;p&gt;Everything is &lt;strong&gt;powered&lt;/strong&gt; by a regular 9V wall adapter, giving 3.3V to the LEDs and Infrared detectors, and 9V to the solenoids. Of course, it is possible to use a 9V batterie to power it. To avoid damage coming from the solenoid discharges, the circuit is protected by a couple of diodes at this level.&lt;/p&gt;
&lt;p&gt;And that’s all, &lt;strong&gt;it’s simple&lt;/strong&gt;. The most important thing: it &lt;strong&gt;works&lt;/strong&gt;. The other most important thing: it costs around &lt;strong&gt;80€&lt;/strong&gt;. Here is the to-buy list (or equivalent):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Elegoo UNO R3 (I found them for &lt;strong&gt;10€&lt;/strong&gt;, with USB cable)&lt;/li&gt;
&lt;li&gt;BreadBoard + Acrylic base (&lt;strong&gt;7€&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;9V 1A Wall power supply (&lt;strong&gt;9€&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;2x InfraRed beams, 5mm (&lt;strong&gt;15€&lt;/strong&gt; both, the 3mm ones are even cheaper)&lt;/li&gt;
&lt;li&gt;2x Mini-Solenoid valves (&lt;strong&gt;10€&lt;/strong&gt; both)&lt;/li&gt;
&lt;li&gt;2x red LEDs&lt;/li&gt;
&lt;li&gt;4x 1 KΩ resistors&lt;/li&gt;
&lt;li&gt;2x TIP120 Transistors&lt;/li&gt;
&lt;li&gt;2x 1N4001 diodes&lt;/li&gt;
&lt;li&gt;Wiring (set of jumpers for less than &lt;strong&gt;10€&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;‘Velcro’ to attach the acrylic base where the boards are mounted.&lt;/li&gt;
&lt;li&gt;Plastic tubing and laboratory sample tubes, modified with turning siringe tips to attach/deattach the tubing easily.&lt;/li&gt;
&lt;li&gt;2x or 4x weak magnets to fix the tubes to the walls.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Feel free to access the &lt;a href=&#34;https://github.com/jjballesteros/Arduino-AutoReward&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt; page or the &lt;a href=&#34;http://forum.arduino.cc/index.php?topic=476643.0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arduino forum post&lt;/a&gt; to obtain the &lt;strong&gt;code&lt;/strong&gt;, check for the circuit &lt;strong&gt;sketch&lt;/strong&gt;, and see some &lt;strong&gt;pictures&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;PD: If someone is scandalized by the code, I am getting better on it, it is not my main strength. Please, improve it! Of course, I have in mind many possible upgrades such as a screen, a SD card port, to change the Keypad for a wireless interface (tactile?) … Did someone say smartphone plus Bluetooth? Going fancy, a barcode reader to easily introduce subjects’ data… And here is where I relay in the open-access idea, I offer it and hopefully someone implement any of the ideas. If so, remember to share!&lt;/p&gt;
&lt;p&gt;Jesús J. Ballesteros&lt;/p&gt;
&lt;p&gt;Contact me:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/jjballesterosc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Twitter&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.researchgate.net/profile/J_J_Ballesteros&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ResearchGate&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Skinner Box with RPi&#43;Python</title>
      <link>https://open-neuroscience.com/post/skinnerbox_rpi_python/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/skinnerbox_rpi_python/</guid>
      <description>&lt;p&gt;This project was developed by &lt;a href=&#34;http://www.kscottz.com/about/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Katherine Scott&lt;/a&gt; to be presented at the PyCon 2014. She developed a skinner box for her pet rats using a raspberry pi and some 3D printed parts. The setup contain a food dispenser, a buzzer, levers, a camera to observe the animals and it is hooked in a way that everything can be controlled over the internet!&lt;/p&gt;
&lt;p&gt;You can find the files for 3D parts &lt;a href=&#34;http://www.thingiverse.com/thing:296335&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and a better description of the project &lt;a href=&#34;http://www.kscottz.com/open-skinner-box-pycon-2014/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;iframe width=&#34;790&#34; height=&#34;444&#34; src=&#34;https://www.youtube.com/embed/grMfIoDgn9M&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt; &lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
