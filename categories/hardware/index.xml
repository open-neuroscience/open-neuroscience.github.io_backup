<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hardware | Open Neuroscience</title>
    <link>https://open-neuroscience.com/categories/hardware/</link>
      <atom:link href="https://open-neuroscience.com/categories/hardware/index.xml" rel="self" type="application/rss+xml" />
    <description>Hardware</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>CC BY SA 4.0</copyright><lastBuildDate>Wed, 21 Oct 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://open-neuroscience.com/media/openneuroscience_logo_dark.svg</url>
      <title>Hardware</title>
      <link>https://open-neuroscience.com/categories/hardware/</link>
    </image>
    
    <item>
      <title>EmotiBit</title>
      <link>https://open-neuroscience.com/post/emotibit/</link>
      <pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/emotibit/</guid>
      <description>&lt;p&gt;EmotiBit is a wearable sensor to capture high-quality emotional, physiological, and movement data from just about anywhere on the body.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Sean Montgomery&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.emotibit.com/&#34;&gt;https://www.emotibit.com/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=jbcL2jzyWj4&amp;amp;ab_channel=EmotiBit&#34;&gt;https://www.youtube.com/watch?v=jbcL2jzyWj4&amp;amp;ab_channel=EmotiBit&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Sean Montgomery&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Mouse VR</title>
      <link>https://open-neuroscience.com/post/mouse_vr/</link>
      <pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/mouse_vr/</guid>
      <description>&lt;p&gt;Harvey Lab miniaturized mouse VR rig for head-fixed virtual navigation and decision-making tasks.&lt;/p&gt;
&lt;p&gt;The VR setup is comprised of several independent assemblies:&lt;/p&gt;
&lt;p&gt;The screen assembly: a laser projector projects onto a parabolic screen surrounding the mouse. This is the basis for the visual virtual reality.&lt;/p&gt;
&lt;p&gt;Ball cup assembly: an air-supported 8&amp;quot; styrofoam ball that the mouse can run on, with associated ball cup, sensors, and electronics&lt;/p&gt;
&lt;p&gt;Reward delivery system and lick sensor: lick spout, liquid reward reservoir, solenoid, and associated electronics&lt;/p&gt;
&lt;p&gt;Enclosure: A box surrounding the behavioral setup.&lt;/p&gt;
&lt;p&gt;Each of these components is independent of the others: i.e. just the screen could be used in combination with a different treadmill and reward delivery system. The electronics for the ball sensors, reward delivery, and lick detection are all mounted on the same PCB. If only one or two of these functions are needed, you do not need to populate the entire PCB.&lt;/p&gt;
&lt;p&gt;The screen assembly is designed to be small enough to be mounted within a standard 19&amp;quot; server rack, which could easily fit 3 rigs stacked vertically (or two + monitor and keyboard station).&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Noah Pettit; Matthias Minderer; Selmaan Chettih; Charlotte Arlt; Jim Bohnslav; Pavel Gorelick; Ofer Mazor; Christopher Harvey&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/HarveyLab/mouseVR&#34;&gt;https://github.com/HarveyLab/mouseVR&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Noah Pettit&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>culture_shock</title>
      <link>https://open-neuroscience.com/post/culture_shock/</link>
      <pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/culture_shock/</guid>
      <description>&lt;p&gt;Culture Shock is an open-source electroporator that was developed
through internet based collaboration, starting on the DIYbio Google
Group. It is an evolution on the traditional capacitive discharge
circuit topology, instead using pulsed induction to enable a
programmable waveform as well as reduce the size, weight, and cost of
the equipment. With all these benefits, we hope to reduce the burden
of laboratory consumables for DNA transformation and electrofusion
procedures, where chemical supplies are currently relied on. The added
benefit of programmability allows many cell types to be manipulated by
altering the voltage level, or even giving the voltage profile a
particular shape.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;John Griessen; Nathan McCorkle; Bryan Bishop&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kanzure/culture_shock&#34;&gt;https://github.com/kanzure/culture_shock&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Nathan McCorkle&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>PiDose</title>
      <link>https://open-neuroscience.com/post/pidose/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/pidose/</guid>
      <description>&lt;p&gt;PiDose is an open-source tool for scientists performing drug administration experiments with mice. It allows for automated daily oral dosing of mice over long time periods (weeks to months) without the need for experimenter interaction and handling. To accomplish this, a small 3D-printed chamber is mounted adjacent to a regular mouse home-cage, with an opening in the cage to allow animals to freely access the chamber.&lt;/p&gt;
&lt;p&gt;The chamber is supported by a load cell, and does not contact the cage but sits directly next to the entrance opening. Prior to treatment, mice have a small RFID capsule implanted subcutaneously, and when they enter the chamber they are detected by an RFID reader. While the mouse is in the chamber, readings are taken from the load cell in order to determine the mouse&amp;rsquo;s bodyweight. At the opposite end of the chamber from the entrance, a nose-poke port accesses a spout which dispenses drops from two separate liquid reservoirs. This spout is wired to a capacitive touch sensor controller in order to detect licks, and delivers liquid drops in response to licking.&lt;/p&gt;
&lt;p&gt;Each day, an average weight is calculated for each mouse and a drug dosage is determined based on this. When a mouse licks at the spout it dispenses either regular drinking water or a drop of drug solution depending on if they have received their daily dosage or not. All components are controlled by a Python script running on a Raspberry Pi.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Cameron Woodard; Wissam Nasrallah; Bahram Samiei; Tim Murphy; Lynn Raymond&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://osf.io/rpyfm/&#34;&gt;https://osf.io/rpyfm/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Cameron Woodard&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>pyControl</title>
      <link>https://open-neuroscience.com/post/pycontrol/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/pycontrol/</guid>
      <description>&lt;p&gt;pyControl is a system of open source hardware and software for controlling behavioural experiments, built around the Micropython microcontroller.&lt;/p&gt;
&lt;p&gt;pyControl makes it easy to program complex behavioural tasks using a clean, intuitive, and flexible syntax for specifying tasks as state machines. User created task definition files, written in Python, run directly on the microcontroller, supported by pyControl framework code. This gives users the power and simplicity of Python for specifying task behaviour, while allowing advanced users low-level access to the microcontroller hardware.&lt;/p&gt;
&lt;p&gt;pyControl hardware consists of a breakout board and a set of devices such as nose-pokes, audio boards, LED drivers, rotary encoders and stepper motor controllers that are connected to the breakout board to create behavioural setups.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Thomas Akam&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://pycontrol.readthedocs.io&#34;&gt;https://pycontrol.readthedocs.io&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Thomas Akam&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>pyPhotometry</title>
      <link>https://open-neuroscience.com/post/pyphotometry/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/pyphotometry/</guid>
      <description>&lt;p&gt;pyPhotometry is system of open source, Python based, hardware and software for neuroscience fiber photometry data acquisition, consisting of an acquisition board and graphical user interface.&lt;/p&gt;
&lt;p&gt;pyPhotometry supports data aquisition from two analog and two digital inputs, and control of two LEDs via built in LED drivers with an adjustable 0-100mA output. The system supports time-division multiplexed illumination which allows fluoresence evoked by different excitation wavelengths to be independenly readout from a single photoreciever signal.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Thomas Akam&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://pyphotometry.readthedocs.io&#34;&gt;https://pyphotometry.readthedocs.io&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Thomas Akam&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>PiVR</title>
      <link>https://open-neuroscience.com/post/pivr/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/pivr/</guid>
      <description>&lt;p&gt;PiVR is a system that allows experimenters to immerse small animals into virtual realities. The system tracks the position of the animal and presents light stimulation according to predefined rules, thus creating a virtual landscape in which the animal can behave. By using optogenetics, we have used PiVR to present fruit fly larvae with virtual olfactory realities, adult fruit flies with a virtual gustatory reality and zebrafish larvae with a virtual light gradient.&lt;/p&gt;
&lt;p&gt;PiVR operates at high temporal resolution (70Hz) with low latencies (&amp;lt;30 milliseconds) while being affordable (&amp;lt;US$500) and easy to build (&amp;lt;6 hours). Through extensive documentation (&lt;a href=&#34;http://www.PiVR.org&#34;&gt;www.PiVR.org&lt;/a&gt;), this tool was designed to be accessible to a wide public, from high school students to professional researchers studying systems neuroscience in academia.&lt;/p&gt;
&lt;p&gt;The project is open source (BSD-3) and the documented code written in the freely available programming language Python. We hope that PiVR will be adapted by advanced users for their particular needs, for example to create closed-loop experiments involving other sensory modalities (e.g., sound/vibration) through the use of PWM controllable devices. We envision PiVR to be used as the central module when creating virtual realities for a variety of sensory modalities. This ‘PiVR module’ takes care of detecting the animal and presenting the appropriate PWM signal that is then picked up by the PWM controllable device installed by the user, for example to produce a sound whenever an animal enters a pre-defined region.&lt;/p&gt;
&lt;p&gt;In short, PiVR is a powerful and affordable experimental platform allowing experimenters to create a wide array of virtual reality experiments. Our hope is that PiVR will be adapted by several labs to democratize closed-loop experiments and, by standardizing image quality and the animal detection algorithm, increase reproducibility.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;David Tadres; Matthieu Louis&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.PiVR.org&#34;&gt;http://www.PiVR.org&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=w5tIG6B6FWo&#34;&gt;https://www.youtube.com/watch?v=w5tIG6B6FWo&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
David Tadres&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>OpenDrop Digital Microfluidics Platform</title>
      <link>https://open-neuroscience.com/post/opendrop_digital_microfluidics_platform/</link>
      <pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/opendrop_digital_microfluidics_platform/</guid>
      <description>&lt;p&gt;OpenDrop a modular, open source digital microfludics platform for research purposes. The device uses recent electro-wetting technology to control small droplets of liquids. Potential applications are lab on a chip devices for automating processes of digital biology.&lt;/p&gt;
&lt;p&gt;The OpenDrop V4 is modular electrowetting controller. The driver board is equipped with a connector that can host a circuit board cartridge with a 14×8 electrode array and 4 reservoirs. The liquids stay on a thin, hydrophobic foil laminated to the circuit board . The device is powered from USB though an included USB-C cable. All the voltage level are generated on the device and can be set with the built in soft menu from 150-300 Volts, DC or AC.&lt;/p&gt;
&lt;p&gt;OpenDrop Cartridges
The modular concepts of the OpenDrop V4 allows different configurations of cartridges: A gold coated electrode array board that can be coated with any dielectric layer and hydrophobic coating to make cartridges for topless digital microfluidic applications using readily available materials.The OpenDrop V4 Cartridge is a close-cell cartridge capable of “move, mix, split and reservoir dispensing”. The 4×8 electrode array and 4 reservoirs are laminated with a 15um ETFE foil, hydrophobic coating and ITO top cover.&lt;/p&gt;
&lt;p&gt;Programming
The OpenDrop V4 can be operated standalone and droplets can be moved through the built in joystick. A control software to program sequences of patterns from a computer is available as a free download. The board is also compatible with Adafruit Feather M0 controller boards and can be reprogrammed through the free Arduino IDE for custom specific applications.  A sample code with the instruction to activate electrodes can be found on the OpenDrop GitHub.&lt;/p&gt;
&lt;p&gt;Features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Modular Cartridge System
- Connector to connect electrode board with up to 128 channels
- Gold coated 14×8 electrodes array, 2.75 mm x 2.75 mm in size, 4mil gaps&lt;/li&gt;
&lt;li&gt;Reservoirs – the new electrode array features 4 CT-type reservoirs&lt;/li&gt;
&lt;li&gt;AC and DC voltage generated on the device form USB power. True AC voltage driving capability (up to 300VAC).&lt;/li&gt;
&lt;li&gt;32bit AVR SAMD21G18 microprocessor with plenty of memory and power
- Electronic settings for voltage level, frequency and AC/DC selection
- Electronic reading of actual voltage level
- One connector for communication and powering (USB-C)
- Optical isolation of the high-voltage electronics trough opto-couplers and PhotoMOS
- New polyphonic audio amplifier and speaker (it’s a synth!)
- Cartridge presence detection
- Feedback amplifier
- Super flat OLED Display
- Nice joystick and 2 buttons, 3 LEDs
- Reset button
- All files open source, designed on KiCAD&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;MSc Urs Gaudenz&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.gaudi.ch/OpenDrop/&#34;&gt;http://www.gaudi.ch/OpenDrop/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=TY97QfWY6J4&#34;&gt;https://www.youtube.com/watch?v=TY97QfWY6J4&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Anonymous&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Stytra</title>
      <link>https://open-neuroscience.com/post/stytra/</link>
      <pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/stytra/</guid>
      <description>&lt;p&gt;Stytra, a flexible, open-source software package, written in Python and designed to cover all the general requirements involved in larval zebrafish behavioral experiments.&lt;/p&gt;
&lt;p&gt;It provides timed stimulus presentation, interfacing with external devices and simultaneous real-time tracking of behavioral parameters such as position, orientation, tail and eye motion in both freely-swimming and head-restrained preparations.&lt;/p&gt;
&lt;p&gt;Stytra logs all recorded quantities, metadata, and code version in standardized formats to allow full provenance tracking, from data acquisition through analysis to publication.&lt;/p&gt;
&lt;p&gt;The package is modular and expandable for different experimental protocols and setups. We also provide complete documentation with examples for extending the package to new stimuli and hardware, as well as a schema and parts list for behavioural setups.&lt;/p&gt;
&lt;p&gt;The software can be used in the context of calcium imaging experiments by interfacing with other acquisition devices.&lt;/p&gt;
&lt;p&gt;Our aims are to enable more laboratories to easily implement behavioral experiments, as well as to provide a platform for sharing stimulus protocols that permits easy reproduction of experiments and straightforward validation.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Vilim Stih; Luigi Petrucco&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/portugueslab/stytra&#34;&gt;https://github.com/portugueslab/stytra&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Anonymous&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>An Open-source Anthropomorphic Robot Hand System: HRI Hand</title>
      <link>https://open-neuroscience.com/post/an_open-source_anthropomorphic_robot_hand_system_hri_hand/</link>
      <pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/an_open-source_anthropomorphic_robot_hand_system_hri_hand/</guid>
      <description>&lt;p&gt;We present an open-source anthropomorphic robot hand system called HRI hand. Our robot hand system was developed with a focus on the end-effector role of the collaborative robot manipulator. HRI hand is a research platform that can be built at a lower price (approximately $500, using only 3D printing) than commercial end-effectors. Moreover, it was designed as a two four-bar linkage for the under-actuated mechanism and provides pre-shaping motion similar to the human hand prior to touching an object. A URDF, python node, and rviz package is also provided to support the Robot Operating System (ROS). All hardware CAD design files and software source codes have been released and can be easily assembled and modified. The system proposed in this paper is developed with a five-finger structure, but each finger is modularized, so it can be developed with end-effectors of various shapes depending on the shape of the palm.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Hyeonjun Park; Donghan Kim&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/MrLacuqer/HRI-hand-firmware.git&#34;&gt;https://github.com/MrLacuqer/HRI-hand-firmware.git&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/c5Ry3tl9FVw&#34;&gt;https://youtu.be/c5Ry3tl9FVw&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Andre M Chagas&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Open Source Automated Western Blot Processor</title>
      <link>https://open-neuroscience.com/post/open_source_automated_western_blot_processor/</link>
      <pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/open_source_automated_western_blot_processor/</guid>
      <description>&lt;p&gt;Researchers in the biomedical area are always involved in methodologies comprising several processes that are repetitive and time-consuming; these researchers can take advantage of this time for other more important things.&lt;/p&gt;
&lt;p&gt;For many years, the trend for this type of problem has been automation. One of the routine methodologies used by researchers in broad areas of basic investigation is the Western blot technique.&lt;/p&gt;
&lt;p&gt;This method allows the detection through specific antibodies and the eventual quantification of the protein of interest in different biological lysates transferred onto a suitable membrane. This methodology involves several repetitive processes; one of them is the washing of blots after incubations with primary and secondary antibodies.&lt;/p&gt;
&lt;p&gt;The present device has been designed to automate this process at a low cost. Researchers must use several tools to carry out the same task at a much higher price, and more importantly, in a time-consuming process. Although it is designed for the Western blot, it can be optimized for other cyclic tasks.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Jorge Bravo-Martinez&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://dx.doi.org/10.17632/xcvckyc9mh.1&#34;&gt;http://dx.doi.org/10.17632/xcvckyc9mh.1&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Jorge Bravo-Martinez&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Small cost efficient 3D printed peristaltic pumps</title>
      <link>https://open-neuroscience.com/post/small_cost_efficient_3d_printed_peristaltic_pumps/</link>
      <pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/small_cost_efficient_3d_printed_peristaltic_pumps/</guid>
      <description>&lt;p&gt;The project overall aim is to provide cost efficient solution to drive microfluidics systems for e.g. cell culture and organ on a chip applications. Pumps, valves and other accessories are ofter expensive to buy or very expensive to custom made. The 8-channel FAST pump is a 3D printed pump that uses some off the shelf parts (steel pins and ball bearings) and is easily fabricated and assembled. A step by step protocol is published (&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2468067220300249)&#34;&gt;https://www.sciencedirect.com/science/article/pii/S2468067220300249)&lt;/a&gt;. The link to the picture is from this publication. The pump is far cheaper and smaller  than commercial pumps and still has at least as good as or better pump performance. The 8-channel pump is excellent for use in parallel cell culture applications.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Alexander Jönsson; Arianna Toppi; Martin Dufva&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2468067220300249&#34;&gt;https://www.sciencedirect.com/science/article/pii/S2468067220300249&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Martin Dufva&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>A Cartesian Coordinate Robot for Dispensing Fruit Fly Food</title>
      <link>https://open-neuroscience.com/post/a_cartesian_coordinate_robot_for_dispensing_fruit_fly_food/</link>
      <pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/a_cartesian_coordinate_robot_for_dispensing_fruit_fly_food/</guid>
      <description>&lt;p&gt;The fruit fly, Drosophila melanogaster, continues to be one of the most widely used model organisms in biomedical research.&lt;/p&gt;
&lt;p&gt;Though chosen for its ease of husbandry, maintaining large numbers of stocks of fruit flies, as done by many laboratories, is labour-intensive.&lt;/p&gt;
&lt;p&gt;One task which lends itself to automation is the production of the vials of food in which the flies are reared. Fly facilities typically have to generate several thousand vials of fly food each week to sustain their fly stocks.&lt;/p&gt;
&lt;p&gt;The system presented here combines a cartesian coordinate robot with a peristaltic pump. The design of the robot is based on an open hardware CNC (computer numerical control) machine, and uses belt and pulley actuators for the X and Y axes, and a leadscrew actuator for the Z axis.&lt;/p&gt;
&lt;p&gt;CNC motion and operation of the peristaltic pump are controlled by grbl (&lt;a href=&#34;https://github.com/gnea/grbl),&#34;&gt;https://github.com/gnea/grbl),&lt;/a&gt; an open source, embedded, G-code parser. Grbl is written in optimized C and runs directly on an Arduino. A Raspberry Pi is used to generate and stream G-code instructions to Grbl.&lt;/p&gt;
&lt;p&gt;A touch screen on the Raspberry Pi provides a graphical user interface to the system. Whilst the robot was built for the express purpose of filling vials of fly food, it could potentially be used for other liquid handling tasks in the laboratory.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Matt Wayland; Matthias Landgraf&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/WaylandM/fly-food-robot&#34;&gt;https://github.com/WaylandM/fly-food-robot&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.6084/m9.figshare.5175223.v1&#34;&gt;https://doi.org/10.6084/m9.figshare.5175223.v1&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matt Wayland&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Ethoscopes</title>
      <link>https://open-neuroscience.com/post/ethoscopes/</link>
      <pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/ethoscopes/</guid>
      <description>&lt;p&gt;Ethoscopes are machines for high-throughput analysis of behavior in Drosophila and other animals.&lt;/p&gt;
&lt;p&gt;Ethoscopes provide a software and hardware solution that is reproducible and easily scalable.&lt;/p&gt;
&lt;p&gt;They perform, in real-time, tracking and profiling of behavior by using a supervised machine learning algorithm, are able to deliver behaviorally triggered stimuli to flies in a feedback-loop mode, and are highly customizable and open source.&lt;/p&gt;
&lt;p&gt;Ethoscopes can be built easily by using 3D printing technology and rely on Raspberry Pi microcomputers and Arduino boards to provide affordable and flexible hardware.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Quentin Geissmann; Luis Garcia; Giorgio Gilestro&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://lab.gilest.ro/ethoscope&#34;&gt;http://lab.gilest.ro/ethoscope&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Giorgio Gilestro&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Poseidon</title>
      <link>https://open-neuroscience.com/post/poseidon/</link>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/poseidon/</guid>
      <description>&lt;p&gt;The Poseidon is an open-source syringe pump and microscope system. It uses 3D printed parts and common components that can be easily purchased. It can be used in microfluidics experiments or other applications. You can assemble it in a short-time for under $400. The system is modular and highly customizable. Examples of applications are: control the chemical environment of a bioreactor, purify proteins and precisely add reagents to chemical reactions over time.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Pachter Lab&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://pachterlab.github.io/poseidon&#34;&gt;https://pachterlab.github.io/poseidon&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Miguel Fernandes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>LED Zappelin&#39;</title>
      <link>https://open-neuroscience.com/post/led_zappelin/</link>
      <pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/led_zappelin/</guid>
      <description>&lt;p&gt;Two-photon (2P) microscopy is a cornerstone technique in neuroscience research. However, combining 2P imaging with spectrally arbitrary light stimulation can be challenging due to crosstalk between stimulation light and fluorescence detection. To overcome this limitation, we present a simple and low-cost electronic solution based on an ESP32 microcontroller and a TLC5947 LED driver to rapidly time-interleave stimulation and detection epochs during scans. Implemented for less than $100, our design can independently drive up to 24 arbitrary spectrum LEDs to meet user requirements. We demonstrate the utility of our stimulator for colour vision experiments on the in vivo tetrachromatic zebrafish retina and for optogenetic circuit mapping in Drosophila.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Maxime Zimmermann; Andre Maia Chagas; Philipp Bartel; Sinzi Pop, Lucia Pierto Godino; Tom Baden&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/BadenLab/LED-Zappelin&#34;&gt;https://github.com/BadenLab/LED-Zappelin&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Maxime Zimmermann&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>FishCam</title>
      <link>https://open-neuroscience.com/post/fishcam/</link>
      <pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/fishcam/</guid>
      <description>&lt;p&gt;We describe the “FishCam”, a low-cost (500 USD) autonomous camera package to record videos and images underwater. The system is composed of easily accessible components and can be programmed to turn ON and OFF on customizable schedules. Its 8-megapixel camera module is capable of taking 3280 × 2464-pixel images and videos. An optional buzzer circuit inside the pressure housing allows synchronization of the video data from the FishCam with passive acoustic recorders. Ten FishCam deployments were performed along the east coast of Vancouver Island, British Columbia, Canada, from January to December 2019. Field tests demonstrate that the proposed system can record up to 212 h of video data over a period of at least 14 days. The FishCam data collected allowed us to identify fish species and observe species interactions and behaviors. The FishCam is an operational, easily-reproduced and inexpensive camera system that can help expand both the temporal and spatial coverage of underwater observations in ecological research. With its low cost and simple design, it has the potential to be integrated into educational and citizen science projects, and to facilitate learning the basics of electronics and programming.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Xavier Mouy&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2468067220300195&#34;&gt;https://www.sciencedirect.com/science/article/pii/S2468067220300195&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>OpenTrons</title>
      <link>https://open-neuroscience.com/post/opentrons/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/opentrons/</guid>
      <description>&lt;p&gt;Today, biologists spend too much time pipetting by hand. We think biologists should have robots to do pipetting for them. People doing science should be free of tedious benchwork and repetitive stress injuries. They should be able to spend their time designing experiments and analyzing data.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s why we started Opentrons.&lt;/p&gt;
&lt;p&gt;We make robots for biologists. Our mission is to provide the scientific community with a common platform to easily share protocols and reproduce each other&amp;rsquo;s results. Our robots automate experiments that would otherwise be done by hand, allowing our community to spend more time pursuing answers to some of the 21st century’s most important questions&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Opentrons&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://opentrons.com/about&#34;&gt;https://opentrons.com/about&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCvMRmXIxnHs3AutkVhuqaQg&#34;&gt;https://www.youtube.com/channel/UCvMRmXIxnHs3AutkVhuqaQg&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Fingertip laser sensor</title>
      <link>https://open-neuroscience.com/post/fingertip_laser_sensor/</link>
      <pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/fingertip_laser_sensor/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://toychest.ai.uni-bremen.de/wiki/projects:fingertip#fingertip_laser_sensor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The fingertip laser project&lt;/a&gt; makes use of the sensor used in an Avago ADNS-9500 laser mouse, to improve the capabilities of robotic hands, giving them the capability to detect distance, surface type and slippage of grasped objects. Very elegant hack of a mouse sensor!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UC2</title>
      <link>https://open-neuroscience.com/post/uc2/</link>
      <pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/uc2/</guid>
      <description>&lt;p&gt;The open-source optical toolbox UC2 [YouSeeToo] simplifies the process of building optical setups, by combining 3D-printed cubes, each holding a specific component (e.g. lens, mirror) on a magnetic square-grid baseplate. The use of widely available consumables and 3D printing, together with documentation and software, offers an extremely low-cost and accessible alternative for both education and research areas. In order to reduce the entry barrier, we provide a fully comprehensive toolbox called TheBOX. A paper describing the scientific application in detail can be found &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2020.03.02.973073v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Benedict Diederich; René Lachmann; Barbora Marsikova&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://useetoo.org&#34;&gt;https://useetoo.org&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ey4uEFEG6MY&#34;&gt;https://www.youtube.com/watch?v=ey4uEFEG6MY&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Barbora Marsikova&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Head-Mounted Mesoscope</title>
      <link>https://open-neuroscience.com/post/head-mounted_mesoscope/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/head-mounted_mesoscope/</guid>
      <description>&lt;p&gt;The  advent  of  genetically encoded  calcium  indicators,  along  with  surgical  preparations  such  as thinned skulls or refractive index matched skulls, have enabled mesoscale cortical activity imaging in head-fixed mice. Such imaging studies have revealed complex patterns of coordinated activity across  the cortex during  spontaneous  behaviors,  goal-directed  behavior,  locomotion,  motor learning,and perceptual decision making. However, neural activity during unrestrained behavior significantly  differs from neural  activity in head-fixed  animals. Whole-cortex  imaging  in  freely behaving  mice  will  enable  the  study  of  neural  activity  in  a  larger,  more  complex  repertoire  of behaviors  not  possible  in  head-fixed  animals. Here  we present  the “Mesoscope,”  a  wide-field miniaturized, head-mounted fluorescence microscope compatible with transparent polymer skulls recently developed by our group. With afield of view of 8 mm x 10 mm and weighing less than 4 g, the Mesoscope can image most of the mouse dorsal cortex with resolution ranging from 39 to 56μm. Stroboscopic illumination with blue and green LEDs allows fort he measurement of both fluorescence changes  due  to calcium  activity  and  reflectance  signals to  capture hemodynamic changes. We have used the Mesoscope to successfully record mesoscale calcium activity across the dorsal cortex during sensory-evoked stimuli, open field behaviors, and social interactions.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Biosensing and Biorobotics Lab&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2020.05.25.114892v1.full.pdf&#34;&gt;https://www.biorxiv.org/content/10.1101/2020.05.25.114892v1.full.pdf&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Open Source Eye Tracking</title>
      <link>https://open-neuroscience.com/post/open_source_eye_tracking/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/open_source_eye_tracking/</guid>
      <description>&lt;p&gt;The purpose of this project is to convey a location in 3 dimensional space to a machine, hands free and in real time.&lt;/p&gt;
&lt;p&gt;Currently it is very difficult to control machines without making the user provide input with their hands. Additionally it can be very difficult to specify a location in space without a complex input device. This system provides a novel solution to this problem by allowing the user to specify a location simply by looking at it.&lt;/p&gt;
&lt;p&gt;Normally eyetracking solutions are prohibitively expensive and not open source, limiting their use for creators to integrate them into new projects. This solution is fully open source, easy to build and will provide a huge variety of options for makers interested in using this fascinating and powerful technology.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;John Evans&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hackaday.io/project/153293-low-cost-open-source-eye-tracking&#34;&gt;https://hackaday.io/project/153293-low-cost-open-source-eye-tracking&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Open Source Syringe Pump Controller</title>
      <link>https://open-neuroscience.com/post/open_source_syringe_pump_controller/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/open_source_syringe_pump_controller/</guid>
      <description>&lt;p&gt;Syringe pumps are a necessary piece of laboratory equipment that are used for fluid delivery in behavioral neuroscience laboratories. Many experiments provide rodents and primates with fluid rewards such as juice, water, or liquid sucrose. Current commercialized syringe pumps are not customizable and do not have the ability to deliver multiple volumes of fluid based on different inputs to the pump. Additionally, many syringe pumps are expensive and cannot be used in experiments with paired neurophysiological recordings due to electrical noise. We developed an open source syringe pump controller using commonly available parts. The controller adjusts the acceleration and speed of the motor to deliver three different volumes of fluid reward within one common time epoch. This syringe pump controller is cost effective and has been successfully implemented in rodent behavioral experiments with paired neurophysiological recordings in the rat frontal cortex while rats lick for different volumes of liquid sucrose rewards. Our syringe pump controller will enable new experiments to address the potential confound of temporal information in studies of reward signaling by fluid magnitude.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Laubach Lab&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/LaubachLab/OpenSourceSyringePump&#34;&gt;https://github.com/LaubachLab/OpenSourceSyringePump&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Sample Rotator Mixer and Shaker</title>
      <link>https://open-neuroscience.com/post/sample_rotator_mixer_and_shaker/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/sample_rotator_mixer_and_shaker/</guid>
      <description>&lt;p&gt;An open-source 3-D printable laboratory sample rotator mixer is developed here in two variants that allow users to opt for the level of functionality, cost saving and associated complexity needed in their laboratories. First, a laboratory sample rotator is designed and demonstrated that can be used for tumbling as well as gentle mixing of samples in a variety of tube sizes by mixing them horizontally, vertically, or any position in between. Changing the mixing angle is fast and convenient and requires no tools. This device is battery powered and can be easily transported to operate in various locations in a lab including desktops, benches, clean hoods, chemical hoods, cold rooms, glove boxes, incubators or biological hoods. Second, an on-board Arduino-based microcontroller is incorporated that adds the functionality of a laboratory sample shaker. These devices can be customized both mechanically and functionally as the user can simply select the operation mode on the switch or alter the code to perform custom experiments. The open source laboratory sample rotator mixer can be built by non-specialists for under US$30 and adding shaking functionality can be done for under $20 more. Thus, these open source devices are technically superior to the proprietary commercial equipment available on the market while saving over 90% of the costs.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;MOST&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.appropedia.org/Open_Source_Laboratory_Sample_Rotator_Mixer_and_Shaker&#34;&gt;https://www.appropedia.org/Open_Source_Laboratory_Sample_Rotator_Mixer_and_Shaker&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Ta2ACV1oIjI&amp;amp;feature=emb_logo&#34;&gt;https://www.youtube.com/watch?v=Ta2ACV1oIjI&amp;amp;feature=emb_logo&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Automated Operant Conditioning</title>
      <link>https://open-neuroscience.com/post/automated_operant_conditioning/</link>
      <pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/automated_operant_conditioning/</guid>
      <description>&lt;p&gt;Operant conditioning (OC) is a classical paradigm and a standard technique used in experimental psychology in which animals learn to perform an action to achieve a reward. By using this paradigm, it is possible to extract learning curves and measure accurately reaction times (RTs). Both these measurements are proxy of cognitive capabilities and can be used to evaluate the effectiveness of therapeutic interventions in mouse models of disease. Here, we describe a fully 3D printable device that is able to perform OC on freely moving mice, while performing real-time tracking of the animal position. We successfully trained six mice, showing stereotyped learning curves that are highly reproducible across mice and reaching &amp;gt;70% of accuracy after 2 d of conditioning. Different products for OC are commercially available, though most of them do not provide customizable features and are relatively expensive. This data demonstrate that this system is a valuable alternative to available state-of-the-art commercial devices, representing a good balance between performance, cost, and versatility in its use.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Raffaele Mazziotti, Giulia Sagona, Leonardo Lupori, Virginia Martini and Tommaso Pizzorusso&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/raffaelemazziotti/oc_chamber&#34;&gt;https://github.com/raffaelemazziotti/oc_chamber&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Craniobot</title>
      <link>https://open-neuroscience.com/post/craniobot/</link>
      <pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/craniobot/</guid>
      <description>&lt;p&gt;The Craniobot is a cranial microsurgery platform that combines automated skull surface profiling with a computer numerical controlled (CNC) milling machine to perform a variety of cranial microsurgical procedures in mice. The Craniobot utilizes a low force contact sensor to profile the skull surface and uses this information to perform micrometer-scale precise milling operations within minutes. The procedure of removing the sub-millimeter thick mouse skull precisely without damaging the underlying brain can be technically challenging and often takes significant skill and practice. This can now be overcome using the Craniobot.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Mathew Rynes, Leila Ghanbari, Micheal Laroque, Greg Johnson, Daniel Sousa Schulman, Suhasa Kodandaramaiah&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.labmaker.org/products/craniobot&#34;&gt;https://www.labmaker.org/products/craniobot&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Labmaker</title>
      <link>https://open-neuroscience.com/post/labmaker/</link>
      <pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/labmaker/</guid>
      <description>&lt;p&gt;LabMaker is a maker and assembly service for OPEN SCIENCE instruments. OPEN SCIENCE initiatives provide part lists or &amp;ldquo;Bill Of Materials&amp;rdquo; (BOM) for openly available scientific instruments. LabMaker bridges the gap between the BOM and the ready-to-use instrument for those not wanting to build by themselves. LabMaker is based in Berlin, Germany and ships worldwide. Berlin, as a city, is not only amongst the frontrunners for the title &amp;ldquo;start-up capital of Europe&amp;rdquo;, but also home to a large diversity of companies rooted in traditional precision manufacturing.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Labmaker&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.labmaker.org/&#34;&gt;https://www.labmaker.org/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>MicroscoPy</title>
      <link>https://open-neuroscience.com/post/microscopy/</link>
      <pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/microscopy/</guid>
      <description>&lt;p&gt;An open-source, motorized, and modular microscope built using LEGO bricks, Arduino, Raspberry Pi and 3D printing. The microscope uses a Raspberry Pi mini-computer with an 8MP camera to capture images and videos. Stepper motors and the illumination are controlled using a circuit board comprising an Arduino microcontroller, six stepper motor drivers and a high-power LED driver. All functions can be controlled from a keyboard connected to the Raspberry Pi or a separate custom-built Arduino joystick connected to the mainboard. LEGO bricks are used to construct the main body of the microscope to achieve a modular and easy-to-assemble design concept.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Yuksel Temiz and IBM&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/IBM/MicroscoPy&#34;&gt;https://github.com/IBM/MicroscoPy&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=PBSYnk9T4o4&amp;amp;feature=youtu.be&#34;&gt;https://www.youtube.com/watch?v=PBSYnk9T4o4&amp;amp;feature=youtu.be&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Autoreward 2</title>
      <link>https://open-neuroscience.com/post/autoreward2/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/autoreward2/</guid>
      <description>&lt;p&gt;The &lt;strong&gt;motivation&lt;/strong&gt; to start this project arises when we started to include a new behavioral paradigm in the lab, an alternation T-mace with return arms (like the one in Wood e_t al._ 2000). We wanted a clean performance, as well as a clean video record, so we consider necessary to interfere neither with the animal attention (mice, how they are!) nor the camera’s field of view. I decided then to give a try to the new hobby I was getting into, “Do-It-Yourself” (DIY) stuff.&lt;/p&gt;
&lt;p&gt;In my head, it was pictured very simple. At the end of the day, I just needed a) something to detect the animal passing by, b) something to deliver a drop of water and c) something to make it happen in a coordinated way. And that’s what Autoreward2 is, no more, no less.&lt;/p&gt;
&lt;p&gt;Well perhaps it is a bit more. &lt;strong&gt;So far, the project can&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Detect&lt;/strong&gt; when the animal reaches the end of any of the two arms.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deliver&lt;/strong&gt; a small drop of fluid through the corresponding licking port (easy to make it happen in the opposite, if wanted).&lt;/li&gt;
&lt;li&gt;Give visual cues to the experimenter, indicating which arm has been reached.&lt;/li&gt;
&lt;li&gt;Allow to &lt;strong&gt;select&lt;/strong&gt; different modes of working for different working protocols: ‘Waiting for selection’, ‘Habituation’, ‘Training’, ‘Experimental’ and “Filling and cleaning” modes (and is ready to include more!).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To achieve it, I decided for very &lt;strong&gt;simple approach&lt;/strong&gt;. A couple of cheap infrared emitters are continuously read by an UNO R3 board. Breaking any of the beams triggers the signal to open the corresponding solenoid valve, connected to the fluid tank. That lets the liquid flow by gravity for around 75 milliseconds, resulting in a single drop at the tip of the licking port.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;img src=&#34;./featured2.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;There is a delay after each detection, to avoid repetitive delivery if animals don’t leave the area. A couple LEDs mounted in the bare-board (out of animal sight) light up when the process is triggered, one for each side. They also work as indicators for the ‘Waiting for selection’ mode, when they are continuously on, meanwhile no option is choose or the ‘return to waiting mode action’ is pressed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The selection&lt;/strong&gt; is made through a 4×4 membrane keypad. Right now, only options 1 to 4 are programmed, making up to 12 more programs available! When any section is made, the in-built LED blinks the corresponding times and the system is ready to work. At any moment, pressing any key makes the system reset to the waiting mode. As easy as that.&lt;/p&gt;
&lt;p&gt;Everything is &lt;strong&gt;powered&lt;/strong&gt; by a regular 9V wall adapter, giving 3.3V to the LEDs and Infrared detectors, and 9V to the solenoids. Of course, it is possible to use a 9V batterie to power it. To avoid damage coming from the solenoid discharges, the circuit is protected by a couple of diodes at this level.&lt;/p&gt;
&lt;p&gt;And that’s all, &lt;strong&gt;it’s simple&lt;/strong&gt;. The most important thing: it &lt;strong&gt;works&lt;/strong&gt;. The other most important thing: it costs around &lt;strong&gt;80€&lt;/strong&gt;. Here is the to-buy list (or equivalent):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Elegoo UNO R3 (I found them for &lt;strong&gt;10€&lt;/strong&gt;, with USB cable)&lt;/li&gt;
&lt;li&gt;BreadBoard + Acrylic base (&lt;strong&gt;7€&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;9V 1A Wall power supply (&lt;strong&gt;9€&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;2x InfraRed beams, 5mm (&lt;strong&gt;15€&lt;/strong&gt; both, the 3mm ones are even cheaper)&lt;/li&gt;
&lt;li&gt;2x Mini-Solenoid valves (&lt;strong&gt;10€&lt;/strong&gt; both)&lt;/li&gt;
&lt;li&gt;2x red LEDs&lt;/li&gt;
&lt;li&gt;4x 1 KΩ resistors&lt;/li&gt;
&lt;li&gt;2x TIP120 Transistors&lt;/li&gt;
&lt;li&gt;2x 1N4001 diodes&lt;/li&gt;
&lt;li&gt;Wiring (set of jumpers for less than &lt;strong&gt;10€&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;‘Velcro’ to attach the acrylic base where the boards are mounted.&lt;/li&gt;
&lt;li&gt;Plastic tubing and laboratory sample tubes, modified with turning siringe tips to attach/deattach the tubing easily.&lt;/li&gt;
&lt;li&gt;2x or 4x weak magnets to fix the tubes to the walls.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Feel free to access the &lt;a href=&#34;https://github.com/jjballesteros/Arduino-AutoReward&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt; page or the &lt;a href=&#34;http://forum.arduino.cc/index.php?topic=476643.0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arduino forum post&lt;/a&gt; to obtain the &lt;strong&gt;code&lt;/strong&gt;, check for the circuit &lt;strong&gt;sketch&lt;/strong&gt;, and see some &lt;strong&gt;pictures&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;PD: If someone is scandalized by the code, I am getting better on it, it is not my main strength. Please, improve it! Of course, I have in mind many possible upgrades such as a screen, a SD card port, to change the Keypad for a wireless interface (tactile?) … Did someone say smartphone plus Bluetooth? Going fancy, a barcode reader to easily introduce subjects’ data… And here is where I relay in the open-access idea, I offer it and hopefully someone implement any of the ideas. If so, remember to share!&lt;/p&gt;
&lt;p&gt;Jesús J. Ballesteros&lt;/p&gt;
&lt;p&gt;Contact me:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/jjballesterosc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Twitter&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.researchgate.net/profile/J_J_Ballesteros&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ResearchGate&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenFlexure</title>
      <link>https://open-neuroscience.com/post/openflexure/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/openflexure/</guid>
      <description>&lt;p&gt;OpenFlexure is a 3D printed flexure translation stage, developed by a group at the Bath University. The stage is capable of sub-micron-scale motion, with very small drift over time. Which makes it quite good, among other things, for time-lapse protocols that need to be done over days/weeks time, and under space restricted areas, such as fume hoods. A paper describing it in detail can be found &lt;a href=&#34;http://arxiv.org/abs/1509.05394&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Adding a camera and servo motors, turns the stage into an automated microscope. More details about the project can be found &lt;a href=&#34;https://openflexure.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenFuge</title>
      <link>https://open-neuroscience.com/post/openfuge/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/openfuge/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.thingiverse.com/thing:151406&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenFuge&lt;/a&gt; describes all the materials and gives step by step instructions to the assembly of a centrifuge that is able to deliver 6000 G’s of force and to rotate at 9000 RPM, while being able to hold 4 eppendorf tubes. Developed by &lt;a href=&#34;https://www.thingiverse.com/CopabX/about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CopabX&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pulse Pal</title>
      <link>https://open-neuroscience.com/post/pulse-pal/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/pulse-pal/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Pulse Pal is an open and inexpensive (~$210) alternative to pulse generators used in neurophysiology research, and is most often used to create precisely timed light trains in optogenetics assays. Pulse Pal generates four channels of configurable square pulse trains ranging in voltage from +10 to -10V using a bipolar DAC. Two digital trigger channels can be used to start and stop playback. APIs are available in C++, Python and MATLAB, and the hardware designs and firmware are fully open source.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Be sure to check the &lt;a href=&#34;http://journal.frontiersin.org/article/10.3389/fneng.2014.00043/abstract&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; about it and their &lt;a href=&#34;https://sites.google.com/site/pulsepalwiki/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wiki page.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Red Pitaya</title>
      <link>https://open-neuroscience.com/post/red-pitaya/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/red-pitaya/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.redpitaya.com/?skip_intro=yes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Red Pitaya&lt;/a&gt; is an computer+FPGA that has digital input and outputs and really fast analog inputs and outputs. It allows connection over ethernet and programming of custom routines. The system is powerful enough to have application in mostly all branches of neuroscience labs: oscilloscopes, signal generators and even a candidate for recording systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GogoFuge</title>
      <link>https://open-neuroscience.com/post/gogofuge/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/gogofuge/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://diybio.org/2012/06/12/gogofuge/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GogoFuge&lt;/a&gt; is a good example of the power of opensource designs. IT was based on the idea of the DremelFuge and altered to be a tabletop centrifuge with vortex capability. It was created by &lt;a href=&#34;fablabatschool.org/profile/KeeganCooke&#34;&gt;Keegan Cooke&lt;/a&gt;&lt;/p&gt;
&lt;iframe width=&#34;474&#34; height=&#34;360&#34; src=&#34;https://www.youtube.com/embed/Qcl04sqXqY4&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Intelligent hearing aid</title>
      <link>https://open-neuroscience.com/post/intelligent-hearing-aid/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/intelligent-hearing-aid/</guid>
      <description>&lt;p&gt;Ojoshi at instructables.com has posted a manual on how to build this arduino based &lt;a href=&#34;http://www.instructables.com/id/Intelligent-Hearing-Aid/?ALLSTEPS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hearing aid system&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;From his &lt;a href=&#34;http://www.instructables.com/id/Intelligent-Hearing-Aid/?ALLSTEPS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;instructables page&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;it has tuning functionality that allows the wearer to tune the amplification to his or her needs. It has a conversational mode which recognizes voice input and amplifies it while reducing background noise. It saves all data to memory so that the device can be quickly powered up and ready to use. This device also has a very easy user interface to keep operation quick and simple.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;If you are going to try and build this, take maximum care and do it at your own risk!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;figure style=&#34;width: 703px&#34; class=&#34;wp-caption aligncenter&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i1.wp.com/cdn.instructables.com/F5D/JQVI/HOUFWUWY/F5DJQVIHOUFWUWY.LARGE.jpg?resize=703%2C937&#34; alt=&#34;&#34; width=&#34;703&#34; height=&#34;937&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;figcaption class=&#34;wp-caption-text&#34;&gt;from: &lt;a href=&#34;http://cdn.instructables.com/F5D/JQVI/HOUFWUWY/F5DJQVIHOUFWUWY.LARGE.jpg&#34;&gt;http://cdn.instructables.com/F5D/JQVI/HOUFWUWY/F5DJQVIHOUFWUWY.LARGE.jpg&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NeuroTinker</title>
      <link>https://open-neuroscience.com/post/neurotinker/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/neurotinker/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://hackaday.io/project/3339-neurons-neurons-neurons&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeuroTinker project&lt;/a&gt; is all about hardware emulated neurons. The creators made them in a way that each hardware neuron has excitatory and inhibitory inputs and one output that can be split up to affect dowsntream neurons. They are also cheap enough so that one can build several of them and wire them together to see which properties will emerge in the system. Design files are available on the &lt;a href=&#34;https://github.com/neurotinker&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project&amp;rsquo;s GitHub organization&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Super-Releaser</title>
      <link>https://open-neuroscience.com/post/super_releaser/</link>
      <pubDate>Mon, 21 Mar 2016 10:00:12 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/super_releaser/</guid>
      <description>&lt;p&gt;Ever thought about making soft robots? The folks at &lt;a href=&#34;http://superreleaser.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Super-Releaser&lt;/a&gt; have, and they are doing very cool projects! Some for &lt;a href=&#34;http://superreleaser.com/project-profiles/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;medical applications and some for research&lt;/a&gt; purposes. Check one of their cool robots below:&lt;/p&gt;
&lt;div class=&#34;ytp-html5-clipboard&#34;&gt;
  &lt;span class=&#34;embed-youtube&#34; style=&#34;text-align:center; display: block;&#34;&gt;&lt;/span&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>5 Dollar PCR machine</title>
      <link>https://open-neuroscience.com/post/5_dollar_pcr/</link>
      <pubDate>Tue, 09 Jun 2015 09:53:14 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/5_dollar_pcr/</guid>
      <description>&lt;p&gt;The 5 dollar PCR machine is a project from &lt;a href=&#34;https://hackaday.io/dnhkng&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Ng&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;he created a very interesting design for the PCR machine. Instead of using eppendorfs, he is using teflon tubes and three different heating elements, which allows for cheaper (he has a working PCR machine for 5 dollars!) and faster DNA amplifications.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://hackaday.io/project/1864-5-dna-replicator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here you can find the project page, with nice description and instructions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Below is a video from David explaining the project:&lt;/p&gt;
&lt;iframe width=&#34;790&#34; height=&#34;481&#34; src=&#34;https://www.youtube.com/embed/S9Fq5CGj9Kg&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Open bionics</title>
      <link>https://open-neuroscience.com/post/open-bionics/</link>
      <pubDate>Sat, 31 Jan 2015 22:56:41 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/open-bionics/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.openbionics.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Open bionics project&lt;/a&gt; was inspired by the Yale open hand project, aiming to develop light, affordable, and modular robot hands and myoelectric prosthesis. Also they want to make them easy to replicate using off the shelf materials. On the video below taken from their website you can see the hands in action, either as a prosthesis, or attached to a small drone being operated remotely.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open prosthetics and robotics</title>
      <link>https://open-neuroscience.com/post/prosthetics-and-robotics/</link>
      <pubDate>Sat, 31 Jan 2015 22:08:16 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/prosthetics-and-robotics/</guid>
      <description>&lt;p&gt;With the rise of low cost 3D printers, and other cheap manufacturing tools, the field of robotics and prosthetics has been gaining quite a few open source projects. Two very nice compilations can be found at &lt;a href=&#34;//openrobothardware.org/&#34;&gt;openrobot hardware&lt;/a&gt; and at &lt;a href=&#34;http://softroboticstoolkit.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Soft robotics toolkit&lt;/a&gt;. Below are some related to neuroscience:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://openeuroscience.com/hardware-projects/open-prosthetics-and-robotics/open-hand-project/&#34; title=&#34;Open Hand Project&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The open hand project&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://openeuroscience.com/hardware-projects/open-prosthetics-and-robotics/the-yale-open-hand-project/&#34; title=&#34;The Yale open hand project&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Yale open hand project&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://openeuroscience.com/hardware-projects/open-prosthetics-and-robotics/open-bionics/&#34; title=&#34;Open bionics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Openbionics&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://openeuroscience.com/hardware-projects/open-prosthetics-and-robotics/fingertip-laser-sensor/&#34; title=&#34;Fingertip laser sensor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fingertip laser sensor&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://openeuroscience.com/hardware-projects/open-prosthetics-and-robotics/takktile/&#34; title=&#34;Takktile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;takktile&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Backyard Brains</title>
      <link>https://open-neuroscience.com/post/backyard_brains/</link>
      <pubDate>Tue, 29 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/backyard_brains/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://backyardbrains.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Backyard brains&lt;/a&gt; started out producing low cost, portable, electrophysiology systems to bring neuroscience to classrooms and help promote it.&lt;/p&gt;
&lt;p&gt;“Backyard brains wants to be for neuroscience, what the telescope is for astronomers” – meaning that the idea is that with a couple of hundred dollars anyone can get one of these recording systems and start doing experiments, like amateur astronomers can buy telescopes and start observing the cosmos.&lt;/p&gt;
&lt;iframe width=&#34;790&#34; height=&#34;444&#34; src=&#34;https://www.youtube.com/embed/-mKen7tCDCs&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>10$ smartphone microscope</title>
      <link>https://open-neuroscience.com/post/10_smartphone_microscope/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/10_smartphone_microscope/</guid>
      <description>&lt;p&gt;This &lt;a href=&#34;http://www.instructables.com/id/10-Smartphone-to-digital-microscope-conversion/%20how%20to%20use%20a%20smartphone%20for%20big%20amplifications&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;neat little project&lt;/a&gt; uses some plexi-glass, lens extracted from a laser pointer to harvest the power of smartphone cameras for some very big amplifications! Yoshinok manged to see cell plasmolysis and some other cool features with it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i2.wp.com/www.instructables.com/files/deriv/FX0/QLMO/HMMF5O43/FX0QLMOHMMF5O43.MEDIUM.jpg?w=800&#34; alt=&#34;Vegetal slice&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Attys</title>
      <link>https://open-neuroscience.com/post/attys/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/attys/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.attys.tech/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attys&lt;/a&gt; is an wearable data acquisition device with a special focus on biomedical signals such as heart activity (ECG), muscle activity (EMG) and brain activity (EEG). It’s open firmware, open API and has open source applications on github in C++ and JAVA to encourage people to create their own custom versions for mobile devices, tablets and PC.&lt;/p&gt;
&lt;p&gt;The story of the Attys started when Dr. Bernd Porr filmed numerous youTube clips to educate the public about the possibilities and limits of biosignal measurement (&lt;a href=&#34;http://biosignals.berndporr.me.uk&#34;&gt;http://biosignals.berndporr.me.uk&lt;/a&gt;) which are featured here: &lt;a href=&#34;http://openeuroscience.com/hardware-projects/human-electrophysiology/bio-signal/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BPM link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The site has been very popular ever since and visitors have been asking if a ready made bio-amp could be made available. This year Dr. Porr then decided to make one. This was the birth of the Attys.&lt;/p&gt;
&lt;p&gt;Attys is also a general educational tool to measure any physical quantity such as temperature, pressure or light intensity. It works with Google’s open source Science Journal and turns every Android phone or tablet into an electronic lab book / oscilloscope. Of course one can measure biosignals with it, too.&lt;/p&gt;
&lt;p&gt;Vasso Georgiadou has been the main presenter for our biosignal channel. Here, she shows off the Attys:&lt;/p&gt;
&lt;iframe width=&#34;790&#34; height=&#34;444&#34; src=&#34;https://www.youtube.com/embed/TG5cRvgFEDA&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>BB LED Matrix</title>
      <link>https://open-neuroscience.com/post/bb_led_matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/bb_led_matrix/</guid>
      <description>&lt;p&gt;This project uses a 32X32 LED array (1024 LEDs in total) and a beagle bone black board. &lt;a href=&#34;https://bikerglen.com/projects/lighting/led-panel-1up/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The page describing the project&lt;/a&gt; has very nice explanations on how the whole system works (and LED displays in general).&lt;/p&gt;
&lt;p&gt;From this project, the creator &lt;a href=&#34;https://twitter.com/bikerglen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Glen Akins&lt;/a&gt;, went on to construct a 3X2 matrix of 32X32 LEDS, or a total of 6144 RGB LEDs that have a 200Hz refresh rate! Check out the video below of the panel in action:&lt;/p&gt;
&lt;iframe width=&#34;790&#34; height=&#34;444&#34; src=&#34;https://www.youtube.com/embed/LBeVMGOgWvY&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Blinkenschild</title>
      <link>https://open-neuroscience.com/post/blinkeschild/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/blinkeschild/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://hackaday.io/project/363-blinkenschild&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blinkenschild&lt;/a&gt; is a portable sign consisting of 960 RGB LEDs. The images/movies to be displayed are stored in a SD card in a Teensy3 board and controlled via bluetooth. Resolution is not as high as LCD monitors but the refresh rate is much higher:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;This is done in realtime and pixelvalues are recalculated before display.

This is still too fast so i had to add 30 ms delay between the frames or we would not perceive it as a fluid animation but rather just blinking bright light.
&lt;/code&gt;&lt;/pre&gt;
&lt;iframe width=&#34;790&#34; height=&#34;593&#34; src=&#34;https://www.youtube.com/embed/VX14pmky07Q&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>BPM Biosignal</title>
      <link>https://open-neuroscience.com/post/bpm_biosignal/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/bpm_biosignal/</guid>
      <description>&lt;p&gt;BPM Biosignal is a two stage amplifier created mainly for educational purposes.&lt;/p&gt;
&lt;p&gt;Check their &lt;a href=&#34;https://www.youtube.com/c/BPMbiosignals&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YouTube Channel&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Brain Map</title>
      <link>https://open-neuroscience.com/post/brain_map/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/brain_map/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://people.ece.cornell.edu/land/courses/ece4760/FinalProjects/s2012/pmd68_mab448/pmd68_mab448/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BrainMap&lt;/a&gt; expands the accessible DIY projects for brain activity measurements.&lt;/p&gt;
&lt;p&gt;This is the conclusion project of Patrick Dear and Mark Bunney Jr. at Cornell university where they used infrared leds to measure differences in blood flow at the scalp and map the motor cortex.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DIY PCR</title>
      <link>https://open-neuroscience.com/post/diy_pcr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/diy_pcr/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://hackaday.io/hacker/24043-katherina-baranova&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Katharina&lt;/a&gt; and &lt;a href=&#34;https://hackaday.io/hacker/24028-alex-bondarekno&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alex&lt;/a&gt; are developing a classic PCR machine: 16 samples and a heated lid.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://hackaday.io/project/2548-open-source-thermal-cycler&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find more details of their project here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here is a demo video:&lt;/p&gt;
&lt;iframe width=&#34;500&#34; height=&#34;281&#34; src=&#34;https://www.youtube.com/embed/R7leQlkBKJw&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>DremelFuge</title>
      <link>https://open-neuroscience.com/post/dremelfuge/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/dremelfuge/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.thingiverse.com/thing:1483&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DremelFuge&lt;/a&gt; is a very simple and clever centrifuge, buit perhaps not the safest one (be careful if you end up using it!).&lt;/p&gt;
&lt;p&gt;It takes advantage of 3d printing technology to print an adaptor that goes on to a Dremel (a precision tool that has really high rotation rates). It was created by &lt;a href=&#34;https://www.thingiverse.com/cathalgarvey/about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cathal&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.thingiverse.com/renders/ff/74/4c/b2/c4/2009-12-30-023824_display_large_preview_featured.jpg&#34; alt=&#34;3d printed dremel attachment&#34; title=&#34;DremelFuge&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open BCI</title>
      <link>https://open-neuroscience.com/post/bio_amp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/bio_amp/</guid>
      <description>&lt;p&gt;BioAmp is a biopotential acquisition device (EEG, ECG, EMG, EOG, etc.) developed in the Prototyping Laboratory at the School of Engineering of the National University of Entre Rios (Argentina).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./bio_amp_frontal.jpg&#34; alt=&#34;Frontal view&#34;&gt;&lt;/p&gt;
&lt;p&gt;Main features:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;8 independent acquisition channels
24 bits of resolution per channel
2000 Hz is the maximum sampling frequency
USB connection (power and data transmission)
inputs for trigger signal
designed under electrical safety standards for medical use (electrical insulation, touch-proof connectors, etc.)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A very interesting feature of the BioAmp is the possibility of combining two amplifiers to double the number of recording channels. It is also possible to program each channel individually, offering the possibility of registering different types of signal simultaneously. For example, EEG, EOG, and EMG could be recorded during a sleep study, or EMG and ECG during a physical activity study, etc.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./bio_amp_back.jpg&#34; alt=&#34;Posterior view&#34;&gt;&lt;/p&gt;
&lt;p&gt;This project is currently in evolution and development, continually changes and updates are made to improve the product. Both the hardware source files (PCB and cabinet for 3D printing) and firmware are available in the project repository.&lt;/p&gt;
&lt;p&gt;For more information on this project and other projects carried out in the Prototyping Laboratory, visit the laboratory website.&lt;/p&gt;
&lt;iframe id=&#34;video-2209-1_youtube_iframe&#34; allowfullscreen=&#34;1&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; title=&#34;YouTube video player&#34; src=&#34;https://www.youtube.com/embed/F7R7IxtyfGw?controls=0&amp;amp;rel=0&amp;amp;disablekb=1&amp;amp;showinfo=0&amp;amp;modestbranding=0&amp;amp;html5=1&amp;amp;iv_load_policy=3&amp;amp;autoplay=0&amp;amp;end=0&amp;amp;loop=0&amp;amp;playsinline=0&amp;amp;start=0&amp;amp;nocookie=false&amp;amp;enablejsapi=1&amp;amp;origin=https%3A%2F%2Fopeneuroscience.com&amp;amp;widgetid=1&#34; width=&#34;829&#34; height=&#34;466.3125&#34; frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Open BCI</title>
      <link>https://open-neuroscience.com/post/open-bci/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/open-bci/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://openbci.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenBCI&lt;/a&gt; is a complete open source EEG system that can be built either on top of an Arduino (8-bit system), or on top of chipKIT (32-bit system), which gives the system more local memory and allows for faster speeds.&lt;/p&gt;
&lt;p&gt;All software code and hardware (including a model for a 3D printable headset) plans can be found freely available at their &lt;a href=&#34;https://openbci.com/index.php/downloads&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;download section&lt;/a&gt; or at &lt;a href=&#34;https://github.com/OpenBCI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open EEG</title>
      <link>https://open-neuroscience.com/post/open_eeg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/open_eeg/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://openeeg.sourceforge.net/doc/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The openEEG&lt;/a&gt; project aims at describing and putting manuals for building a two channel EEG system for about U$200.
More on instructions on how to build one, can be found &lt;a href=&#34;http://openeeg.sourceforge.net/doc/SimpleEEG/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The latest update on the page seems to be a bit old, but Olimex sells the necessary PCB boards and accessories to &lt;a href=&#34;https://www.olimex.com/Products/EEG/OpenEEG/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;build the device&lt;/a&gt;. They also sell the &lt;a href=&#34;https://www.olimex.com/Products/EEG/OpenEEG/EEG-SMT/open-source-hardware&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;openEEG completely assembled&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Ephys</title>
      <link>https://open-neuroscience.com/post/open-ephys/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/open-ephys/</guid>
      <description>&lt;p&gt;Open Ephys is a great initiative to create a suite that encompasses hardware for LFP and spiking recording, optogenetics combined with custom written software for microstimulation, environmental stimuli, extracellular recording and optogen. perturbations. Their ultimate goal is to create a system optimized for tetrodes and optogenetics where one is able to record and analyse data in real-time. On the &lt;a href=&#34;http://www.open-ephys.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project’s website&lt;/a&gt; one can download plans on how to build the devices and estimate on part cost (which is much, much lower than commercially available systems out there).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open ExG</title>
      <link>https://open-neuroscience.com/post/open_exg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/open_exg/</guid>
      <description>&lt;p&gt;OpenHardwareExG: is a project that provides both open source hardware and software for the measurement and analysis of different types of biosignals&lt;/p&gt;
&lt;p&gt;From the &lt;a href=&#34;http://openelectronicslab.github.io/OpenHardwareExG/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project page&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;About the OpenHardwareExG project
Project goals
The main goal of the project is to build a device that allows the creation of electrophysiologic signal processing applications. In addition:

    Hardware and software that we develop will have a free/open source license. We also prefer to use hardware and software that are free/open source.
    We would like to keep the hardware &amp;quot;DIY compatible&amp;quot; (hand solderable, with parts that are readily available in small quantities, etc.)
    For us, this is a hobby and learning project. It&#39;s important to keep it fun, and take the time to learn along the way.

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Open PCR</title>
      <link>https://open-neuroscience.com/post/open_pcr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/open_pcr/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://openpcr.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open PCR&lt;/a&gt; is an open source PCR machine with heated lid and space for 12 samples&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenSpritzer</title>
      <link>https://open-neuroscience.com/post/openspritzer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/openspritzer/</guid>
      <description>&lt;p&gt;A very neat picospritzer initially created by Joe (PI at &lt;a href=&#34;http://raimondolab.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Raimondo Lab&lt;/a&gt;) using basically a solenoid valve, microcontroller and a power source.&lt;/p&gt;
&lt;p&gt;Was later further developed by &lt;a href=&#34;https://chrisjforman.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chris&lt;/a&gt; at the &lt;a href=&#34;badenlab.org&#34;&gt;Baden Lab&lt;/a&gt;, and collaboratively published as &lt;a href=&#34;https://www.nature.com/articles/s41598-017-02301-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a peer reviewed article&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Details on how to build it, can be found on the &lt;a href=&#34;https://github.com/BadenLab/Openspritzer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project&amp;rsquo;s Git repository&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenStage</title>
      <link>https://open-neuroscience.com/post/openstage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/openstage/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0088977&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open stage&lt;/a&gt; is a low-cost motorised microscope stage capable of movement in the micrometer range.&lt;/p&gt;
&lt;p&gt;It features manual control via a control-pad, different movement velocities and pc communication through the serial port.&lt;/p&gt;
&lt;p&gt;The authors also state that due to its simplicity, the system could be used to drive micromanipulators and other devices&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Parallela</title>
      <link>https://open-neuroscience.com/post/parallela/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/parallela/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.parallella.org/Introduction/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parallela&lt;/a&gt;, an open source, open access card sized supercomputer, has the mission of bringing parallel computing to the masses by combining multiple RISC processors and very low power consumption. Produced by the &lt;a href=&#34;http://www.adapteva.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adapteva company&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Signal Generators</title>
      <link>https://open-neuroscience.com/post/signal-generators/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/signal-generators/</guid>
      <description>&lt;p&gt;Every lab needs a signal generator once in a while. They are useful to see if your acquisition program is working properly, to test why a certain piece of equipment is not working properly or to generate cues and targets at behavioural paradigms. Listed below are different generators, built using arduinos and other microcontrollers. They have different degrees of complexity and capabilities, so it would be wise to briefly look through them and see what fits you best!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&#34;http://www.instructables.com/id/Arduino-Waveform-Generator/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The arduino waveform generator&lt;/a&gt; is a pretty straight forward project that is able to generate four different waveforms from 1Hz to 50kHz. Gain, frequency, modulation and waveform type are controlled by nobs.&lt;/p&gt;
&lt;iframe width=&#34;800&#34; height=&#34;422&#34; src=&#34;https://www.youtube.com/embed/gz_gVKWFN8E&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&#34;http://www.instructables.com/id/Atmel-Xmega-USBSerial-Arbitrary-Waveform-Generato/?ALLSTEPS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Atmel Xmega USB/Serial Arbitrary Waveform Generator&lt;/a&gt; runs using a boston android XMEGA evaluation board and is able to deliver square, sine, triangular and arbitrary waveforms in between 5Hz and 20kHz. This one is not a stand alone system, which means that to set a new waveform type, one would have to have the board connect to a computer at all times.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i1.wp.com/www.instructables.com/files/deriv/FWF/PWX4/G79D44SM/FWFPWX4G79D44SM.LARGE.jpg?w=800&#34; alt=&#34;arbitrary waveform generator&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&#34;http://arduino.cc/en/Tutorial/DueSimpleWaveformGenerator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simple waveform generator&lt;/a&gt; seems to be the most straight forward of all projects, requiring only a potentiometer, a couple of resistors and push buttons. The trade off is that with the present sketch, waveforms of only up to 170Hz can be generated. It generates sawtooth, square, triangular and sine waveforms.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i0.wp.com/arduino.cc/en/uploads/Tutorial/DueSimpleWaveform_fritzing.png?w=800&#34; alt=&#34;arduino due waveform generator&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Skinner Box with RPi&#43;Python</title>
      <link>https://open-neuroscience.com/post/skinnerbox_rpi_python/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/skinnerbox_rpi_python/</guid>
      <description>&lt;p&gt;This project was developed by &lt;a href=&#34;http://www.kscottz.com/about/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Katherine Scott&lt;/a&gt; to be presented at the PyCon 2014. She developed a skinner box for her pet rats using a raspberry pi and some 3D printed parts. The setup contain a food dispenser, a buzzer, levers, a camera to observe the animals and it is hooked in a way that everything can be controlled over the internet!&lt;/p&gt;
&lt;p&gt;You can find the files for 3D parts &lt;a href=&#34;http://www.thingiverse.com/thing:296335&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and a better description of the project &lt;a href=&#34;http://www.kscottz.com/open-skinner-box-pycon-2014/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;iframe width=&#34;790&#34; height=&#34;444&#34; src=&#34;https://www.youtube.com/embed/grMfIoDgn9M&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt; &lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stereo microscope</title>
      <link>https://open-neuroscience.com/post/stereo_microscope/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/stereo_microscope/</guid>
      <description>&lt;p&gt;Although stereo microscopes are an essential piece of hardware in biology labs, sometimes we wish they had more features, like the possibility to record the magnified images with a camera, or have a better lighting system to enhance contrast on those small samples.&lt;/p&gt;
&lt;p&gt;One person has taken those issues to heart and tackled them all in a very brilliant way. Below you&amp;rsquo;ll find links to &lt;a href=&#34;http://www.tangentaudio.com/about/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Steve&amp;rsquo;s blog&lt;/a&gt;, where he describes, in a very detailed way, three projects to enhance the all familiar stereo microscope:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.tangentaudio.com/mechanical/microscope-camera-output/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Camera eye piece adaptor.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.tangentaudio.com/2013/03/aziz-light/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AZIZ a ring lighting system.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i1.wp.com/www.tangentaudio.com/wp-content/uploads/2013/03/DSC_6828-modified-1024x680.jpg?resize=800%2C531&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.tangentaudio.com/2013/02/epic-builds-articulated-stereo-microscope-arm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Articulated stereo microscope mount.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;embed-youtube&#34; style=&#34;text-align:center; display: block;&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Some of them are not that easy to reproduce, but maybe be a good starting point for other DIY versions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Syringe Pump</title>
      <link>https://open-neuroscience.com/post/syringe_pump/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/syringe_pump/</guid>
      <description>&lt;p&gt;From the &lt;a href=&#34;http://www.mse.mtu.edu/~pearce/Index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pearce lab&lt;/a&gt;, this syringe pump was &lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0107216&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;published in Plos One&lt;/a&gt; and is built using 3d printed parts, stepper motors and a raspberry pi, costing 5% or less than commercial available systems. Can be calibrated and customized for different applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Takktile</title>
      <link>https://open-neuroscience.com/post/takktile/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/takktile/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.takktile.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Takktile&lt;/a&gt;, is a tactile sensor to be used on robotic applications. The developers want to make it move away from the closed walls of research institutions by making it open source and cheap. It is built based on MEMs barometers and can sense 1 gram loads as well as coping with hammer blows (see video from their website below).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;embed-youtube&#34; style=&#34;text-align:center; display: block;&#34;&gt;&lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>tDCS</title>
      <link>https://open-neuroscience.com/post/tdcs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/tdcs/</guid>
      <description>&lt;p&gt;Although of simple complexity and using low currents, this tDCS machine is still to be considered a piece of equipment that could be dangerous both in the assembly and in the operation phases, so please inform yourself as best as you can before either of these steps! Also remember that the openeuroscience website cannot be held responsible for any injuries that might occur from improper use of this tool.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.instructables.com/id/Build-a-Human-Enhancement-Device-Basic-tDCS-Suppl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DIY tDCS instructables&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Yale open hand project</title>
      <link>https://open-neuroscience.com/post/the_yale_open_hand_project/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/post/the_yale_open_hand_project/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.eng.yale.edu/grablab/openhand/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Yale open hand project&lt;/a&gt;, has a similar purpose of the open hand project, that is, to make prosthetic hands more widely available through the lowering of costs. They have a different design from the open hand project. Additionally the project wants to take advantage of the lowered costs to speed up the development cycle and provide, together with input from the user community, several different useful hand designs.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
