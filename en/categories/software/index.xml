<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Software | Open Neuroscience</title>
    <link>https://open-neuroscience.com/en/categories/software/</link>
      <atom:link href="https://open-neuroscience.com/en/categories/software/index.xml" rel="self" type="application/rss+xml" />
    <description>Software</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>CC BY SA 4.0</copyright><lastBuildDate>Wed, 21 Oct 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://open-neuroscience.com/media/openneuroscience_logo_dark.svg</url>
      <title>Software</title>
      <link>https://open-neuroscience.com/en/categories/software/</link>
    </image>
    
    <item>
      <title>BigPint Bioconductor package that makes BIG (RNA-seq) data pint-sized</title>
      <link>https://open-neuroscience.com/en/post/bigpint_bioconductor_package_that_makes_big__rna_seq__data_pint_sized/</link>
      <pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/bigpint_bioconductor_package_that_makes_big__rna_seq__data_pint_sized/</guid>
      <description>&lt;p&gt;The BigPint package can help examine any large multivariate dataset. However, we note that the example datasets and example code in this package consider RNA-sequencing datasets. If you are using this software for RNA-sequencing data, then it can help you confirm that the variability between your treatment groups is larger than that between your replicates and determine how various normalization techniques in popular RNA-sequencing analysis packages (such as edgeR, DESeq2, and limma) affect your dataset. Moreover, you can easily superimpose lists of differentially expressed genes (DEGs) onto your dataset to check that they show the expected patterns (large variability between treatment groups and small variability between replicates).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;BigPint software website: &lt;a href=&#34;https://lindsayrutter.github.io/bigPint/&#34;&gt;https://lindsayrutter.github.io/bigPint/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Article explaining the BigPint methodology:
&lt;a href=&#34;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2968-1&#34;&gt;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2968-1&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Research article showcasing the BigPint software:
&lt;a href=&#34;https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-5767-1&#34;&gt;https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-5767-1&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Article explaining the BigPint software:
&lt;a href=&#34;https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007912&#34;&gt;https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007912&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Lindsay Rutter; Dianne Cook&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/lindsayrutter/bigPint&#34;&gt;https://github.com/lindsayrutter/bigPint&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Anonymous&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>EmotiBit</title>
      <link>https://open-neuroscience.com/en/post/emotibit/</link>
      <pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/emotibit/</guid>
      <description>&lt;p&gt;EmotiBit is a wearable sensor to capture high-quality emotional, physiological, and movement data from just about anywhere on the body.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Sean Montgomery&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.emotibit.com/&#34;&gt;https://www.emotibit.com/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=jbcL2jzyWj4&amp;amp;ab_channel=EmotiBit&#34;&gt;https://www.youtube.com/watch?v=jbcL2jzyWj4&amp;amp;ab_channel=EmotiBit&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Sean Montgomery&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>BrainSMASH</title>
      <link>https://open-neuroscience.com/en/post/brainsmash/</link>
      <pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/brainsmash/</guid>
      <description>&lt;p&gt;BrainSMASH is a Python-based framework for quantifying the significance of a brain map’s spatial topography in studies of large-scale brain organization. BrainSMASH was designed to generate synthetic brain maps with spatial autocorrelation (SA) matched to the SA of a target brain map.&lt;/p&gt;
&lt;p&gt;SA is a prominent and ubiquitous property of brain maps that violates the assumptions of independence/exchangeability that underlie many conventional statistical tests. Controlling for this property is therefore necessary to disambiguate meaningful topographic relationships from chance associations. To this end, BrainSMASH instantiates a generative null model for simulating surrogate brain maps, constrained by empirical data, that preserve the SA of cortical (surface-based), subcortical (volumetric), parcellated, and dense brain maps.&lt;/p&gt;
&lt;p&gt;BrainSMASH requires only two inputs: a brain map of interest, and a matrix of pairwise distances between elements of the brain map. How these inputs are derived is left to user discretion, though additional support has been provided for investigators working with HCP-compliant neuroimaging files. Specifically, BrainSMASH includes routines to generate two-dimensional Euclidean and geodesic distance matrices from surface geometry (GIFTI) files, and subcortical Euclidean distance matrices from CIFTI-format neuroimaging files.&lt;/p&gt;
&lt;p&gt;Detailed documentation for BrainSMASH can be found at &lt;a href=&#34;https://brainsmash.readthedocs.io/&#34;&gt;https://brainsmash.readthedocs.io/&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Joshua B. Burt; Markus Helmer; Maxwell Shinn; Alan Anticevic; John D. Murray&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/murraylab/brainsmash&#34;&gt;https://github.com/murraylab/brainsmash&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Joshua B. Burt&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>PsychRNN: An Accessible and Flexible Python Package for Training Recurrent Neural Network Models on Cognitive Tasks</title>
      <link>https://open-neuroscience.com/en/post/psychrnn_an_accessible_and_flexible_python_package_for_training_recurrent_neural_network_models_on_cognitive_tasks/</link>
      <pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/psychrnn_an_accessible_and_flexible_python_package_for_training_recurrent_neural_network_models_on_cognitive_tasks/</guid>
      <description>&lt;p&gt;PsychRNN is designed for neuroscientists and psychologists who are interested in RNNs as models of cognitive function in the brain.&lt;/p&gt;
&lt;p&gt;Despite growing interest in RNNs as models of brain function, this approach poses relatively high barriers to entry to researchers, due to the technical know-how required for specialized deep learning software (e.g. TensorFlow or PyTorch) to train artificial neural network models.&lt;/p&gt;
&lt;p&gt;We designed PsychRNN with accessibility and flexibility as important goals.&lt;/p&gt;
&lt;p&gt;The frontend for users to define tasks and train RNNs uses only Python &amp;amp; NumPy, with no requirement for deep learning software.&lt;/p&gt;
&lt;p&gt;The backend, based on TensorFlow for model training, is readily extensible. This design allows for accessible high-level specification and parameterization of tasks and models, using only a few lines of Python.&lt;/p&gt;
&lt;p&gt;Modularity is central to PsychRNN&amp;rsquo;s design, to achieve flexibility in defining and parameterizing tasks and networks. This facilitates investigation of how task features (e.g. timing or input/output channels) shape the network solutions learned by the models.&lt;/p&gt;
&lt;p&gt;PsychRNN also provides support for implementation of neurobiologically motivated constraints on synaptic connectivity, such as: no autapses, structured connectivity (e.g. for multi-region RNNs), Dale&amp;rsquo;s principle (separate excitatory &amp;amp; inhibitory cells), and fixed nonplastic subset of synapses. Modularity enables implementation of curriculum learning, or task shaping. RNNs can be trained in closed-loop, with tasks progressively adjusted as behavioral performance improves. This is more similar to animal training, for investigation of how shaping impacts neural solutions.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Daniel B Ehrlich; Jasmine T Stone; David Brandfonbrener; Alexander Atanasov; John D Murray&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/murraylab/PsychRNN&#34;&gt;https://github.com/murraylab/PsychRNN&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Jasmine Stone&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>FastTrack</title>
      <link>https://open-neuroscience.com/en/post/fasttrack/</link>
      <pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/fasttrack/</guid>
      <description>&lt;p&gt;FastTrack is an open-source cross-platform tracking software. Easy to install and easy to use, it can track a large variety of systems from active particles to animals, with a known or unknown number of objects. It can process movies from any quality on low-end to high-end computers.&lt;/p&gt;
&lt;p&gt;Two main features are implemented in the software:
- A fast and automatic tracking algorithm that can detect and track objects, conserving the objects&#39; identities across the video recording.
- A manual tool to review the tracking where errors can be corrected rapidly and easily to achieve 100% accuracy with a minimum of efforts.&lt;/p&gt;
&lt;p&gt;FastTrack do not require coding abilities to be used. A developer documentation is available for users who want to embed FastTrack tracking algorithm directly inside their projects.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Benjamin Gallois&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/FastTrackOrg/FastTrack&#34;&gt;https://github.com/FastTrackOrg/FastTrack&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.fasttrack.sh/UserManual/docs/assets/example_vid.webm&#34;&gt;http://www.fasttrack.sh/UserManual/docs/assets/example_vid.webm&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Benjamin Gallois&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>MorphoPy: A python package for feature extraction of neural morphologies</title>
      <link>https://open-neuroscience.com/en/post/morphopy_a_python_package_for_feature_extraction_of_neural_morphologies/</link>
      <pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/morphopy_a_python_package_for_feature_extraction_of_neural_morphologies/</guid>
      <description>&lt;p&gt;MorphoPy is an open software package written in Python3 that allows for visualization and processing of morphological reconstructions of neural data. It has been created to facilitate the translation from morphology graphs into descriptive features like density maps, morphometric statistics, and persistence diagrams for down-stream exploration and statistical analysis.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Sophie Laturnus; Adam von Daranyi; Ziwei Huang; Philipp Berens&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/berenslab/MorphoPy&#34;&gt;https://github.com/berenslab/MorphoPy&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Sophie Laturnus&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>DIPY</title>
      <link>https://open-neuroscience.com/en/post/dipy/</link>
      <pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/dipy/</guid>
      <description>&lt;p&gt;DIPY is the paragon 3D/4D+ imaging library in Python. Contains generic methods for spatial normalization, signal processing, machine learning, statistical analysis and visualization of medical images. Additionally, it contains specialized methods for computational anatomy including diffusion, perfusion and structural imaging.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/dipy/dipy/graphs/contributors&#34;&gt;https://github.com/dipy/dipy/graphs/contributors&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://dipy.org&#34;&gt;https://dipy.org&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCHnEuCRDGFOR5cfEo0nD3pw&#34;&gt;https://www.youtube.com/channel/UCHnEuCRDGFOR5cfEo0nD3pw&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
anonymous&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>NeuroFedora</title>
      <link>https://open-neuroscience.com/en/post/neurofedora/</link>
      <pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/neurofedora/</guid>
      <description>&lt;p&gt;NeuroFedora is an initiative to provide a ready to use Fedora Linux based Free/Open source software platform for neuroscience. We believe that similar to Free software, science should be free for all to use, share, modify, and study. The use of Free software also aids reproducibility, data sharing, and collaboration in the research community. By making the tools used in the scientific process easier to use, NeuroFedora aims to take a step to enable this ideal.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;NeuroFedora volunteers @ the Fedora project&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://neuro.fedoraproject.org&#34;&gt;https://neuro.fedoraproject.org&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Ankur Sinha (NeuroFedora SIG member)&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Brainglobe atlas API</title>
      <link>https://open-neuroscience.com/en/post/brainglobe_atlas_api/</link>
      <pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/brainglobe_atlas_api/</guid>
      <description>&lt;p&gt;Many excellent brain atlases exist for different species. Some of them have an API (application programming interface) to allow users to interact with the data programmatically (e.g. the excellent Allen Mouse Brain Atlas), but many do not, and there is no consistent way to process data from multiple sources.&lt;/p&gt;
&lt;p&gt;The brainglobe atlas API (BG-AtlasAPI) deals with this problem by providing a common interface for programmers to download and process atlas data from multiple sources.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Adam Tyson; Federico Claudi; Luigi Petrucco&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/brainglobe/bg-atlasapi&#34;&gt;https://github.com/brainglobe/bg-atlasapi&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Adam Tyson&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>culture_shock</title>
      <link>https://open-neuroscience.com/en/post/culture_shock/</link>
      <pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/culture_shock/</guid>
      <description>&lt;p&gt;Culture Shock is an open-source electroporator that was developed
through internet based collaboration, starting on the DIYbio Google
Group. It is an evolution on the traditional capacitive discharge
circuit topology, instead using pulsed induction to enable a
programmable waveform as well as reduce the size, weight, and cost of
the equipment. With all these benefits, we hope to reduce the burden
of laboratory consumables for DNA transformation and electrofusion
procedures, where chemical supplies are currently relied on. The added
benefit of programmability allows many cell types to be manipulated by
altering the voltage level, or even giving the voltage profile a
particular shape.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;John Griessen; Nathan McCorkle; Bryan Bishop&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kanzure/culture_shock&#34;&gt;https://github.com/kanzure/culture_shock&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Nathan McCorkle&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>brainrender</title>
      <link>https://open-neuroscience.com/en/post/brainrender/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/brainrender/</guid>
      <description>&lt;p&gt;brainrender is a python package for the visualization of three dimensional neuro-anatomical data. It can be used to render data from publicly available data set (e.g. Allen Brain atlas) as well as user generated experimental data. The goal of brainrender is to facilitate the exploration and dissemination of neuro-anatomical data by providing a user-friendly platform to create high-quality 3D renderings.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Federico Claudi&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/BrancoLab/BrainRender&#34;&gt;https://github.com/BrancoLab/BrainRender&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Federico Claudi&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>JASP</title>
      <link>https://open-neuroscience.com/en/post/jasp/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/jasp/</guid>
      <description>&lt;p&gt;JASP is a cross-platform statistical software program with a state-of-the-art graphical user interface. The JASP interface allows you to conduct statistical analyses in seconds, and without having to learn programming or risking a programming mistake. JASP is open-source and free of charge, and we provide it as a service to the community. JASP is statistically inclusive as it offers both frequentist and Bayesian analysis methods.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;The JASP Team&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://jasp-stats.org/&#34;&gt;https://jasp-stats.org/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=HxqB7CUA-XI&#34;&gt;https://www.youtube.com/watch?v=HxqB7CUA-XI&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
EJ Wagenmakers&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>pyControl</title>
      <link>https://open-neuroscience.com/en/post/pycontrol/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/pycontrol/</guid>
      <description>&lt;p&gt;pyControl is a system of open source hardware and software for controlling behavioural experiments, built around the Micropython microcontroller.&lt;/p&gt;
&lt;p&gt;pyControl makes it easy to program complex behavioural tasks using a clean, intuitive, and flexible syntax for specifying tasks as state machines. User created task definition files, written in Python, run directly on the microcontroller, supported by pyControl framework code. This gives users the power and simplicity of Python for specifying task behaviour, while allowing advanced users low-level access to the microcontroller hardware.&lt;/p&gt;
&lt;p&gt;pyControl hardware consists of a breakout board and a set of devices such as nose-pokes, audio boards, LED drivers, rotary encoders and stepper motor controllers that are connected to the breakout board to create behavioural setups.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Thomas Akam&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://pycontrol.readthedocs.io&#34;&gt;https://pycontrol.readthedocs.io&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Thomas Akam&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>pyPhotometry</title>
      <link>https://open-neuroscience.com/en/post/pyphotometry/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/pyphotometry/</guid>
      <description>&lt;p&gt;pyPhotometry is system of open source, Python based, hardware and software for neuroscience fiber photometry data acquisition, consisting of an acquisition board and graphical user interface.&lt;/p&gt;
&lt;p&gt;pyPhotometry supports data aquisition from two analog and two digital inputs, and control of two LEDs via built in LED drivers with an adjustable 0-100mA output. The system supports time-division multiplexed illumination which allows fluoresence evoked by different excitation wavelengths to be independenly readout from a single photoreciever signal.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Thomas Akam&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://pyphotometry.readthedocs.io&#34;&gt;https://pyphotometry.readthedocs.io&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Thomas Akam&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>SLEAP</title>
      <link>https://open-neuroscience.com/en/post/sleap/</link>
      <pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/sleap/</guid>
      <description>&lt;p&gt;SLEAP (Social LEAP Estimates Animal Poses) is a multi-animal pose tracker based on deep learning. It is the successor of LEAP (Pereira et al., Nature Methods, 2019) and was designed to deal with the problem of tracking body landmarks of multiple freely interacting animals.&lt;/p&gt;
&lt;p&gt;Using deep learning, SLEAP trains neural network models from few user annotations to enable highly accurate body part localization, grouping and tracking. It supports multiple neural network architectures, including pretrained state-of-the-art models and lightweight customizable architectures. SLEAP has been used successfully to track mice, fruit flies, bees and other species of animals under a variety of experimental and imaging conditions.&lt;/p&gt;
&lt;p&gt;The software was designed to make it easy for users with no experience with deep learning through a fully featured GUI, as well as providing a rich functionality for advanced users seeking to develop a custom solution for their project. Tutorials and guides are available on our website (&lt;a href=&#34;https://sleap.ai&#34;&gt;https://sleap.ai&lt;/a&gt;) detailing steps for easy installation (Windows/Mac/Linux), labeling a new project, training on the locally or on the cloud, and tracking new data.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Talmo Pereira; Joshua Shaevitz; Mala Murthy&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://sleap.ai&#34;&gt;https://sleap.ai&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zwCf1pGnBUw&#34;&gt;https://www.youtube.com/watch?v=zwCf1pGnBUw&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Talmo Pereira&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Neuroimaging Informatics Tools and Resources Collaboratory (NITRC)</title>
      <link>https://open-neuroscience.com/en/post/neuroimaging_informatics_tools_and_resources_collaboratory_nitrc/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/neuroimaging_informatics_tools_and_resources_collaboratory_nitrc/</guid>
      <description>&lt;p&gt;NeuroImaging Tools &amp;amp; Resources Collaboratory is an award-winning free web-based resource that offers comprehensive information on an ever expanding scope of neuroinformatics software and data. Since debuting in 2007, NITRC has helped the neuroscience community make further discoveries using software and data produced from research that used to end up lost or disregarded.&lt;/p&gt;
&lt;p&gt;NITRC also provides free access to data and enables pay-per-use cloud-based access to unlimited computing power, enabling worldwide scientific collaboration with minimal startup and cost. NITRC’s scientific focus includes: MR, PET/SPECT, CT, EEG/MEG, optical imaging, clinical neuroimaging, computational neuroscience, and imaging genomics software tools, data, and computational resources.&lt;/p&gt;
&lt;p&gt;With NITRC and its components—the Resources Registry (NITRC-R), Image Repository (NITRC-IR), and Computational Environment (NITRC-CE)—a researcher can obtain pilot or proof-of-concept data to validate a hypothesis for just a few dollars.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;NITRC Development Team&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.nitrc.org&#34;&gt;http://www.nitrc.org&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
David Kennedy&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>PiVR</title>
      <link>https://open-neuroscience.com/en/post/pivr/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/pivr/</guid>
      <description>&lt;p&gt;PiVR is a system that allows experimenters to immerse small animals into virtual realities. The system tracks the position of the animal and presents light stimulation according to predefined rules, thus creating a virtual landscape in which the animal can behave. By using optogenetics, we have used PiVR to present fruit fly larvae with virtual olfactory realities, adult fruit flies with a virtual gustatory reality and zebrafish larvae with a virtual light gradient.&lt;/p&gt;
&lt;p&gt;PiVR operates at high temporal resolution (70Hz) with low latencies (&amp;lt;30 milliseconds) while being affordable (&amp;lt;US$500) and easy to build (&amp;lt;6 hours). Through extensive documentation (&lt;a href=&#34;http://www.PiVR.org&#34;&gt;www.PiVR.org&lt;/a&gt;), this tool was designed to be accessible to a wide public, from high school students to professional researchers studying systems neuroscience in academia.&lt;/p&gt;
&lt;p&gt;The project is open source (BSD-3) and the documented code written in the freely available programming language Python. We hope that PiVR will be adapted by advanced users for their particular needs, for example to create closed-loop experiments involving other sensory modalities (e.g., sound/vibration) through the use of PWM controllable devices. We envision PiVR to be used as the central module when creating virtual realities for a variety of sensory modalities. This ‘PiVR module’ takes care of detecting the animal and presenting the appropriate PWM signal that is then picked up by the PWM controllable device installed by the user, for example to produce a sound whenever an animal enters a pre-defined region.&lt;/p&gt;
&lt;p&gt;In short, PiVR is a powerful and affordable experimental platform allowing experimenters to create a wide array of virtual reality experiments. Our hope is that PiVR will be adapted by several labs to democratize closed-loop experiments and, by standardizing image quality and the animal detection algorithm, increase reproducibility.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;David Tadres; Matthieu Louis&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.PiVR.org&#34;&gt;http://www.PiVR.org&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=w5tIG6B6FWo&#34;&gt;https://www.youtube.com/watch?v=w5tIG6B6FWo&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
David Tadres&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>ReproNim: A Center for Reproducible Neuroimaging Computation</title>
      <link>https://open-neuroscience.com/en/post/repronim_a_center_for_reproducible_neuroimaging_computation/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/repronim_a_center_for_reproducible_neuroimaging_computation/</guid>
      <description>&lt;p&gt;ReproNim&amp;rsquo;s goal is to improve the reproducibility of neuroimaging science and extend the value of our national investment in neuroimaging research, while making the process easier and more efficient for investigators.&lt;/p&gt;
&lt;p&gt;ReproNim delivers a reproducible analysis framework comprised of components that include: 1) data and software discovery; 2) implementation of standardized description of data, results and workflows; 3) development of execution options that facilitates operation in all computational environments; 4)
provision of training and education to the community.&lt;/p&gt;
&lt;p&gt;All components of the framework are intended to foster continued use and development of the reproducible and generalizable framework in neuroimaging research.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;The ReproNim Development Team&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ReproNim&#34;&gt;https://github.com/ReproNim&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
David Kennedy&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Simple Behavioral Analysis (SimBA)</title>
      <link>https://open-neuroscience.com/en/post/simple_behavioral_analysis_simba/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/simple_behavioral_analysis_simba/</guid>
      <description>&lt;p&gt;Several excellent computational frameworks exist that enable high-throughput and consistent tracking of freely moving unmarked animals. SimBA introduce and distribute a plug-and play pipeline that enables users to use these pose-estimation approaches in combination with behavioral annotation for the generation of supervised machine-learning behavioral predictive classifiers.&lt;/p&gt;
&lt;p&gt;SimBA was developed for the analysis of complex social behaviors, but includes the flexibility for users to generate predictive classifiers across other behavioral modalities with minimal effort and no specialized computational background.&lt;/p&gt;
&lt;p&gt;SimBA has a variety of extended functions for large scale batch video pre-processing, generating descriptive statistics from movement features, and interactive modules for user-defined regions of interest and visualizing classification probabilities and movement patterns.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Simon Nilsson: Jia Jie Chhong; Sophia Hwang; Nastacia Goodwin; Sam A Golden&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/sgoldenlab/simba&#34;&gt;https://github.com/sgoldenlab/simba&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Frq6mMcaHBc&amp;amp;list=PLi5Vwf0hhy1R6NDQJ3U28MOUJPfl2YWYl&amp;amp;index=2&amp;amp;t=0s&#34;&gt;https://www.youtube.com/watch?v=Frq6mMcaHBc&amp;amp;list=PLi5Vwf0hhy1R6NDQJ3U28MOUJPfl2YWYl&amp;amp;index=2&amp;amp;t=0s&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Simon Nilsson&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Neurodata Without Borders</title>
      <link>https://open-neuroscience.com/en/post/neurodata_without_borders/</link>
      <pubDate>Sun, 19 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/neurodata_without_borders/</guid>
      <description>&lt;p&gt;Neurodata Without Borders is a data standard for neurophysiology, providing neuroscientists with a common standard to share, archive, use, and build analysis tools for neurophysiology data. NWB is designed to store a variety of neurophysiology data, including data from intracellular and extracellular electrophysiology experiments, data from optical physiology experiments, and tracking and stimulus data.&lt;/p&gt;
&lt;p&gt;The NWB team consists of neuroscientists and software developers who recognize that adoption of a unified data format is an important step toward breaking down the barriers to data sharing in neuroscience.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Andrew Tritt; Ryan Ly; Ben Dichter; Oliver Ruebel&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nwb.org/&#34;&gt;https://www.nwb.org/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/vfQsMyl0HQI&#34;&gt;https://youtu.be/vfQsMyl0HQI&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Ben Dichter&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>OpenDrop Digital Microfluidics Platform</title>
      <link>https://open-neuroscience.com/en/post/opendrop_digital_microfluidics_platform/</link>
      <pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/opendrop_digital_microfluidics_platform/</guid>
      <description>&lt;p&gt;OpenDrop a modular, open source digital microfludics platform for research purposes. The device uses recent electro-wetting technology to control small droplets of liquids. Potential applications are lab on a chip devices for automating processes of digital biology.&lt;/p&gt;
&lt;p&gt;The OpenDrop V4 is modular electrowetting controller. The driver board is equipped with a connector that can host a circuit board cartridge with a 14×8 electrode array and 4 reservoirs. The liquids stay on a thin, hydrophobic foil laminated to the circuit board . The device is powered from USB though an included USB-C cable. All the voltage level are generated on the device and can be set with the built in soft menu from 150-300 Volts, DC or AC.&lt;/p&gt;
&lt;p&gt;OpenDrop Cartridges
The modular concepts of the OpenDrop V4 allows different configurations of cartridges: A gold coated electrode array board that can be coated with any dielectric layer and hydrophobic coating to make cartridges for topless digital microfluidic applications using readily available materials.The OpenDrop V4 Cartridge is a close-cell cartridge capable of “move, mix, split and reservoir dispensing”. The 4×8 electrode array and 4 reservoirs are laminated with a 15um ETFE foil, hydrophobic coating and ITO top cover.&lt;/p&gt;
&lt;p&gt;Programming
The OpenDrop V4 can be operated standalone and droplets can be moved through the built in joystick. A control software to program sequences of patterns from a computer is available as a free download. The board is also compatible with Adafruit Feather M0 controller boards and can be reprogrammed through the free Arduino IDE for custom specific applications.  A sample code with the instruction to activate electrodes can be found on the OpenDrop GitHub.&lt;/p&gt;
&lt;p&gt;Features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Modular Cartridge System
- Connector to connect electrode board with up to 128 channels
- Gold coated 14×8 electrodes array, 2.75 mm x 2.75 mm in size, 4mil gaps&lt;/li&gt;
&lt;li&gt;Reservoirs – the new electrode array features 4 CT-type reservoirs&lt;/li&gt;
&lt;li&gt;AC and DC voltage generated on the device form USB power. True AC voltage driving capability (up to 300VAC).&lt;/li&gt;
&lt;li&gt;32bit AVR SAMD21G18 microprocessor with plenty of memory and power
- Electronic settings for voltage level, frequency and AC/DC selection
- Electronic reading of actual voltage level
- One connector for communication and powering (USB-C)
- Optical isolation of the high-voltage electronics trough opto-couplers and PhotoMOS
- New polyphonic audio amplifier and speaker (it’s a synth!)
- Cartridge presence detection
- Feedback amplifier
- Super flat OLED Display
- Nice joystick and 2 buttons, 3 LEDs
- Reset button
- All files open source, designed on KiCAD&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;MSc Urs Gaudenz&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.gaudi.ch/OpenDrop/&#34;&gt;http://www.gaudi.ch/OpenDrop/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=TY97QfWY6J4&#34;&gt;https://www.youtube.com/watch?v=TY97QfWY6J4&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Anonymous&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Stytra</title>
      <link>https://open-neuroscience.com/en/post/stytra/</link>
      <pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/stytra/</guid>
      <description>&lt;p&gt;Stytra, a flexible, open-source software package, written in Python and designed to cover all the general requirements involved in larval zebrafish behavioral experiments.&lt;/p&gt;
&lt;p&gt;It provides timed stimulus presentation, interfacing with external devices and simultaneous real-time tracking of behavioral parameters such as position, orientation, tail and eye motion in both freely-swimming and head-restrained preparations.&lt;/p&gt;
&lt;p&gt;Stytra logs all recorded quantities, metadata, and code version in standardized formats to allow full provenance tracking, from data acquisition through analysis to publication.&lt;/p&gt;
&lt;p&gt;The package is modular and expandable for different experimental protocols and setups. We also provide complete documentation with examples for extending the package to new stimuli and hardware, as well as a schema and parts list for behavioural setups.&lt;/p&gt;
&lt;p&gt;The software can be used in the context of calcium imaging experiments by interfacing with other acquisition devices.&lt;/p&gt;
&lt;p&gt;Our aims are to enable more laboratories to easily implement behavioral experiments, as well as to provide a platform for sharing stimulus protocols that permits easy reproduction of experiments and straightforward validation.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Vilim Stih; Luigi Petrucco&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/portugueslab/stytra&#34;&gt;https://github.com/portugueslab/stytra&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Anonymous&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>An Open-source Anthropomorphic Robot Hand System: HRI Hand</title>
      <link>https://open-neuroscience.com/en/post/an_open-source_anthropomorphic_robot_hand_system_hri_hand/</link>
      <pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/an_open-source_anthropomorphic_robot_hand_system_hri_hand/</guid>
      <description>&lt;p&gt;We present an open-source anthropomorphic robot hand system called HRI hand. Our robot hand system was developed with a focus on the end-effector role of the collaborative robot manipulator. HRI hand is a research platform that can be built at a lower price (approximately $500, using only 3D printing) than commercial end-effectors. Moreover, it was designed as a two four-bar linkage for the under-actuated mechanism and provides pre-shaping motion similar to the human hand prior to touching an object. A URDF, python node, and rviz package is also provided to support the Robot Operating System (ROS). All hardware CAD design files and software source codes have been released and can be easily assembled and modified. The system proposed in this paper is developed with a five-finger structure, but each finger is modularized, so it can be developed with end-effectors of various shapes depending on the shape of the palm.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Hyeonjun Park; Donghan Kim&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/MrLacuqer/HRI-hand-firmware.git&#34;&gt;https://github.com/MrLacuqer/HRI-hand-firmware.git&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/c5Ry3tl9FVw&#34;&gt;https://youtu.be/c5Ry3tl9FVw&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Andre M Chagas&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>cellfinder</title>
      <link>https://open-neuroscience.com/en/post/cellfinder/</link>
      <pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/cellfinder/</guid>
      <description>&lt;p&gt;cellfinder is software from the Margrie Lab at the Sainsbury Wellcome Centre for automated 3D cell detection and registration of whole-brain images (e.g. serial two-photon or lightsheet imaging).&lt;/p&gt;
&lt;p&gt;It’s a work in progress, but cellfinder can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detect labelled cells in 3D in whole-brain images (many hundreds of GB)&lt;/li&gt;
&lt;li&gt;Register the image to an atlas (such as the Allen Mouse Brain Atlas)&lt;/li&gt;
&lt;li&gt;Segment the brain based on the reference atlas&lt;/li&gt;
&lt;li&gt;Calculate the volume of each brain area, and the number of labelled cells within it&lt;/li&gt;
&lt;li&gt;Transform everything into standard space for analysis and visualisation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Adam Tyson&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/SainsburyWellcomeCentre/cellfinder&#34;&gt;https://github.com/SainsburyWellcomeCentre/cellfinder&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Adam Tyson&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>DeepLabCut</title>
      <link>https://open-neuroscience.com/en/post/deeplabcut/</link>
      <pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/deeplabcut/</guid>
      <description>&lt;p&gt;DeepLabCut™ is an efficient method for 3D markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results (i.e. you can match human labeling accuracy) with minimal training data (typically 50-200 frames). We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors.&lt;/p&gt;
&lt;p&gt;The package is open source, fast, robust, and can be used to compute 3D pose estimates. Please see the original paper and the latest work below.  This package is collaboratively developed by the Mathis Group &amp;amp; Mathis Lab at EPFL/Harvard.&lt;/p&gt;
&lt;p&gt;The code is freely available and easy to install in a few clicks with Anaconda (and pypi). Please see instructions on deeplabcut.org. We also provide a very easy to use GUI interface, and a step-by-step user guide!&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Mackenzie Mathis, Alexander Mathis &amp;amp; contributors&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://deeplabcut.org/&#34;&gt;http://deeplabcut.org/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC2HEbWpC_1v6i9RnDMy-dfA&#34;&gt;https://www.youtube.com/channel/UC2HEbWpC_1v6i9RnDMy-dfA&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Mackenzie Mathis&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>MNE-Python</title>
      <link>https://open-neuroscience.com/en/post/mne-python/</link>
      <pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/mne-python/</guid>
      <description>&lt;p&gt;MNE is a software package for processing electrophysiological signals primarily from magnetoencephalographic (MEG) and electroencephalographic (EEG) recordings, and more recently sEEG, ECoG and fNIRS. It provides a comprehensive solution for data preprocessing, forward modeling (with boundary element models), distributed source imaging, time–frequency analysis, non-parametric multivariate statistics, multivariate pattern analysis, and connectivity estimation. Importantly, this package allows all of these analyses to be applied in both sensor or source space. MNE is developed by an international team, with particular care for computational efficiency, code quality, and readability, as well as the common goal of facilitating reproducibility in neuroscience.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Alexandre Gramfort;Eric Larson;Denis Engemann;Daniel Strohmeier;Christian Brodbeck;Roman Goj;Mainak Jas;Teon Brooks;Lauri Parkkonen;Matti Hämäläinen;Jaakko Leppakangas;Jona Sassenhagen;Jean-Rémi King;Daniel McCloy;Marijn van Vliet;Clemens Brunner;Chris Holdgraf;Martin Luessi;Joan Massich;Guillaume Favelier;Andrew R. Dykstra;Mikolaj Magnuski;Stefan Appelhoff;Britta Westner;Richard Höchenberger;Robert Luke;Luke Bloy;Thomas Hartmann;Olaf Hauk;Adam Li&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://mne.tools&#34;&gt;https://mne.tools&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Anonymous&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Nilearn</title>
      <link>https://open-neuroscience.com/en/post/nilearn/</link>
      <pubDate>Sat, 11 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/nilearn/</guid>
      <description>&lt;p&gt;Nilearn is a Python module for fast and easy statistical learning on NeuroImaging data. It leverages the scikit-learn Python toolbox for multivariate statistics with applications such as predictive modelling, classification, decoding, or connectivity analysis.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/orgs/nilearn/people&#34;&gt;https://github.com/orgs/nilearn/people&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://nilearn.github.io/&#34;&gt;http://nilearn.github.io/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Anonymous&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>A Cartesian Coordinate Robot for Dispensing Fruit Fly Food</title>
      <link>https://open-neuroscience.com/en/post/a_cartesian_coordinate_robot_for_dispensing_fruit_fly_food/</link>
      <pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/a_cartesian_coordinate_robot_for_dispensing_fruit_fly_food/</guid>
      <description>&lt;p&gt;The fruit fly, Drosophila melanogaster, continues to be one of the most widely used model organisms in biomedical research.&lt;/p&gt;
&lt;p&gt;Though chosen for its ease of husbandry, maintaining large numbers of stocks of fruit flies, as done by many laboratories, is labour-intensive.&lt;/p&gt;
&lt;p&gt;One task which lends itself to automation is the production of the vials of food in which the flies are reared. Fly facilities typically have to generate several thousand vials of fly food each week to sustain their fly stocks.&lt;/p&gt;
&lt;p&gt;The system presented here combines a cartesian coordinate robot with a peristaltic pump. The design of the robot is based on an open hardware CNC (computer numerical control) machine, and uses belt and pulley actuators for the X and Y axes, and a leadscrew actuator for the Z axis.&lt;/p&gt;
&lt;p&gt;CNC motion and operation of the peristaltic pump are controlled by grbl (&lt;a href=&#34;https://github.com/gnea/grbl),&#34;&gt;https://github.com/gnea/grbl),&lt;/a&gt; an open source, embedded, G-code parser. Grbl is written in optimized C and runs directly on an Arduino. A Raspberry Pi is used to generate and stream G-code instructions to Grbl.&lt;/p&gt;
&lt;p&gt;A touch screen on the Raspberry Pi provides a graphical user interface to the system. Whilst the robot was built for the express purpose of filling vials of fly food, it could potentially be used for other liquid handling tasks in the laboratory.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Matt Wayland; Matthias Landgraf&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/WaylandM/fly-food-robot&#34;&gt;https://github.com/WaylandM/fly-food-robot&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.6084/m9.figshare.5175223.v1&#34;&gt;https://doi.org/10.6084/m9.figshare.5175223.v1&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matt Wayland&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Bonsai</title>
      <link>https://open-neuroscience.com/en/post/bonsai/</link>
      <pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/bonsai/</guid>
      <description>&lt;p&gt;Bonsai is a high-performance, easy to use, and flexible visual programming language for designing closed-loop neuroscience experiments combining physiology and behaviour data.&lt;/p&gt;
&lt;p&gt;Bonsai has allowed scientists with no previous programming experience to quickly develop their own experimental rigs and is also being increasingly used as a platform to integrate new open-source hardware and software from the experimental neuroscience community.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Gonçalo Lopes&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://bonsai-rx.org/&#34;&gt;https://bonsai-rx.org/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Gonçalo Lopes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Ethoscopes</title>
      <link>https://open-neuroscience.com/en/post/ethoscopes/</link>
      <pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/ethoscopes/</guid>
      <description>&lt;p&gt;Ethoscopes are machines for high-throughput analysis of behavior in Drosophila and other animals.&lt;/p&gt;
&lt;p&gt;Ethoscopes provide a software and hardware solution that is reproducible and easily scalable.&lt;/p&gt;
&lt;p&gt;They perform, in real-time, tracking and profiling of behavior by using a supervised machine learning algorithm, are able to deliver behaviorally triggered stimuli to flies in a feedback-loop mode, and are highly customizable and open source.&lt;/p&gt;
&lt;p&gt;Ethoscopes can be built easily by using 3D printing technology and rely on Raspberry Pi microcomputers and Arduino boards to provide affordable and flexible hardware.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Quentin Geissmann; Luis Garcia; Giorgio Gilestro&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://lab.gilest.ro/ethoscope&#34;&gt;http://lab.gilest.ro/ethoscope&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Giorgio Gilestro&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Bonvision</title>
      <link>https://open-neuroscience.com/en/post/bonvision/</link>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/bonvision/</guid>
      <description>&lt;p&gt;BonVision is an open-source closed-loop visual environment generator developed by the Saleem Lab and Solomon Lab at the UCL Institute of Behavioural Neuroscience in collaboration with NeuroGEARS.&lt;/p&gt;
&lt;p&gt;BonVision’s key features include:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Naturally closed-loop system based on reactive coding of the Bonsai framework
Handles 2D and 3D stimuli with equal ease
Visual environment generated independent of display configuration
Graphical programming language of the Bonsai framework
Can be used for Augmented Reality, Virtual Reality or 2D visual stimuli
Does not require the observer to be in a fixed position
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Bonvision&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://bonvision.github.io&#34;&gt;http://bonvision.github.io&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Poseidon</title>
      <link>https://open-neuroscience.com/en/post/poseidon/</link>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/poseidon/</guid>
      <description>&lt;p&gt;The Poseidon is an open-source syringe pump and microscope system. It uses 3D printed parts and common components that can be easily purchased. It can be used in microfluidics experiments or other applications. You can assemble it in a short-time for under $400. The system is modular and highly customizable. Examples of applications are: control the chemical environment of a bioreactor, purify proteins and precisely add reagents to chemical reactions over time.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Pachter Lab&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://pachterlab.github.io/poseidon&#34;&gt;https://pachterlab.github.io/poseidon&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Miguel Fernandes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>DeepLabStream</title>
      <link>https://open-neuroscience.com/en/post/deeplabstream/</link>
      <pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/deeplabstream/</guid>
      <description>&lt;p&gt;DeepLabStream is a python based multi-purpose tool that enables the realtime tracking of animals and manipulation of experiments. Our toolbox is adapted from the previously published DeepLabCut (Mathis et al., 2018) and expands on its core capabilities. DeepLabStreams core feature is the real-time analysis using any type of camera-based video stream (incl. multiple streams). Building onto that, we designed a full experimental closed-loop toolkit. It enables running experimental protocols that are dependent on a constant stream of bodypart positions and feedback activation of several input/output devices. It&amp;rsquo;s capabilities range from simple region of interest (ROI) based triggers to headdirection or behavior dependent stimulation.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Schwarz Neurocon Lab&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/SchwarzNeuroconLab/DeepLabStream&#34;&gt;https://github.com/SchwarzNeuroconLab/DeepLabStream&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>SpikeInterface</title>
      <link>https://open-neuroscience.com/en/post/spikeinterface/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/spikeinterface/</guid>
      <description>&lt;p&gt;SpikeInterface is a unified Python framework for spike sorting. With its high-level API, it is designed to be accessible and easy to use, allowing users to build full analysis pipelines for spike sorting (reading-writing (IO) / preprocessing / spike sorting / postprocessing / validation / curation / comparison / visualization) with a few lines of code.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Alessio Buccino*; Cole Hurwitz*; Samuel Garcia; Jeremy Magland; Josh Siegle; Matthias Hennig&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/SpikeInterface/spikeinterface&#34;&gt;https://github.com/SpikeInterface/spikeinterface&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=nWJGwFB7oII&#34;&gt;https://www.youtube.com/watch?v=nWJGwFB7oII&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Alessio Buccino&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>neuTube</title>
      <link>https://open-neuroscience.com/en/post/neutube/</link>
      <pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/neutube/</guid>
      <description>&lt;p&gt;neuTube is an open source software for reconstructing neurons from fluorescence microscope images. It is easy to use and improves the efficiency of reconstructing neuron structures accurately. The framework combines 2D/3D visualization, semi-automated tracing algorithms, and flexible editing options that simplify the task of neuron reconstruction.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Ting Zhao&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.neutracing.com/&#34;&gt;https://www.neutracing.com/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Miguel Fernandes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>OpenTrons</title>
      <link>https://open-neuroscience.com/en/post/opentrons/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/opentrons/</guid>
      <description>&lt;p&gt;Today, biologists spend too much time pipetting by hand. We think biologists should have robots to do pipetting for them. People doing science should be free of tedious benchwork and repetitive stress injuries. They should be able to spend their time designing experiments and analyzing data.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s why we started Opentrons.&lt;/p&gt;
&lt;p&gt;We make robots for biologists. Our mission is to provide the scientific community with a common platform to easily share protocols and reproduce each other&amp;rsquo;s results. Our robots automate experiments that would otherwise be done by hand, allowing our community to spend more time pursuing answers to some of the 21st century’s most important questions&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Opentrons&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://opentrons.com/about&#34;&gt;https://opentrons.com/about&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCvMRmXIxnHs3AutkVhuqaQg&#34;&gt;https://www.youtube.com/channel/UCvMRmXIxnHs3AutkVhuqaQg&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Deep Cinac</title>
      <link>https://open-neuroscience.com/en/post/deep_cinac/</link>
      <pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/deep_cinac/</guid>
      <description>&lt;p&gt;Two-photon calcium imaging is now widely used to infer neuronal dynamics from changes in fluorescence of an indicator. However, state of the art computational tools are not optimized for the reliable detection of fluorescence transients from highly synchronous neurons located in densely packed regions such as the CA1 pyramidal layer of the hippocampus during early postnatal  stages  of  development.  Indeed,the  latest  analytical  tools  often  lack  proper benchmark  measurements.  To  meet  this  challenge,  we  first  developed  a  graphical  user interface allowing for a precise manual detection of all calcium transients from imaged neurons based on the visualization of the calcium imaging movie. Then, we analyzed the movies using a convolutional neural network with an attention process and a bidirectional long-short term memory network. This method is able to reach human performance and offers a better F1 score (harmonic mean of sensitivity and precision) than CaImAn to infer neural activity in the developingCA1 without any user intervention. It also enables automatically identifying activity originating from GABAergic neurons. Overall, DeepCINAC offers a simple, fast and flexible open-source toolbox for processing a wide variety of calcium imaging datasets while providing the tools to evaluate its performance.&lt;/p&gt;
&lt;p&gt;See full text at &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/803726v2.full.pdf&#34;&gt;https://www.biorxiv.org/content/10.1101/803726v2.full.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Julien Denis&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://gitlab.com/cossartlab/deepcinac&#34;&gt;https://gitlab.com/cossartlab/deepcinac&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>UC2</title>
      <link>https://open-neuroscience.com/en/post/uc2/</link>
      <pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/uc2/</guid>
      <description>&lt;p&gt;The open-source optical toolbox UC2 [YouSeeToo] simplifies the process of building optical setups, by combining 3D-printed cubes, each holding a specific component (e.g. lens, mirror) on a magnetic square-grid baseplate. The use of widely available consumables and 3D printing, together with documentation and software, offers an extremely low-cost and accessible alternative for both education and research areas. In order to reduce the entry barrier, we provide a fully comprehensive toolbox called TheBOX. A paper describing the scientific application in detail can be found &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2020.03.02.973073v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Benedict Diederich; René Lachmann; Barbora Marsikova&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://useetoo.org&#34;&gt;https://useetoo.org&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ey4uEFEG6MY&#34;&gt;https://www.youtube.com/watch?v=ey4uEFEG6MY&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Barbora Marsikova&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Automated Operant Conditioning</title>
      <link>https://open-neuroscience.com/en/post/automated_operant_conditioning/</link>
      <pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/automated_operant_conditioning/</guid>
      <description>&lt;p&gt;Operant conditioning (OC) is a classical paradigm and a standard technique used in experimental psychology in which animals learn to perform an action to achieve a reward. By using this paradigm, it is possible to extract learning curves and measure accurately reaction times (RTs). Both these measurements are proxy of cognitive capabilities and can be used to evaluate the effectiveness of therapeutic interventions in mouse models of disease. Here, we describe a fully 3D printable device that is able to perform OC on freely moving mice, while performing real-time tracking of the animal position. We successfully trained six mice, showing stereotyped learning curves that are highly reproducible across mice and reaching &amp;gt;70% of accuracy after 2 d of conditioning. Different products for OC are commercially available, though most of them do not provide customizable features and are relatively expensive. This data demonstrate that this system is a valuable alternative to available state-of-the-art commercial devices, representing a good balance between performance, cost, and versatility in its use.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Raffaele Mazziotti, Giulia Sagona, Leonardo Lupori, Virginia Martini and Tommaso Pizzorusso&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/raffaelemazziotti/oc_chamber&#34;&gt;https://github.com/raffaelemazziotti/oc_chamber&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Heuristic Spike Sorting Tuner (HSST), a framework to determine optimal parameter selection for a generic spike sorting algorithm</title>
      <link>https://open-neuroscience.com/en/post/heuristic_spike_sorting_tuner_hsst_a_framework_to_determine_optimal_parameter_selection_for_a_generic_spike_sorting_algorithm/</link>
      <pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/heuristic_spike_sorting_tuner_hsst_a_framework_to_determine_optimal_parameter_selection_for_a_generic_spike_sorting_algorithm/</guid>
      <description>&lt;p&gt;Extracellular microelectrodes frequently record neural activity from more than one neuron in the vicinity of the electrode. The process of labeling each recorded spike waveform with the identity of its source neuron is called spike sorting and is often approached from an abstracted statistical perspective. However, these approaches do not consider neurophysiological realities and may ignore important features that could improve the accuracy of these methods. Further, standard algorithms typically require selection of at least one free parameter, which can have significant effects on the quality of the output. We describe a Heuristic Spike Sorting Tuner (HSST) that determines the optimal choice of the free parameters for a given spike sorting algorithm based on the neurophysiological qualification of unit isolation and signal discrimination. A set of heuristic metrics are used to score the output of a spike sorting algorithm over a range of free parameters resulting in optimal sorting quality. We demonstrate that these metrics can be used to tune parameters in several spike sorting algorithms. The HSST algorithm shows robustness to variations in signal to noise ratio, number and relative size of units per channel. Moreover, the HSST algorithm is computationally efficient, operates unsupervised, and is parallelizable for batch processing.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;David A. Bjanes; Lee B. Fisher; Robert A. Gaunt; Douglas J. Weber&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/davidbjanes/hsst&#34;&gt;https://github.com/davidbjanes/hsst&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
David Bjanes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>VocalMat</title>
      <link>https://open-neuroscience.com/en/post/vocalmat/</link>
      <pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/vocalmat/</guid>
      <description>&lt;p&gt;Mice emit ultrasonic vocalizations (USV) to transmit socially-relevant information. To detect and classify these USVs, here we describe the development of VocalMat. VocalMat is a software that uses image-processing and differential geometry approaches to detect USVs in audio files, eliminating the need for user-defined parameter tuning. VocalMat also uses computational vision and machine learning methods to classify USVs into distinct categories. In a dataset of &amp;gt;4,000 USVs emitted by mice, VocalMat detected more than &amp;gt;98% of the USVs and accurately classified ≈86% of USVs when considering the most likely label out of 11 different USV types. We then used Diffusion Maps and Manifold Alignment to analyze the probability distribution of USV classification among different experimental groups, providing a robust method to quantify and qualify the vocal repertoire of mice. Thus, VocalMat allows accurate and highly quantitative analysis of USVs, opening the opportunity for detailed and high-throughput analysis of this behavior.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Antonio H. O. Fonseca, Gustavo M. Santana, Sergio Bampi, Marcelo O Dietrich&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.dietrich-lab.org/vocalmat&#34;&gt;https://www.dietrich-lab.org/vocalmat&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>BossDB</title>
      <link>https://open-neuroscience.com/en/post/bossdb/</link>
      <pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/bossdb/</guid>
      <description>&lt;p&gt;BossDB is a volumetric database that lives in the AWS cloud. Hundreds of terabytes of electron microscopy, light microscopy, and x-ray tomography data are available for free download and study.&lt;/p&gt;
&lt;p&gt;Have a project you want to share with the world for free? Get in touch!
&lt;a href=&#34;https://twitter.com/thebossdb&#34;&gt;https://twitter.com/thebossdb&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;JHU|APL&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://bossdb.org/&#34;&gt;https://bossdb.org/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Jordan Matelsky&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>3D Slicer</title>
      <link>https://open-neuroscience.com/en/post/3d_slicer/</link>
      <pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/3d_slicer/</guid>
      <description>&lt;p&gt;3D Slicer is a software for medical image informatics, image processing, and three-dimensional visualization. It’s extremely powerful and versatile with plenty of different options. It is a great tool for volume rendering, registration, interactive segmentation of images and even offers the possibility of running Python scripts thought an embedded Python interpreter.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Ron Kikinis&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Slicer/Slicer&#34;&gt;https://github.com/Slicer/Slicer&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Miguel Fernandes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Colaboratory</title>
      <link>https://open-neuroscience.com/en/post/colaboratory/</link>
      <pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/colaboratory/</guid>
      <description>&lt;p&gt;Colaboratory is a free Jupyter notebook environment that runs in the cloud. Your notebooks get stored on Google Drive. The great advantage is that you don’t have to install anything (however, for some features you need a Google account) on your system to use it. You can perform specific computations during data analysis with pre-installed Python libraries and gives you access to accelerated hardware for free (e.g. GPUs and TPUs).&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Google&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/notebooks/intro.ipynb&#34;&gt;https://colab.research.google.com/notebooks/intro.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Miguel Fernandes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Suite2P</title>
      <link>https://open-neuroscience.com/en/post/suite2p/</link>
      <pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/suite2p/</guid>
      <description>&lt;p&gt;Suite2P is a very modular imaging processing pipeline written in Python which allows you to perform registration of raw data movies, automatic cell detection, extraction of calcium traces and infers spike times. It is a very fast and accurate tool and can work on standard workstations. It also includes a visualization graphical user interface (GUI) that facilitates analysis and manual curation of the cell detection algorithm.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Carsen Stringer and Marius Pachitariu&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://mouseland.github.io/suite2p/_build/html/index.html&#34;&gt;https://mouseland.github.io/suite2p/_build/html/index.html&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Miguel Fernandes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Psychophysics toolboxes</title>
      <link>https://open-neuroscience.com/en/post/psychophysics-toolboxes/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/psychophysics-toolboxes/</guid>
      <description>&lt;br&gt;
&lt;p&gt;Roughly put, &lt;a href=&#34;http://en.wikipedia.org/wiki/Psychophysics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;psychophysics&lt;/a&gt; studies the relationships of physical stimuli and their respective elicited sensations and perception. Psyhophysics also relates to the techniques used to probe these relationships and the toolboxes here presented are mainly dealing with these techniques.&lt;/p&gt;
&lt;br&gt;
&lt;hr&gt;
&lt;br&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/smathot/osdoc/3.2/themes/cogsci/static/img/banner.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://osdoc.cogsci.nl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt; OpenSesame&lt;/a&gt; is a graphical opensource experiment builder. It has drag and drop features as well as customization possibilities, via python scripting and custom plugins. here is a &lt;a href=&#34;http://link.springer.com/article/10.3758%2Fs13428-011-0168-7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt; to a paper describing the software&lt;figure style=&#34;width: 853px&#34; class=&#34;wp-caption alignnone&#34;&gt;&lt;/p&gt;
&lt;br&gt;
&lt;hr&gt;
&lt;br&gt;
&lt;p&gt;&lt;a href=&#34;http://psychtoolbox.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Psychtoolbox&lt;/a&gt;, or PTB, is a free versatile toolbox to be used mainly in visual experiments, it is able to deliver visual and auditory stimuli and to receive subject input. It has a big quantity of active users (15,000 as stated on their &lt;a href=&#34;http://psychtoolbox.org/forum/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;) what should make the life of the beginner user somehow easier (they have a &lt;a href=&#34;https://psychtoolbox.discourse.group/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;forum page&lt;/a&gt;) The latest version (PTB-3 as this page was written) is able to run under MATLAB (version 7.X) and Octave (version 3.2.X) in any of the three main operational systems out there (Mac, Windows and Linux).  A paper describing the toolbox can be found &lt;a href=&#34;http://color.psych.upenn.edu/brainard/papers/Psychtoolbox.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;hr&gt;
&lt;br&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;img src=&#34;./psychopyLogoOnline.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://www.psychopy.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PsychoPy&lt;/a&gt; is also a free toolbox that can be used to deliver visual and auditory stimuli and receive inputs from subjects, on top of keyboard, mouse and button boxes, it also supports serial and parallel ports and compiled drivers (allowing interface with pretty much any hardware installed in your computer). It is written in Python, and it can be used with Windows, Mac or Linux. Two papers describing the toolbox can be found &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0165027006005772&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/neuro.11.010.2008/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Python, NumPy, SciPy &amp; Matplotlib</title>
      <link>https://open-neuroscience.com/en/post/python-numpy-scipy-matplotlib/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/python-numpy-scipy-matplotlib/</guid>
      <description>&lt;p&gt;Python is a free programming language that is widely used, most of the software developed for Linux is written in Python. It contains several libraries that cover a lot of problem domains, from asynchronous processing to zip files. Also it is available for most platforms. More information can be found at the language &lt;a href=&#34;http://www.python.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;official page.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More specifically to scientific computation, the &lt;a href=&#34;http://www.numpy.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NumPy&lt;/a&gt; project brings n-dimension array objects, random number capabilities, fourier transforms and many other useful tools.&lt;/p&gt;
&lt;p&gt;Boosting NumPy capabilities is &lt;a href=&#34;http://www.scipy.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SciPy&lt;/a&gt;, which is another Python library that adds signal processing, optimization and statistical tools to Python.&lt;/p&gt;
&lt;p&gt;After all the calculations are done, they can be plotted also using python and another useful library: &lt;a href=&#34;http://matplotlib.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matplotlib.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spike Gadgets</title>
      <link>https://open-neuroscience.com/en/post/spike-gadgets/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/spike-gadgets/</guid>
      <description>&lt;p&gt;A brief description of their current software (09.Sep.2016) is provided by one of their founders, Mattias Karlsson:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://www.spikegadgets.com/software/statescript.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;State Script:&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Do you need to control lasers for optogenetics, stimulators, or other TTL-based devices with precise, temporally defined patterns? Do you need to monitor beam breaks, lever presses, or other digital events in real time to define behavioral tasks?  You could program an Arduino, but that’s a lot of work. Or, you can use StateScript, which allows users with minimal programming experience define complex input/output relationships for the most demanding hardware control experiments.&lt;/p&gt;
&lt;div&gt;
&lt;p&gt;This open-source project now runs on two available hardware platforms, the MBED LPC1768 micro controller board ($50) and the SpikeGadgets electrophysiology and behavioral control system.  More hardware support in is the works. A software interface, which is part of the Trodes open-source eletrophysiology suite (&lt;a href=&#34;http://www.spikegadgets.com/software/trodes.html&#34; target=&#34;_blank&#34;&gt;&lt;a href=&#34;http://www.spikegadgets.com/software/trodes.html&#34;&gt;http://www.spikegadgets.com/software/trodes.html&lt;/a&gt;&lt;/a&gt;), allows you to upload scripts and dynamically interact with variables and ports states.&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;Anyone is welome to contribute. Here is the &lt;a href=&#34;https://bitbucket.org/mkarlsso/statescript&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bitbucket repo.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
  &lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;img src=&#34;https://i2.wp.com/www.spikegadgets.com/images/statescript_screenshot_2.png?resize=800%2C571&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
  &lt;/div&gt;
&lt;div&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://www.spikegadgets.com/software/trodes.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Trodes:&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Trodes is a software suite with a focus on data acquisition for extracellular neural recordings.  It has a growing user base and welcomes contributors with open arms! It is built using the ever-popular and powerful Qt C++ framework. While it is specialized to be used with SpikeGadgets’ ephys hardware, it also has built-in support for the Intan demo system and Open-Ephys hardware.&lt;/p&gt;
&lt;p&gt;It has some pretty impressive capabilities, including visualization of thousands of channels, spike viewing, online spike sorting, and low latency feedback control.  It has video processing, allowing position tracking that is synchronized to the recording, and integrates powerful environment control (lasers for optogenetics, levers, lights, pumps, etc.) with StateScript.&lt;/p&gt;
&lt;/div&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;img src=&#34;https://i1.wp.com/www.spikegadgets.com/images/trodesscreenshot.png?resize=800%2C444&#34; alt=&#34;Trodes interface&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt; Trodes interface &lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i1.wp.com/www.spikegadgets.com/images/trodes_screenshot_cameramod.png?resize=800%2C554&#34; alt=&#34;Trodes interface&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt; Trodes &lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Brainflow</title>
      <link>https://open-neuroscience.com/en/post/brainflow/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/brainflow/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://brainflow.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BrainFlow&lt;/a&gt; BrainFlow is a library intended to obtain, parse and analyze EEG, EMG, ECG and other kinds of data from biosensors, it provides two APIs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data Acquisition API to obtain data from BCI boards&lt;/li&gt;
&lt;li&gt;Signal Processing API which is completely independent and can be used without Data Acquisition API&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both of these APIs are uniform for all supported boards, so it allows to write completely board agnostic code.&lt;/p&gt;
&lt;p&gt;BrainFlow has bindings for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C++&lt;/li&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Java&lt;/li&gt;
&lt;li&gt;C#&lt;/li&gt;
&lt;li&gt;R&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And provides almost the same API for all languages above.&lt;/p&gt;
&lt;p&gt;Check &lt;a href=&#34;https://brainflow.readthedocs.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BrainFlow Docs&lt;/a&gt; for details.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Image, Office suites, and other general purpose software</title>
      <link>https://open-neuroscience.com/en/post/image-office-suits-and-other-general-purpose-software/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/image-office-suits-and-other-general-purpose-software/</guid>
      <description>&lt;p&gt;If you are using Linux, changes are that this page is not that useful for you, since most of these programs come installed by default. For you who are not yet into linux, most of these programs have Windows/Mac versions:&lt;/p&gt;
&lt;p&gt;Office suites (spreadsheet calculation, slide manufacturing , document writing):&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.openoffice.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open Office&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.libreoffice.org/#0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Libre Office&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Image manipulation programs (vectorized images or photoshop style):&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://inkscape.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inkscape&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.gimp.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gimp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;3D modelling (to create animations, solids or even things that can be printed):&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://free-cad.sourceforge.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FreeCad&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.blender.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blender&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.openscad.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenScad&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IPipet</title>
      <link>https://open-neuroscience.com/en/post/ipipet/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/ipipet/</guid>
      <description>&lt;p&gt;IPipet is a neat system to help you not to lose track of which wells you have already pipetted in or from. The idea is simple, you place a tablet running a link with your specific pipetting protocol under your source and destination plates. The tablet will illuminate the corresponding wells. After you pipette one sample, you press next on the tablet and the next sample will be illuminated. For more details watch the video (below) and visit the &lt;a href=&#34;http://ipipet.teamerlich.org/usage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project&amp;rsquo;s homepage.&lt;/a&gt; They even have a &lt;a href=&#34;http://www.thingiverse.com/thing:339588&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3D printable adaptor&lt;/a&gt; to prevent the well plate from slipping on the tablet surface.&lt;/p&gt;
&lt;br&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;iframe src=&#34;https://player.vimeo.com/video/90988265&#34; width=&#34;640&#34; height=&#34;360&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; fullscreen&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://vimeo.com/90988265&#34;&gt;iPipet Demo&lt;/a&gt; from &lt;a href=&#34;https://vimeo.com/user26499168&#34;&gt;Team Erlich&lt;/a&gt; on &lt;a href=&#34;https://vimeo.com&#34;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Lab management software</title>
      <link>https://open-neuroscience.com/en/post/lab-management-software/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/lab-management-software/</guid>
      <description>&lt;p&gt;Since organisation of ideas, stocks, and projects is a major concern (or at least should be) of labs and researchers, here is a small compilation of cost free sofware to help out:&lt;/p&gt;
&lt;br&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;img src=&#34;./Quartzy.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://www.quartzy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quartzy&lt;/a&gt; is a free web based application (supported by life sciences related companies) it focuses on sharing protocols, tracking orders, manage lab inventory and shared quipment management.&lt;/p&gt;
&lt;br&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;img src=&#34;./elabftw-logo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elabftw.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;eLabFTW&lt;/a&gt; is a management system created by Nicolas Carpi. It is opensource (which means each lab can customize it for special needs), free and it can be installed locally. Its &lt;a href=&#34;https://demo.elabftw.net/login.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;online demo version&lt;/a&gt; focuses on experiment log, database (where drugs, chemicals, animal strains and etc can be logged) and team (where lab members can be listed).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Operating systems</title>
      <link>https://open-neuroscience.com/en/post/linux-distributions/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/linux-distributions/</guid>
      <description>&lt;p&gt;Linux is an open source operating system and it is the major OS used in servers and supercomputers.  &lt;a href=&#34;http://www.ubuntu.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ubuntu&lt;/a&gt;, one of the best known distributions has been gaining space in the personal computing scene, now days already being factory &lt;a href=&#34;http://www.omgubuntu.co.uk/2012/05/ubuntu-to-ship-on-5-of-all-pcs-sold-next-year&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shipped&lt;/a&gt; by major manufacturers.&lt;/p&gt;
&lt;p&gt;But how practical is to migrate to a Linux distribution? Well, very. If one passes beyond the hassle of backing up data and installing a new OS, there are many advantages that come with it. For starters these OSs are safer than any Microsoft or Apple OS. There is a large community of users sharing solutions to problems, bugs and so on (there hasn’t been to today a widespread of any &lt;a href=&#34;http://en.wikipedia.org/wiki/Linux_malware&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;malware through Linux systems&lt;/a&gt;). Being open source, the distributions are perfect for customization, something really useful for science labs.&lt;/p&gt;
&lt;p&gt;A Small list of distributions that make a good starting point:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.debian.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Debian&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://neuro.debian.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeuroDebian&lt;/a&gt; (Debian oriented to neuroscience)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;www.ubuntu.com&#34;&gt;Ubuntu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.linuxmint.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://en.opensuse.org/Main_Page&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenSuse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fedoraproject.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fedora&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;http://www.ros.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ROS&lt;/a&gt; –&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The robot operating system is a flexible framework for writing robot software. It is a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behavior across a wide variety of robotic platforms.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Computer Vision and motion tracking software</title>
      <link>https://open-neuroscience.com/en/post/computer-vision-and-motion-tracking-software/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/computer-vision-and-motion-tracking-software/</guid>
      <description>&lt;p&gt;Motion tracking can be really useful in neurosciences, for automatic measurements of behaviour, among other things. Here you’ll find a small list of tracking softwares or libraries used to build such softwares:&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Complete softwares:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://ctrax.sourceforge.net/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ctrax&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;figure style=&#34;width: 128px&#34; class=&#34;wp-caption alignnone&#34;&gt;[&lt;img src=&#34;https://i0.wp.com/ctrax.sourceforge.net/images/ctrax-logo2b_128.png?resize=128%2C128&#34; alt=&#34;&#34; width=&#34;128&#34; height=&#34;128&#34; data-recalc-dims=&#34;1&#34; /&gt;](https://i0.wp.com/ctrax.sourceforge.net/images/ctrax-logo2b_128.png)&lt;figcaption class=&#34;wp-caption-text&#34;&gt;taken from: http://ctrax.sourceforge.net/index.html&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;blockquote&gt;
&lt;p&gt;Ctrax is an open-source, freely available, machine vision program for estimating the positions and orientations of many walking flies, maintaining their individual identities over long periods of time. It was designed to allow high-throughput, quantitative analysis of behavior in freely moving flies.&lt;/ul&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The bio tracking project, designed for multiple object tracking, developed at Georgia tech:&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.bio-tracking.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;a href=&#34;http://www.bio-tracking.org/&#34;&gt;http://www.bio-tracking.org/&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;embed-youtube&#34; style=&#34;text-align:center; display: block;&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Community Core Vision: Built with computer vision and machine sensing in mind, they mention multi touch applications as one of their focus on the website.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ccv.nuigroup.com/&#34;&gt;http://ccv.nuigroup.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;embed-youtube&#34; style=&#34;text-align:center; display: block;&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://derek.simkowiak.net/motion-tracking-with-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Motion Tracking using python&lt;/a&gt;: Independent developed software by Derek Simkowiak, in a project he ran a couple of years back with his daughter, to track Gerbills&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;embed-youtube&#34; style=&#34;text-align:center; display: block;&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://personal.ee.surrey.ac.uk/Personal/Z.Kalal/tld.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tracking-Learning-Detection&lt;/a&gt;: Developed by &lt;a href=&#34;http://personal.ee.surrey.ac.uk/Personal/Z.Kalal/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zdenek Kalal&lt;/a&gt; this software intends to track pretty much anything (object determination can be done via mouse) in real time and to learn features from the object as tracking goes on.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;embed-youtube&#34; style=&#34;text-align:center; display: block;&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; &lt;a href=&#34;http://openvisionc.sourceforge.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open Vision Control&lt;/a&gt;: Developed on top of OpenCV (see below) in Python, it is a general purpose tracking software with several applications&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;embed-youtube&#34; style=&#34;text-align:center; display: block;&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SwisTrack: Developed at EPFL, it is also a tracking system for multiple objects&lt;figure id=&#34;attachment_744&#34; style=&#34;width: 300px&#34; class=&#34;wp-caption aligncenter&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://i0.wp.com/openeuroscience.com/wp-content/uploads/2014/04/800px-swistrack4-ubuntu.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img class=&#34;size-medium wp-image-744&#34; src=&#34;https://i0.wp.com/openeuroscience.com/wp-content/uploads/2014/04/800px-swistrack4-ubuntu.png?resize=300%2C211&#34; alt=&#34;From http://en.wikibooks.org/wiki/Swistrack&#34; width=&#34;300&#34; height=&#34;211&#34; srcset=&#34;https://i0.wp.com/openeuroscience.com/wp-content/uploads/2014/04/800px-swistrack4-ubuntu.png?w=800 800w, https://i0.wp.com/openeuroscience.com/wp-content/uploads/2014/04/800px-swistrack4-ubuntu.png?resize=300%2C212 300w, https://i0.wp.com/openeuroscience.com/wp-content/uploads/2014/04/800px-swistrack4-ubuntu.png?resize=768%2C541 768w&#34; sizes=&#34;(max-width: 300px) 100vw, 300px&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;figcaption class=&#34;wp-caption-text&#34;&gt;From &lt;a href=&#34;http://en.wikibooks.org/wiki/Swistrack&#34;&gt;http://en.wikibooks.org/wiki/Swistrack&lt;/a&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://infoscience.epfl.ch/record/85929&#34;&gt;http://infoscience.epfl.ch/record/85929&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://infoscience.epfl.ch/record/125704&#34;&gt;http://infoscience.epfl.ch/record/125704&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0042247&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tracking software for Drosophila&lt;/a&gt;, by Colomb &lt;em&gt;et al&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Computer vision/tracking libraries:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://opencv.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open CV&lt;/a&gt; is a library for machine learning and computer vision. It is written for different computer languages and different operational systems.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The library has more than 2500 optimized algorithms, which includes a comprehensive set of both classic and state-of-the-art computer vision and machine learning algorithms. These algorithms can be used to detect and recognize faces, identify objects, classify human actions in videos, track camera movements, track moving objects, extract 3D models of objects, produce 3D point clouds from stereo cameras, stitch images together to produce a high resolution image of an entire scene, find similar images from an image database, remove red eyes from images taken using flash, follow eye movements, recognize scenery and establish markers to overlay it with augmented reality, etc.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.simplecv.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simple CV&lt;/a&gt; is a framework that tries to simplify the development of software that require computer vision/machine learning, since a lot of researchers have the necessity of building on such concepts, but sometimes don’t have the time/training necessary to do so.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>PySpace</title>
      <link>https://open-neuroscience.com/en/post/pyspace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/pyspace/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://pyspace.github.io/pyspace/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PySpace&lt;/a&gt; is a signal processing and classificiation environment for Python.&lt;/p&gt;
&lt;p&gt;Modular software for processing of large data streams that has been specifically designed to enable distributed execution and empirical evaluation of signal processing chains. Various signal processing algorithms are available within the software, from finite impulse response filters over data-dependent spatial filters (e.g. CSP, xDAWN) to established classifiers (e.g. SVM, LDA). pySPACE incorporates the concept of node and node chains of the Modular Toolkit for Data Processing (MDP) framework.&lt;/p&gt;
&lt;p&gt;A paper about PySpace can be found &lt;a href=&#34;http://journal.frontiersin.org/article/10.3389/fninf.2013.00040/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Python for Neurosciences (Frontiers collection)</title>
      <link>https://open-neuroscience.com/en/post/python-for-neuroscience-frontiers-collection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/python-for-neuroscience-frontiers-collection/</guid>
      <description>&lt;p&gt;Frontiers has created not one but two nice collections about open source software for neurosciences written in Python.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://journal.frontiersin.org/researchtopic/8/python-in-neuroscience&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here is collection 1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://journal.frontiersin.org/researchtopic/1591/python-in-neuroscience-ii&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here is collection 2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In these collections the readers will find a lot of nice resources, ranging from stimulus generation, to data formatting and analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simulations</title>
      <link>https://open-neuroscience.com/en/post/simulation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/simulation/</guid>
      <description>&lt;div id=&#34;pl-1067&#34;  class=&#34;panel-layout&#34; &gt;
  &lt;div id=&#34;pg-1067-0&#34;  class=&#34;panel-grid panel-no-style&#34; &gt;
    &lt;div id=&#34;pgc-1067-0-0&#34;  class=&#34;panel-grid-cell&#34; &gt;
      &lt;div id=&#34;panel-1067-0-0-0&#34; class=&#34;so-panel widget widget_sow-editor panel-first-child panel-last-child&#34; data-index=&#34;0&#34; &gt;
        &lt;div class=&#34;so-widget-sow-editor so-widget-sow-editor-base&#34;&gt;
          &lt;div class=&#34;siteorigin-widget-tinymce textwidget&#34;&gt;
            &lt;p&gt;
              Ever thought about playing with a virtual worm? or interacting with a simulated bee brain? Sounds interesting no? These are just two projects that offer anyone the opportunity to play around with brain/neuronal simulations and models. Some of them are hardware based, and some completely software:
            &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;        &amp;lt;p&amp;gt;
          &amp;lt;a href=&amp;quot;http://openeuroscience.com/open-source-simulations-and-models/open-worm/&amp;quot;&amp;gt;OpenWorm&amp;lt;/a&amp;gt;
        &amp;lt;/p&amp;gt;

        &amp;lt;p&amp;gt;
          &amp;lt;a href=&amp;quot;http://openeuroscience.com/open-source-simulations-and-models/green-brain/&amp;quot;&amp;gt;GreenBrain&amp;lt;/a&amp;gt;
        &amp;lt;/p&amp;gt;

        &amp;lt;p&amp;gt;
          &amp;lt;a href=&amp;quot;http://openeuroscience.com/open-source-simulations-and-models/neuronsneuronsneurons/&amp;quot;&amp;gt;Neurons,Neurons,Neurons&amp;lt;/a&amp;gt;
        &amp;lt;/p&amp;gt;

        &amp;lt;p&amp;gt;
          &amp;lt;a href=&amp;quot;http://openeuroscience.com/open-source-simulations-and-models/big-neuron/&amp;quot;&amp;gt;Big Neuron&amp;lt;/a&amp;gt;
        &amp;lt;/p&amp;gt;
      &amp;lt;/div&amp;gt;
    &amp;lt;/div&amp;gt;
  &amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;section class=&#34;blog&#34;&gt;
  &lt;div class=&#34;container&#34;&gt;
    &lt;div class=&#34;post-list&#34; itemscope=&#34;&#34; itemtype=&#34;http://schema.org/Blog&#34;&gt;
      {% for page in site.pages %}
        {% for category in page.categories %}
          {% if category == &#34;Simulation&#34; %}
            {% include card_page.html %}
          {% endif %}
        {% endfor %}
      {% endfor %}
&lt;pre&gt;&lt;code&gt;&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/div&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Vision Egg</title>
      <link>https://open-neuroscience.com/en/post/vision-egg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/vision-egg/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://visionegg.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vision Egg&lt;/a&gt; is a Python library for generating visual stimuli.&lt;/p&gt;
&lt;p&gt;In more detail, it is a high level interface in between Python and OpenGL, and can use inexpensive consumer grade graphics cards to generate precise visual stimuli. A paper with more details can be found here &lt;a href=&#34;http://journal.frontiersin.org/article/10.3389/neuro.11.004.2008/full&#34;&gt;http://journal.frontiersin.org/article/10.3389/neuro.11.004.2008/full&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
