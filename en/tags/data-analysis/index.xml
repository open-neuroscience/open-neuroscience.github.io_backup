<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Analysis | Open Neuroscience</title>
    <link>https://open-neuroscience.com/en/tags/data-analysis/</link>
      <atom:link href="https://open-neuroscience.com/en/tags/data-analysis/index.xml" rel="self" type="application/rss+xml" />
    <description>Data Analysis</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>CC BY SA 4.0</copyright><lastBuildDate>Sat, 26 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://open-neuroscience.com/images/icon_hu98f7bdece981985d79e430785ac9bc37_26395_512x512_fill_lanczos_center_2.png</url>
      <title>Data Analysis</title>
      <link>https://open-neuroscience.com/en/tags/data-analysis/</link>
    </image>
    
    <item>
      <title>DIPY</title>
      <link>https://open-neuroscience.com/en/post/dipy/</link>
      <pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/dipy/</guid>
      <description>&lt;p&gt;DIPY is the paragon 3D/4D+ imaging library in Python. Contains generic methods for spatial normalization, signal processing, machine learning, statistical analysis and visualization of medical images. Additionally, it contains specialized methods for computational anatomy including diffusion, perfusion and structural imaging.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/dipy/dipy/graphs/contributors&#34;&gt;https://github.com/dipy/dipy/graphs/contributors&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://dipy.org&#34;&gt;https://dipy.org&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCHnEuCRDGFOR5cfEo0nD3pw&#34;&gt;https://www.youtube.com/channel/UCHnEuCRDGFOR5cfEo0nD3pw&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
anonymous&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Brainglobe atlas API</title>
      <link>https://open-neuroscience.com/en/post/brainglobe_atlas_api/</link>
      <pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/brainglobe_atlas_api/</guid>
      <description>&lt;p&gt;Many excellent brain atlases exist for different species. Some of them have an API (application programming interface) to allow users to interact with the data programmatically (e.g. the excellent Allen Mouse Brain Atlas), but many do not, and there is no consistent way to process data from multiple sources.&lt;/p&gt;
&lt;p&gt;The brainglobe atlas API (BG-AtlasAPI) deals with this problem by providing a common interface for programmers to download and process atlas data from multiple sources.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Adam Tyson; Federico Claudi; Luigi Petrucco&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/brainglobe/bg-atlasapi&#34;&gt;https://github.com/brainglobe/bg-atlasapi&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Adam Tyson&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>brainrender</title>
      <link>https://open-neuroscience.com/en/post/brainrender/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/brainrender/</guid>
      <description>&lt;p&gt;brainrender is a python package for the visualization of three dimensional neuro-anatomical data. It can be used to render data from publicly available data set (e.g. Allen Brain atlas) as well as user generated experimental data. The goal of brainrender is to facilitate the exploration and dissemination of neuro-anatomical data by providing a user-friendly platform to create high-quality 3D renderings.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Federico Claudi&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/BrancoLab/BrainRender&#34;&gt;https://github.com/BrancoLab/BrainRender&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Federico Claudi&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>JASP</title>
      <link>https://open-neuroscience.com/en/post/jasp/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/jasp/</guid>
      <description>&lt;p&gt;JASP is a cross-platform statistical software program with a state-of-the-art graphical user interface. The JASP interface allows you to conduct statistical analyses in seconds, and without having to learn programming or risking a programming mistake. JASP is open-source and free of charge, and we provide it as a service to the community. JASP is statistically inclusive as it offers both frequentist and Bayesian analysis methods.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;The JASP Team&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://jasp-stats.org/&#34;&gt;https://jasp-stats.org/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=HxqB7CUA-XI&#34;&gt;https://www.youtube.com/watch?v=HxqB7CUA-XI&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
EJ Wagenmakers&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Neuroimaging Informatics Tools and Resources Collaboratory (NITRC)</title>
      <link>https://open-neuroscience.com/en/post/neuroimaging_informatics_tools_and_resources_collaboratory_nitrc/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/neuroimaging_informatics_tools_and_resources_collaboratory_nitrc/</guid>
      <description>&lt;p&gt;NeuroImaging Tools &amp;amp; Resources Collaboratory is an award-winning free web-based resource that offers comprehensive information on an ever expanding scope of neuroinformatics software and data. Since debuting in 2007, NITRC has helped the neuroscience community make further discoveries using software and data produced from research that used to end up lost or disregarded.&lt;/p&gt;
&lt;p&gt;NITRC also provides free access to data and enables pay-per-use cloud-based access to unlimited computing power, enabling worldwide scientific collaboration with minimal startup and cost. NITRC’s scientific focus includes: MR, PET/SPECT, CT, EEG/MEG, optical imaging, clinical neuroimaging, computational neuroscience, and imaging genomics software tools, data, and computational resources.&lt;/p&gt;
&lt;p&gt;With NITRC and its components—the Resources Registry (NITRC-R), Image Repository (NITRC-IR), and Computational Environment (NITRC-CE)—a researcher can obtain pilot or proof-of-concept data to validate a hypothesis for just a few dollars.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;NITRC Development Team&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.nitrc.org&#34;&gt;http://www.nitrc.org&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
David Kennedy&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>ReproNim: A Center for Reproducible Neuroimaging Computation</title>
      <link>https://open-neuroscience.com/en/post/repronim_a_center_for_reproducible_neuroimaging_computation/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/repronim_a_center_for_reproducible_neuroimaging_computation/</guid>
      <description>&lt;p&gt;ReproNim&amp;rsquo;s goal is to improve the reproducibility of neuroimaging science and extend the value of our national investment in neuroimaging research, while making the process easier and more efficient for investigators.&lt;/p&gt;
&lt;p&gt;ReproNim delivers a reproducible analysis framework comprised of components that include: 1) data and software discovery; 2) implementation of standardized description of data, results and workflows; 3) development of execution options that facilitates operation in all computational environments; 4)
provision of training and education to the community.&lt;/p&gt;
&lt;p&gt;All components of the framework are intended to foster continued use and development of the reproducible and generalizable framework in neuroimaging research.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;The ReproNim Development Team&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ReproNim&#34;&gt;https://github.com/ReproNim&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
David Kennedy&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Simple Behavioral Analysis (SimBA)</title>
      <link>https://open-neuroscience.com/en/post/simple_behavioral_analysis_simba/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/simple_behavioral_analysis_simba/</guid>
      <description>&lt;p&gt;Several excellent computational frameworks exist that enable high-throughput and consistent tracking of freely moving unmarked animals. SimBA introduce and distribute a plug-and play pipeline that enables users to use these pose-estimation approaches in combination with behavioral annotation for the generation of supervised machine-learning behavioral predictive classifiers.&lt;/p&gt;
&lt;p&gt;SimBA was developed for the analysis of complex social behaviors, but includes the flexibility for users to generate predictive classifiers across other behavioral modalities with minimal effort and no specialized computational background.&lt;/p&gt;
&lt;p&gt;SimBA has a variety of extended functions for large scale batch video pre-processing, generating descriptive statistics from movement features, and interactive modules for user-defined regions of interest and visualizing classification probabilities and movement patterns.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Simon Nilsson: Jia Jie Chhong; Sophia Hwang; Nastacia Goodwin; Sam A Golden&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/sgoldenlab/simba&#34;&gt;https://github.com/sgoldenlab/simba&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Frq6mMcaHBc&amp;amp;list=PLi5Vwf0hhy1R6NDQJ3U28MOUJPfl2YWYl&amp;amp;index=2&amp;amp;t=0s&#34;&gt;https://www.youtube.com/watch?v=Frq6mMcaHBc&amp;amp;list=PLi5Vwf0hhy1R6NDQJ3U28MOUJPfl2YWYl&amp;amp;index=2&amp;amp;t=0s&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Simon Nilsson&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>DataJoint</title>
      <link>https://open-neuroscience.com/en/post/datajoint/</link>
      <pubDate>Sun, 19 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/datajoint/</guid>
      <description>&lt;p&gt;DataJoint is an open-source library for managing and sharing scientific data pipelines in Python and Matlab.&lt;/p&gt;
&lt;p&gt;DataJoint allows creating and sharing computational data pipelines, which are defined as databases and analysis code for executing steps of activities for data collection and analysis. For example, many neuroscience studies are organized around DataJoint pipelines that start with basic information about the experiment, then ingest acquired data, and then perform processing, analysis, and visualization of results. The entire pipeline is diagrammed as a graph where each node is a table in the database with a corresponding class in the programming language; together they define the data structure and computations.&lt;/p&gt;
&lt;p&gt;DataJoint key features include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;access to shared data pipelines in a relational database (MySQL-compatible) from Python, Matlab, or both.&lt;/li&gt;
&lt;li&gt;data integrity and consistency based founded on the relational data model and transactions&lt;/li&gt;
&lt;li&gt;an intuitive data definition language for pipeline design&lt;/li&gt;
&lt;li&gt;a diagramming notation to visualize data structure and dependencies&lt;/li&gt;
&lt;li&gt;a serialization framework: storing large numerical arrays and other scientific data in a language-independent way&lt;/li&gt;
&lt;li&gt;a flexible query language to retrieve precise cross-sections of data in a desired format&lt;/li&gt;
&lt;li&gt;automated  execution of computational jobs, with built-in job management for distributed computing&lt;/li&gt;
&lt;li&gt;managed storage of large data objects outside the database&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Dimitri Yatsenko; Edgar Walker; Fabian Sinz; Christopher Turner; Raphael Guzman&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://datajoint.io&#34;&gt;https://datajoint.io&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Dimitri Yatsenko&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>cellfinder</title>
      <link>https://open-neuroscience.com/en/post/cellfinder/</link>
      <pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/cellfinder/</guid>
      <description>&lt;p&gt;cellfinder is software from the Margrie Lab at the Sainsbury Wellcome Centre for automated 3D cell detection and registration of whole-brain images (e.g. serial two-photon or lightsheet imaging).&lt;/p&gt;
&lt;p&gt;It’s a work in progress, but cellfinder can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detect labelled cells in 3D in whole-brain images (many hundreds of GB)&lt;/li&gt;
&lt;li&gt;Register the image to an atlas (such as the Allen Mouse Brain Atlas)&lt;/li&gt;
&lt;li&gt;Segment the brain based on the reference atlas&lt;/li&gt;
&lt;li&gt;Calculate the volume of each brain area, and the number of labelled cells within it&lt;/li&gt;
&lt;li&gt;Transform everything into standard space for analysis and visualisation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Adam Tyson&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/SainsburyWellcomeCentre/cellfinder&#34;&gt;https://github.com/SainsburyWellcomeCentre/cellfinder&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Adam Tyson&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>DeepLabCut</title>
      <link>https://open-neuroscience.com/en/post/deeplabcut/</link>
      <pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/deeplabcut/</guid>
      <description>&lt;p&gt;DeepLabCut™ is an efficient method for 3D markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results (i.e. you can match human labeling accuracy) with minimal training data (typically 50-200 frames). We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors.&lt;/p&gt;
&lt;p&gt;The package is open source, fast, robust, and can be used to compute 3D pose estimates. Please see the original paper and the latest work below.  This package is collaboratively developed by the Mathis Group &amp;amp; Mathis Lab at EPFL/Harvard.&lt;/p&gt;
&lt;p&gt;The code is freely available and easy to install in a few clicks with Anaconda (and pypi). Please see instructions on deeplabcut.org. We also provide a very easy to use GUI interface, and a step-by-step user guide!&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Mackenzie Mathis, Alexander Mathis &amp;amp; contributors&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://deeplabcut.org/&#34;&gt;http://deeplabcut.org/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC2HEbWpC_1v6i9RnDMy-dfA&#34;&gt;https://www.youtube.com/channel/UC2HEbWpC_1v6i9RnDMy-dfA&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Mackenzie Mathis&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>MNE-Python</title>
      <link>https://open-neuroscience.com/en/post/mne-python/</link>
      <pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/mne-python/</guid>
      <description>&lt;p&gt;MNE is a software package for processing electrophysiological signals primarily from magnetoencephalographic (MEG) and electroencephalographic (EEG) recordings, and more recently sEEG, ECoG and fNIRS. It provides a comprehensive solution for data preprocessing, forward modeling (with boundary element models), distributed source imaging, time–frequency analysis, non-parametric multivariate statistics, multivariate pattern analysis, and connectivity estimation. Importantly, this package allows all of these analyses to be applied in both sensor or source space. MNE is developed by an international team, with particular care for computational efficiency, code quality, and readability, as well as the common goal of facilitating reproducibility in neuroscience.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Alexandre Gramfort;Eric Larson;Denis Engemann;Daniel Strohmeier;Christian Brodbeck;Roman Goj;Mainak Jas;Teon Brooks;Lauri Parkkonen;Matti Hämäläinen;Jaakko Leppakangas;Jona Sassenhagen;Jean-Rémi King;Daniel McCloy;Marijn van Vliet;Clemens Brunner;Chris Holdgraf;Martin Luessi;Joan Massich;Guillaume Favelier;Andrew R. Dykstra;Mikolaj Magnuski;Stefan Appelhoff;Britta Westner;Richard Höchenberger;Robert Luke;Luke Bloy;Thomas Hartmann;Olaf Hauk;Adam Li&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://mne.tools&#34;&gt;https://mne.tools&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Anonymous&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Nilearn</title>
      <link>https://open-neuroscience.com/en/post/nilearn/</link>
      <pubDate>Sat, 11 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/nilearn/</guid>
      <description>&lt;p&gt;Nilearn is a Python module for fast and easy statistical learning on NeuroImaging data. It leverages the scikit-learn Python toolbox for multivariate statistics with applications such as predictive modelling, classification, decoding, or connectivity analysis.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/orgs/nilearn/people&#34;&gt;https://github.com/orgs/nilearn/people&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://nilearn.github.io/&#34;&gt;http://nilearn.github.io/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Anonymous&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>DeepLabStream</title>
      <link>https://open-neuroscience.com/en/post/deeplabstream/</link>
      <pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/deeplabstream/</guid>
      <description>&lt;p&gt;DeepLabStream is a python based multi-purpose tool that enables the realtime tracking of animals and manipulation of experiments. Our toolbox is adapted from the previously published DeepLabCut (Mathis et al., 2018) and expands on its core capabilities. DeepLabStreams core feature is the real-time analysis using any type of camera-based video stream (incl. multiple streams). Building onto that, we designed a full experimental closed-loop toolkit. It enables running experimental protocols that are dependent on a constant stream of bodypart positions and feedback activation of several input/output devices. It&amp;rsquo;s capabilities range from simple region of interest (ROI) based triggers to headdirection or behavior dependent stimulation.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Schwarz Neurocon Lab&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/SchwarzNeuroconLab/DeepLabStream&#34;&gt;https://github.com/SchwarzNeuroconLab/DeepLabStream&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>SpikeInterface</title>
      <link>https://open-neuroscience.com/en/post/spikeinterface/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/spikeinterface/</guid>
      <description>&lt;p&gt;SpikeInterface is a unified Python framework for spike sorting. With its high-level API, it is designed to be accessible and easy to use, allowing users to build full analysis pipelines for spike sorting (reading-writing (IO) / preprocessing / spike sorting / postprocessing / validation / curation / comparison / visualization) with a few lines of code.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Alessio Buccino*; Cole Hurwitz*; Samuel Garcia; Jeremy Magland; Josh Siegle; Matthias Hennig&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/SpikeInterface/spikeinterface&#34;&gt;https://github.com/SpikeInterface/spikeinterface&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=nWJGwFB7oII&#34;&gt;https://www.youtube.com/watch?v=nWJGwFB7oII&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Alessio Buccino&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>neuTube</title>
      <link>https://open-neuroscience.com/en/post/neutube/</link>
      <pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/neutube/</guid>
      <description>&lt;p&gt;neuTube is an open source software for reconstructing neurons from fluorescence microscope images. It is easy to use and improves the efficiency of reconstructing neuron structures accurately. The framework combines 2D/3D visualization, semi-automated tracing algorithms, and flexible editing options that simplify the task of neuron reconstruction.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Ting Zhao&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.neutracing.com/&#34;&gt;https://www.neutracing.com/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Miguel Fernandes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Deep Cinac</title>
      <link>https://open-neuroscience.com/en/post/deep_cinac/</link>
      <pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/deep_cinac/</guid>
      <description>&lt;p&gt;Two-photon calcium imaging is now widely used to infer neuronal dynamics from changes in fluorescence of an indicator. However, state of the art computational tools are not optimized for the reliable detection of fluorescence transients from highly synchronous neurons located in densely packed regions such as the CA1 pyramidal layer of the hippocampus during early postnatal  stages  of  development.  Indeed,the  latest  analytical  tools  often  lack  proper benchmark  measurements.  To  meet  this  challenge,  we  first  developed  a  graphical  user interface allowing for a precise manual detection of all calcium transients from imaged neurons based on the visualization of the calcium imaging movie. Then, we analyzed the movies using a convolutional neural network with an attention process and a bidirectional long-short term memory network. This method is able to reach human performance and offers a better F1 score (harmonic mean of sensitivity and precision) than CaImAn to infer neural activity in the developingCA1 without any user intervention. It also enables automatically identifying activity originating from GABAergic neurons. Overall, DeepCINAC offers a simple, fast and flexible open-source toolbox for processing a wide variety of calcium imaging datasets while providing the tools to evaluate its performance.&lt;/p&gt;
&lt;p&gt;See full text at &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/803726v2.full.pdf&#34;&gt;https://www.biorxiv.org/content/10.1101/803726v2.full.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Julien Denis&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://gitlab.com/cossartlab/deepcinac&#34;&gt;https://gitlab.com/cossartlab/deepcinac&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>VocalMat</title>
      <link>https://open-neuroscience.com/en/post/vocalmat/</link>
      <pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/vocalmat/</guid>
      <description>&lt;p&gt;Mice emit ultrasonic vocalizations (USV) to transmit socially-relevant information. To detect and classify these USVs, here we describe the development of VocalMat. VocalMat is a software that uses image-processing and differential geometry approaches to detect USVs in audio files, eliminating the need for user-defined parameter tuning. VocalMat also uses computational vision and machine learning methods to classify USVs into distinct categories. In a dataset of &amp;gt;4,000 USVs emitted by mice, VocalMat detected more than &amp;gt;98% of the USVs and accurately classified ≈86% of USVs when considering the most likely label out of 11 different USV types. We then used Diffusion Maps and Manifold Alignment to analyze the probability distribution of USV classification among different experimental groups, providing a robust method to quantify and qualify the vocal repertoire of mice. Thus, VocalMat allows accurate and highly quantitative analysis of USVs, opening the opportunity for detailed and high-throughput analysis of this behavior.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Antonio H. O. Fonseca, Gustavo M. Santana, Sergio Bampi, Marcelo O Dietrich&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.dietrich-lab.org/vocalmat&#34;&gt;https://www.dietrich-lab.org/vocalmat&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>3D Slicer</title>
      <link>https://open-neuroscience.com/en/post/3d_slicer/</link>
      <pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/3d_slicer/</guid>
      <description>&lt;p&gt;3D Slicer is a software for medical image informatics, image processing, and three-dimensional visualization. It’s extremely powerful and versatile with plenty of different options. It is a great tool for volume rendering, registration, interactive segmentation of images and even offers the possibility of running Python scripts thought an embedded Python interpreter.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Ron Kikinis&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Slicer/Slicer&#34;&gt;https://github.com/Slicer/Slicer&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Miguel Fernandes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Colaboratory</title>
      <link>https://open-neuroscience.com/en/post/colaboratory/</link>
      <pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/colaboratory/</guid>
      <description>&lt;p&gt;Colaboratory is a free Jupyter notebook environment that runs in the cloud. Your notebooks get stored on Google Drive. The great advantage is that you don’t have to install anything (however, for some features you need a Google account) on your system to use it. You can perform specific computations during data analysis with pre-installed Python libraries and gives you access to accelerated hardware for free (e.g. GPUs and TPUs).&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Google&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/notebooks/intro.ipynb&#34;&gt;https://colab.research.google.com/notebooks/intro.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Miguel Fernandes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Suite2P</title>
      <link>https://open-neuroscience.com/en/post/suite2p/</link>
      <pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/suite2p/</guid>
      <description>&lt;p&gt;Suite2P is a very modular imaging processing pipeline written in Python which allows you to perform registration of raw data movies, automatic cell detection, extraction of calcium traces and infers spike times. It is a very fast and accurate tool and can work on standard workstations. It also includes a visualization graphical user interface (GUI) that facilitates analysis and manual curation of the cell detection algorithm.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Carsen Stringer and Marius Pachitariu&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://mouseland.github.io/suite2p/_build/html/index.html&#34;&gt;https://mouseland.github.io/suite2p/_build/html/index.html&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Miguel Fernandes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>NiBabel</title>
      <link>https://open-neuroscience.com/en/post/nibabel/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/nibabel/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;http://nipy.org/nibabel/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NiBabel&lt;/a&gt; is a python package, under the NiPy project, that aims at unifying the process of opening different medical and neuroimaging file formats, including: 
&lt;a href=&#34;http://www.grahamwideman.com/gw/brain/analyze/formatdoc.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ANALYZE&lt;/a&gt;,
&lt;a href=&#34;http://www.nitrc.org/projects/gifti&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GIFTI&lt;/a&gt;, 
&lt;a href=&#34;http://nifti.nimh.nih.gov/nifti-1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NIfTI1&lt;/a&gt;, 
&lt;a href=&#34;http://en.wikibooks.org/wiki/MINC/Reference/MINC2.0_File_Format_Reference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MINC&lt;/a&gt;, 
&lt;a href=&#34;http://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/MghFormat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MGH&lt;/a&gt; and 
&lt;a href=&#34;http://xmedcon.sourceforge.net/Docs/Ecat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ECAT&lt;/a&gt; as well as PAR/REC. The package is also able to read and write 
&lt;a href=&#34;http://surfer.nmr.mgh.harvard.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Freesurfer&lt;/a&gt; format.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;img src=&#34;https://nipy.org/nibabel/_static/nipy-logo-bg-138x120.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Nipy</title>
      <link>https://open-neuroscience.com/en/post/nipy/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/nipy/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://nipy.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NiPy&lt;/a&gt; is an effort to make brain imaging research easier and more clear. This is implemented by providing a series of software that deal with file IO, analysis, and interfaces &amp;amp; pipelines.&lt;/p&gt;
&lt;p&gt;The software present up to now (05/05/2020)&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://nipy.org/packages/nipype/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nipype&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://nipy.org/packages/dipy/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;diPy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://nipy.org/packages/mindboggle/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mindboggle&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://nipy.org/packages/nibabel/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NiBabel&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://nipy.org/packages/sdm/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scitran SDM&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://nipy.org/packages/nipy/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nipy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://nipy.org/packages/nitime/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nitime&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://nipy.org/packages/popeye/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;popeye&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://nipy.org/packages/nilearn/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;niLearn&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://nipy.org/packages/pymvpa/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyMVPA&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://nipy.org/packages/mne/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MNE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://nipy.org/packages/niwidgets/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;niwidgets&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BrainBrowser</title>
      <link>https://open-neuroscience.com/en/post/brainbrowser/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/brainbrowser/</guid>
      <description>&lt;p&gt;BrainBrowser is a collection of open source, web-based 3D data visualization tools, mainly for neuroimaging studies. It is built using open technologies such as WebGL and HTML5. It allows exploration of cortical surface models (MNI and Wavefront OBJ, as well as FreeSurfer ASCII surface format) and volumetric MINC data. This project is currently maintained by 
&lt;a href=&#34;http://www.tareksherif.ca/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tarek Sherif&lt;/a&gt; at McGill University, and the source code is available on 
&lt;a href=&#34;https://github.com/aces/brainbrowser&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can find more info on the 
&lt;a href=&#34;https://brainbrowser.cbrain.mcgill.ca/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fiji</title>
      <link>https://open-neuroscience.com/en/post/fiji/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/fiji/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;http://fiji.sc/Fiji&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fiji&lt;/a&gt; is a distribution of ImageJ. The idea of the developers is to make the life of scientists easier by bundling ImageJ with nicely organised plugins and auto update function.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fiji compares to ImageJ as Ubuntu compares to Linux.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;
&lt;a href=&#34;http://openeuroscience.wordpress.com/software/imagej/&#34; title=&#34;ImageJ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt; &lt;img src=&#34;https://i1.wp.com/rsbweb.nih.gov/ij/images/imagej-logo.gif?w=800&#34; alt=&#34;imagej logo&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpace</title>
      <link>https://open-neuroscience.com/en/post/pyspace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/pyspace/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://pyspace.github.io/pyspace/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PySpace&lt;/a&gt; is a signal processing and classificiation environment for Python.&lt;/p&gt;
&lt;p&gt;Modular software for processing of large data streams that has been specifically designed to enable distributed execution and empirical evaluation of signal processing chains. Various signal processing algorithms are available within the software, from finite impulse response filters over data-dependent spatial filters (e.g. CSP, xDAWN) to established classifiers (e.g. SVM, LDA). pySPACE incorporates the concept of node and node chains of the Modular Toolkit for Data Processing (MDP) framework.&lt;/p&gt;
&lt;p&gt;A paper about PySpace can be found 
&lt;a href=&#34;http://journal.frontiersin.org/article/10.3389/fninf.2013.00040/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tensor Flow</title>
      <link>https://open-neuroscience.com/en/post/tensor-flow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/tensor-flow/</guid>
      <description>&lt;p&gt;Google has packaged their deeplearning machine learning tools and made it open source. The project is called tensorflow, and is available 
&lt;a href=&#34;http://www.tensorflow.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here.&lt;/a&gt; Some nice tutorials on the website, so that with a bit of patience, people can start to deep their toes into machine learning!&lt;/p&gt;
&lt;p&gt;Be sure to check the video below for more details!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;embed-youtube&#34; style=&#34;text-align:center; display: block;&#34;&gt;&lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
