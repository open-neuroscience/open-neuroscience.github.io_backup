<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Open Neuroscience</title>
    <link>https://open-neuroscience.com/en/authors/admin/</link>
      <atom:link href="https://open-neuroscience.com/en/authors/admin/index.xml" rel="self" type="application/rss+xml" />
    <description>Open Neuroscience</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>CC BY SA 4.0</copyright><lastBuildDate>Wed, 30 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://open-neuroscience.com/media/openneuroscience_logo_dark.svg</url>
      <title>Open Neuroscience</title>
      <link>https://open-neuroscience.com/en/authors/admin/</link>
    </image>
    
    <item>
      <title>FastTrack</title>
      <link>https://open-neuroscience.com/en/post/fasttrack/</link>
      <pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/fasttrack/</guid>
      <description>&lt;p&gt;FastTrack is an open-source cross-platform tracking software. Easy to install and easy to use, it can track a large variety of systems from active particles to animals, with a known or unknown number of objects. It can process movies from any quality on low-end to high-end computers.&lt;/p&gt;
&lt;p&gt;Two main features are implemented in the software:
- A fast and automatic tracking algorithm that can detect and track objects, conserving the objects&#39; identities across the video recording.
- A manual tool to review the tracking where errors can be corrected rapidly and easily to achieve 100% accuracy with a minimum of efforts.&lt;/p&gt;
&lt;p&gt;FastTrack do not require coding abilities to be used. A developer documentation is available for users who want to embed FastTrack tracking algorithm directly inside their projects.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Benjamin Gallois&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/FastTrackOrg/FastTrack&#34;&gt;https://github.com/FastTrackOrg/FastTrack&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.fasttrack.sh/UserManual/docs/assets/example_vid.webm&#34;&gt;http://www.fasttrack.sh/UserManual/docs/assets/example_vid.webm&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Benjamin Gallois&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Open Source Brain</title>
      <link>https://open-neuroscience.com/en/post/open_source_brain/</link>
      <pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/open_source_brain/</guid>
      <description>&lt;p&gt;Open Source Brain, a platform for sharing, viewing, analyzing, and simulating standardized models from different brain regions and species.&lt;/p&gt;
&lt;p&gt;Model structure and parameters can be automatically visualized and their dynamical properties explored through browser-based simulations.&lt;/p&gt;
&lt;p&gt;Infrastructure and tools for collaborative interaction, development, and testing are also provided.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Open Source Brain contributors&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.opensourcebrain.org/&#34;&gt;http://www.opensourcebrain.org/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Ankur Sinha (@ OSB/University College London)&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>MorphoPy: A python package for feature extraction of neural morphologies</title>
      <link>https://open-neuroscience.com/en/post/morphopy_a_python_package_for_feature_extraction_of_neural_morphologies/</link>
      <pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/morphopy_a_python_package_for_feature_extraction_of_neural_morphologies/</guid>
      <description>&lt;p&gt;MorphoPy is an open software package written in Python3 that allows for visualization and processing of morphological reconstructions of neural data. It has been created to facilitate the translation from morphology graphs into descriptive features like density maps, morphometric statistics, and persistence diagrams for down-stream exploration and statistical analysis.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Sophie Laturnus; Adam von Daranyi; Ziwei Huang; Philipp Berens&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/berenslab/MorphoPy&#34;&gt;https://github.com/berenslab/MorphoPy&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Sophie Laturnus&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>OpenCitations</title>
      <link>https://open-neuroscience.com/en/post/opencitations/</link>
      <pubDate>Sun, 27 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/opencitations/</guid>
      <description>&lt;p&gt;OpenCitations is an independent infrastructure organization for open scholarship dedicated to the publication of open bibliographic and citation data by the use of Semantic Web (Linked Data) technologies. It is also engaged in advocacy for open citations, particularly by organizing the Workshops for Open Citations and Scholarly Metadata, and in its role as a key founding member of the Initiative for Open Citations (I4OC) and the Initiative for Open Abstracts (I4OA).&lt;/p&gt;
&lt;p&gt;OpenCitations espouses fully the founding principles of Open Science. It complies with the FAIR data principles by Force11 that data should be findable, accessible, interoperable and re-usable, and it complies with the recommendations of I4OC that citation data in particular should be structured, separable, and open. On the latter topic, OpenCitations has recently published a formal definition of an Open Citation, and has launched a system for globally unique and persistent identifiers (PIDs) for bibliographic citations – Open Citation Identifiers (OCIs).&lt;/p&gt;
&lt;p&gt;The following publication is the canonical publication describing OpenCitations itself, as an infrastructure organization for open scholarship. The article describes OpenCitations and its datasets, tools, services and activities.&lt;/p&gt;
&lt;p&gt;Silvio Peroni, David Shotton (2020). OpenCitations, an infrastructure organization for open scholarship. Quantitative Science Studies, 1(1): 428-444. &lt;a href=&#34;https://doi.org/10.1162/qss_a_00023&#34;&gt;https://doi.org/10.1162/qss_a_00023&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The OpenCitations Data Model (OCDM) is the metadata model used for the data stored in all the OpenCitations&#39; datasets, described in&lt;/p&gt;
&lt;p&gt;Marilena Daquino, Silvio Peroni, David Shotton (2020). The OpenCitations Data Model. Figshare. &lt;a href=&#34;https://doi.org/10.6084/m9.figshare.3443876&#34;&gt;https://doi.org/10.6084/m9.figshare.3443876&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The largest dataset created by OpenCitations is COCI, the OpenCitations Index of Crossref open DOI-to-DOI citations, an RDF dataset containing details of all the citations that are specified by the open references to DOI-identified works present in Crossref. COCI does not index Crossref references that are not open, nor Crossref open references to entities that lack DOIs. The citations available in COCI are treated as first-class data entities, with accompanying properties including the citations timespan, modelled according to the OpenCitations Data Model.&lt;/p&gt;
&lt;p&gt;Currently, COCI contains information concerning 733,367,140 citations, and 59,455,917 bibliographic resources.  COCI was most recently updated on 6 September 2020.&lt;/p&gt;
&lt;p&gt;For an in-depth description of COCI, see:&lt;/p&gt;
&lt;p&gt;Ivan Heibi, Silvio Peroni, David Shotton (2019). Software review: COCI, the OpenCitations Index of Crossref open DOI-to-DOI citations. Scientometrics, 121 (2): 1213-1228. &lt;a href=&#34;https://doi.org/10.1007/s11192-019-03217-6&#34;&gt;https://doi.org/10.1007/s11192-019-03217-6&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;David Shotton; Silvio Peroni&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://opencitations.net/&#34;&gt;http://opencitations.net/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-bCPS2iIdCc&amp;amp;feature=youtu.be&#34;&gt;https://www.youtube.com/watch?v=-bCPS2iIdCc&amp;amp;feature=youtu.be&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
David Shotton&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>DIPY</title>
      <link>https://open-neuroscience.com/en/post/dipy/</link>
      <pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/dipy/</guid>
      <description>&lt;p&gt;DIPY is the paragon 3D/4D+ imaging library in Python. Contains generic methods for spatial normalization, signal processing, machine learning, statistical analysis and visualization of medical images. Additionally, it contains specialized methods for computational anatomy including diffusion, perfusion and structural imaging.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/dipy/dipy/graphs/contributors&#34;&gt;https://github.com/dipy/dipy/graphs/contributors&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://dipy.org&#34;&gt;https://dipy.org&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCHnEuCRDGFOR5cfEo0nD3pw&#34;&gt;https://www.youtube.com/channel/UCHnEuCRDGFOR5cfEo0nD3pw&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
anonymous&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>NeuroFedora</title>
      <link>https://open-neuroscience.com/en/post/neurofedora/</link>
      <pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/neurofedora/</guid>
      <description>&lt;p&gt;NeuroFedora is an initiative to provide a ready to use Fedora Linux based Free/Open source software platform for neuroscience. We believe that similar to Free software, science should be free for all to use, share, modify, and study. The use of Free software also aids reproducibility, data sharing, and collaboration in the research community. By making the tools used in the scientific process easier to use, NeuroFedora aims to take a step to enable this ideal.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;NeuroFedora volunteers @ the Fedora project&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://neuro.fedoraproject.org&#34;&gt;https://neuro.fedoraproject.org&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Ankur Sinha (NeuroFedora SIG member)&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Brainglobe atlas API</title>
      <link>https://open-neuroscience.com/en/post/brainglobe_atlas_api/</link>
      <pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/brainglobe_atlas_api/</guid>
      <description>&lt;p&gt;Many excellent brain atlases exist for different species. Some of them have an API (application programming interface) to allow users to interact with the data programmatically (e.g. the excellent Allen Mouse Brain Atlas), but many do not, and there is no consistent way to process data from multiple sources.&lt;/p&gt;
&lt;p&gt;The brainglobe atlas API (BG-AtlasAPI) deals with this problem by providing a common interface for programmers to download and process atlas data from multiple sources.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Adam Tyson; Federico Claudi; Luigi Petrucco&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/brainglobe/bg-atlasapi&#34;&gt;https://github.com/brainglobe/bg-atlasapi&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Adam Tyson&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>culture_shock</title>
      <link>https://open-neuroscience.com/en/post/culture_shock/</link>
      <pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/culture_shock/</guid>
      <description>&lt;p&gt;Culture Shock is an open-source electroporator that was developed
through internet based collaboration, starting on the DIYbio Google
Group. It is an evolution on the traditional capacitive discharge
circuit topology, instead using pulsed induction to enable a
programmable waveform as well as reduce the size, weight, and cost of
the equipment. With all these benefits, we hope to reduce the burden
of laboratory consumables for DNA transformation and electrofusion
procedures, where chemical supplies are currently relied on. The added
benefit of programmability allows many cell types to be manipulated by
altering the voltage level, or even giving the voltage profile a
particular shape.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;John Griessen; Nathan McCorkle; Bryan Bishop&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kanzure/culture_shock&#34;&gt;https://github.com/kanzure/culture_shock&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Nathan McCorkle&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>WholeBrain</title>
      <link>https://open-neuroscience.com/en/post/wholebrain/</link>
      <pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/wholebrain/</guid>
      <description>&lt;p&gt;WholeBrain is a software to create anatomical maps. With a code base in C/C++ with wrappers to R and JavaScript/WASM.&lt;/p&gt;
&lt;p&gt;The purpose of WholeBrain is to provide a user-friendly and efficient way for scientist with minimal knowledge of computers to create anatomical maps and integrate this information with behavioral and physiological data for sharing on the web.&lt;/p&gt;
&lt;p&gt;WholeBrain is conceived and created by Daniel Fürth, CSHL.&lt;/p&gt;
&lt;p&gt;Quick question about something you think can be resolved quite fast? Then just go to the gitter room and chat with me: &lt;a href=&#34;https://gitter.im/tractatus/Lobby&#34;&gt;https://gitter.im/tractatus/Lobby&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Daniel Fürth&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/tractatus/wholebrain/&#34;&gt;https://github.com/tractatus/wholebrain/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Daniel Fürth&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>brainrender</title>
      <link>https://open-neuroscience.com/en/post/brainrender/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/brainrender/</guid>
      <description>&lt;p&gt;brainrender is a python package for the visualization of three dimensional neuro-anatomical data. It can be used to render data from publicly available data set (e.g. Allen Brain atlas) as well as user generated experimental data. The goal of brainrender is to facilitate the exploration and dissemination of neuro-anatomical data by providing a user-friendly platform to create high-quality 3D renderings.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Federico Claudi&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/BrancoLab/BrainRender&#34;&gt;https://github.com/BrancoLab/BrainRender&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Federico Claudi&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>JASP</title>
      <link>https://open-neuroscience.com/en/post/jasp/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/jasp/</guid>
      <description>&lt;p&gt;JASP is a cross-platform statistical software program with a state-of-the-art graphical user interface. The JASP interface allows you to conduct statistical analyses in seconds, and without having to learn programming or risking a programming mistake. JASP is open-source and free of charge, and we provide it as a service to the community. JASP is statistically inclusive as it offers both frequentist and Bayesian analysis methods.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;The JASP Team&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://jasp-stats.org/&#34;&gt;https://jasp-stats.org/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=HxqB7CUA-XI&#34;&gt;https://www.youtube.com/watch?v=HxqB7CUA-XI&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
EJ Wagenmakers&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>PiDose</title>
      <link>https://open-neuroscience.com/en/post/pidose/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/pidose/</guid>
      <description>&lt;p&gt;PiDose is an open-source tool for scientists performing drug administration experiments with mice. It allows for automated daily oral dosing of mice over long time periods (weeks to months) without the need for experimenter interaction and handling. To accomplish this, a small 3D-printed chamber is mounted adjacent to a regular mouse home-cage, with an opening in the cage to allow animals to freely access the chamber.&lt;/p&gt;
&lt;p&gt;The chamber is supported by a load cell, and does not contact the cage but sits directly next to the entrance opening. Prior to treatment, mice have a small RFID capsule implanted subcutaneously, and when they enter the chamber they are detected by an RFID reader. While the mouse is in the chamber, readings are taken from the load cell in order to determine the mouse&amp;rsquo;s bodyweight. At the opposite end of the chamber from the entrance, a nose-poke port accesses a spout which dispenses drops from two separate liquid reservoirs. This spout is wired to a capacitive touch sensor controller in order to detect licks, and delivers liquid drops in response to licking.&lt;/p&gt;
&lt;p&gt;Each day, an average weight is calculated for each mouse and a drug dosage is determined based on this. When a mouse licks at the spout it dispenses either regular drinking water or a drop of drug solution depending on if they have received their daily dosage or not. All components are controlled by a Python script running on a Raspberry Pi.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Cameron Woodard; Wissam Nasrallah; Bahram Samiei; Tim Murphy; Lynn Raymond&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://osf.io/rpyfm/&#34;&gt;https://osf.io/rpyfm/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Cameron Woodard&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>pyControl</title>
      <link>https://open-neuroscience.com/en/post/pycontrol/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/pycontrol/</guid>
      <description>&lt;p&gt;pyControl is a system of open source hardware and software for controlling behavioural experiments, built around the Micropython microcontroller.&lt;/p&gt;
&lt;p&gt;pyControl makes it easy to program complex behavioural tasks using a clean, intuitive, and flexible syntax for specifying tasks as state machines. User created task definition files, written in Python, run directly on the microcontroller, supported by pyControl framework code. This gives users the power and simplicity of Python for specifying task behaviour, while allowing advanced users low-level access to the microcontroller hardware.&lt;/p&gt;
&lt;p&gt;pyControl hardware consists of a breakout board and a set of devices such as nose-pokes, audio boards, LED drivers, rotary encoders and stepper motor controllers that are connected to the breakout board to create behavioural setups.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Thomas Akam&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://pycontrol.readthedocs.io&#34;&gt;https://pycontrol.readthedocs.io&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Thomas Akam&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>pyPhotometry</title>
      <link>https://open-neuroscience.com/en/post/pyphotometry/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/pyphotometry/</guid>
      <description>&lt;p&gt;pyPhotometry is system of open source, Python based, hardware and software for neuroscience fiber photometry data acquisition, consisting of an acquisition board and graphical user interface.&lt;/p&gt;
&lt;p&gt;pyPhotometry supports data aquisition from two analog and two digital inputs, and control of two LEDs via built in LED drivers with an adjustable 0-100mA output. The system supports time-division multiplexed illumination which allows fluoresence evoked by different excitation wavelengths to be independenly readout from a single photoreciever signal.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Thomas Akam&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://pyphotometry.readthedocs.io&#34;&gt;https://pyphotometry.readthedocs.io&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Thomas Akam&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>SLEAP</title>
      <link>https://open-neuroscience.com/en/post/sleap/</link>
      <pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/sleap/</guid>
      <description>&lt;p&gt;SLEAP (Social LEAP Estimates Animal Poses) is a multi-animal pose tracker based on deep learning. It is the successor of LEAP (Pereira et al., Nature Methods, 2019) and was designed to deal with the problem of tracking body landmarks of multiple freely interacting animals.&lt;/p&gt;
&lt;p&gt;Using deep learning, SLEAP trains neural network models from few user annotations to enable highly accurate body part localization, grouping and tracking. It supports multiple neural network architectures, including pretrained state-of-the-art models and lightweight customizable architectures. SLEAP has been used successfully to track mice, fruit flies, bees and other species of animals under a variety of experimental and imaging conditions.&lt;/p&gt;
&lt;p&gt;The software was designed to make it easy for users with no experience with deep learning through a fully featured GUI, as well as providing a rich functionality for advanced users seeking to develop a custom solution for their project. Tutorials and guides are available on our website (&lt;a href=&#34;https://sleap.ai&#34;&gt;https://sleap.ai&lt;/a&gt;) detailing steps for easy installation (Windows/Mac/Linux), labeling a new project, training on the locally or on the cloud, and tracking new data.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Talmo Pereira; Joshua Shaevitz; Mala Murthy&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://sleap.ai&#34;&gt;https://sleap.ai&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zwCf1pGnBUw&#34;&gt;https://www.youtube.com/watch?v=zwCf1pGnBUw&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Talmo Pereira&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Neuroimaging Informatics Tools and Resources Collaboratory (NITRC)</title>
      <link>https://open-neuroscience.com/en/post/neuroimaging_informatics_tools_and_resources_collaboratory_nitrc/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/neuroimaging_informatics_tools_and_resources_collaboratory_nitrc/</guid>
      <description>&lt;p&gt;NeuroImaging Tools &amp;amp; Resources Collaboratory is an award-winning free web-based resource that offers comprehensive information on an ever expanding scope of neuroinformatics software and data. Since debuting in 2007, NITRC has helped the neuroscience community make further discoveries using software and data produced from research that used to end up lost or disregarded.&lt;/p&gt;
&lt;p&gt;NITRC also provides free access to data and enables pay-per-use cloud-based access to unlimited computing power, enabling worldwide scientific collaboration with minimal startup and cost. NITRC’s scientific focus includes: MR, PET/SPECT, CT, EEG/MEG, optical imaging, clinical neuroimaging, computational neuroscience, and imaging genomics software tools, data, and computational resources.&lt;/p&gt;
&lt;p&gt;With NITRC and its components—the Resources Registry (NITRC-R), Image Repository (NITRC-IR), and Computational Environment (NITRC-CE)—a researcher can obtain pilot or proof-of-concept data to validate a hypothesis for just a few dollars.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;NITRC Development Team&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.nitrc.org&#34;&gt;http://www.nitrc.org&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
David Kennedy&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>PiVR</title>
      <link>https://open-neuroscience.com/en/post/pivr/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/pivr/</guid>
      <description>&lt;p&gt;PiVR is a system that allows experimenters to immerse small animals into virtual realities. The system tracks the position of the animal and presents light stimulation according to predefined rules, thus creating a virtual landscape in which the animal can behave. By using optogenetics, we have used PiVR to present fruit fly larvae with virtual olfactory realities, adult fruit flies with a virtual gustatory reality and zebrafish larvae with a virtual light gradient.&lt;/p&gt;
&lt;p&gt;PiVR operates at high temporal resolution (70Hz) with low latencies (&amp;lt;30 milliseconds) while being affordable (&amp;lt;US$500) and easy to build (&amp;lt;6 hours). Through extensive documentation (&lt;a href=&#34;http://www.PiVR.org&#34;&gt;www.PiVR.org&lt;/a&gt;), this tool was designed to be accessible to a wide public, from high school students to professional researchers studying systems neuroscience in academia.&lt;/p&gt;
&lt;p&gt;The project is open source (BSD-3) and the documented code written in the freely available programming language Python. We hope that PiVR will be adapted by advanced users for their particular needs, for example to create closed-loop experiments involving other sensory modalities (e.g., sound/vibration) through the use of PWM controllable devices. We envision PiVR to be used as the central module when creating virtual realities for a variety of sensory modalities. This ‘PiVR module’ takes care of detecting the animal and presenting the appropriate PWM signal that is then picked up by the PWM controllable device installed by the user, for example to produce a sound whenever an animal enters a pre-defined region.&lt;/p&gt;
&lt;p&gt;In short, PiVR is a powerful and affordable experimental platform allowing experimenters to create a wide array of virtual reality experiments. Our hope is that PiVR will be adapted by several labs to democratize closed-loop experiments and, by standardizing image quality and the animal detection algorithm, increase reproducibility.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;David Tadres; Matthieu Louis&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.PiVR.org&#34;&gt;http://www.PiVR.org&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=w5tIG6B6FWo&#34;&gt;https://www.youtube.com/watch?v=w5tIG6B6FWo&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
David Tadres&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>ReproNim: A Center for Reproducible Neuroimaging Computation</title>
      <link>https://open-neuroscience.com/en/post/repronim_a_center_for_reproducible_neuroimaging_computation/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/repronim_a_center_for_reproducible_neuroimaging_computation/</guid>
      <description>&lt;p&gt;ReproNim&amp;rsquo;s goal is to improve the reproducibility of neuroimaging science and extend the value of our national investment in neuroimaging research, while making the process easier and more efficient for investigators.&lt;/p&gt;
&lt;p&gt;ReproNim delivers a reproducible analysis framework comprised of components that include: 1) data and software discovery; 2) implementation of standardized description of data, results and workflows; 3) development of execution options that facilitates operation in all computational environments; 4)
provision of training and education to the community.&lt;/p&gt;
&lt;p&gt;All components of the framework are intended to foster continued use and development of the reproducible and generalizable framework in neuroimaging research.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;The ReproNim Development Team&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ReproNim&#34;&gt;https://github.com/ReproNim&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
David Kennedy&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Simple Behavioral Analysis (SimBA)</title>
      <link>https://open-neuroscience.com/en/post/simple_behavioral_analysis_simba/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/simple_behavioral_analysis_simba/</guid>
      <description>&lt;p&gt;Several excellent computational frameworks exist that enable high-throughput and consistent tracking of freely moving unmarked animals. SimBA introduce and distribute a plug-and play pipeline that enables users to use these pose-estimation approaches in combination with behavioral annotation for the generation of supervised machine-learning behavioral predictive classifiers.&lt;/p&gt;
&lt;p&gt;SimBA was developed for the analysis of complex social behaviors, but includes the flexibility for users to generate predictive classifiers across other behavioral modalities with minimal effort and no specialized computational background.&lt;/p&gt;
&lt;p&gt;SimBA has a variety of extended functions for large scale batch video pre-processing, generating descriptive statistics from movement features, and interactive modules for user-defined regions of interest and visualizing classification probabilities and movement patterns.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Simon Nilsson: Jia Jie Chhong; Sophia Hwang; Nastacia Goodwin; Sam A Golden&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/sgoldenlab/simba&#34;&gt;https://github.com/sgoldenlab/simba&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Frq6mMcaHBc&amp;amp;list=PLi5Vwf0hhy1R6NDQJ3U28MOUJPfl2YWYl&amp;amp;index=2&amp;amp;t=0s&#34;&gt;https://www.youtube.com/watch?v=Frq6mMcaHBc&amp;amp;list=PLi5Vwf0hhy1R6NDQJ3U28MOUJPfl2YWYl&amp;amp;index=2&amp;amp;t=0s&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Simon Nilsson&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>DataJoint</title>
      <link>https://open-neuroscience.com/en/post/datajoint/</link>
      <pubDate>Sun, 19 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/datajoint/</guid>
      <description>&lt;p&gt;DataJoint is an open-source library for managing and sharing scientific data pipelines in Python and Matlab.&lt;/p&gt;
&lt;p&gt;DataJoint allows creating and sharing computational data pipelines, which are defined as databases and analysis code for executing steps of activities for data collection and analysis. For example, many neuroscience studies are organized around DataJoint pipelines that start with basic information about the experiment, then ingest acquired data, and then perform processing, analysis, and visualization of results. The entire pipeline is diagrammed as a graph where each node is a table in the database with a corresponding class in the programming language; together they define the data structure and computations.&lt;/p&gt;
&lt;p&gt;DataJoint key features include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;access to shared data pipelines in a relational database (MySQL-compatible) from Python, Matlab, or both.&lt;/li&gt;
&lt;li&gt;data integrity and consistency based founded on the relational data model and transactions&lt;/li&gt;
&lt;li&gt;an intuitive data definition language for pipeline design&lt;/li&gt;
&lt;li&gt;a diagramming notation to visualize data structure and dependencies&lt;/li&gt;
&lt;li&gt;a serialization framework: storing large numerical arrays and other scientific data in a language-independent way&lt;/li&gt;
&lt;li&gt;a flexible query language to retrieve precise cross-sections of data in a desired format&lt;/li&gt;
&lt;li&gt;automated  execution of computational jobs, with built-in job management for distributed computing&lt;/li&gt;
&lt;li&gt;managed storage of large data objects outside the database&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Dimitri Yatsenko; Edgar Walker; Fabian Sinz; Christopher Turner; Raphael Guzman&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://datajoint.io&#34;&gt;https://datajoint.io&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Dimitri Yatsenko&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Neurodata Without Borders</title>
      <link>https://open-neuroscience.com/en/post/neurodata_without_borders/</link>
      <pubDate>Sun, 19 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/neurodata_without_borders/</guid>
      <description>&lt;p&gt;Neurodata Without Borders is a data standard for neurophysiology, providing neuroscientists with a common standard to share, archive, use, and build analysis tools for neurophysiology data. NWB is designed to store a variety of neurophysiology data, including data from intracellular and extracellular electrophysiology experiments, data from optical physiology experiments, and tracking and stimulus data.&lt;/p&gt;
&lt;p&gt;The NWB team consists of neuroscientists and software developers who recognize that adoption of a unified data format is an important step toward breaking down the barriers to data sharing in neuroscience.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Andrew Tritt; Ryan Ly; Ben Dichter; Oliver Ruebel&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nwb.org/&#34;&gt;https://www.nwb.org/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/vfQsMyl0HQI&#34;&gt;https://youtu.be/vfQsMyl0HQI&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Ben Dichter&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>OpenDrop Digital Microfluidics Platform</title>
      <link>https://open-neuroscience.com/en/post/opendrop_digital_microfluidics_platform/</link>
      <pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/opendrop_digital_microfluidics_platform/</guid>
      <description>&lt;p&gt;OpenDrop a modular, open source digital microfludics platform for research purposes. The device uses recent electro-wetting technology to control small droplets of liquids. Potential applications are lab on a chip devices for automating processes of digital biology.&lt;/p&gt;
&lt;p&gt;The OpenDrop V4 is modular electrowetting controller. The driver board is equipped with a connector that can host a circuit board cartridge with a 14×8 electrode array and 4 reservoirs. The liquids stay on a thin, hydrophobic foil laminated to the circuit board . The device is powered from USB though an included USB-C cable. All the voltage level are generated on the device and can be set with the built in soft menu from 150-300 Volts, DC or AC.&lt;/p&gt;
&lt;p&gt;OpenDrop Cartridges
The modular concepts of the OpenDrop V4 allows different configurations of cartridges: A gold coated electrode array board that can be coated with any dielectric layer and hydrophobic coating to make cartridges for topless digital microfluidic applications using readily available materials.The OpenDrop V4 Cartridge is a close-cell cartridge capable of “move, mix, split and reservoir dispensing”. The 4×8 electrode array and 4 reservoirs are laminated with a 15um ETFE foil, hydrophobic coating and ITO top cover.&lt;/p&gt;
&lt;p&gt;Programming
The OpenDrop V4 can be operated standalone and droplets can be moved through the built in joystick. A control software to program sequences of patterns from a computer is available as a free download. The board is also compatible with Adafruit Feather M0 controller boards and can be reprogrammed through the free Arduino IDE for custom specific applications.  A sample code with the instruction to activate electrodes can be found on the OpenDrop GitHub.&lt;/p&gt;
&lt;p&gt;Features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Modular Cartridge System
- Connector to connect electrode board with up to 128 channels
- Gold coated 14×8 electrodes array, 2.75 mm x 2.75 mm in size, 4mil gaps&lt;/li&gt;
&lt;li&gt;Reservoirs – the new electrode array features 4 CT-type reservoirs&lt;/li&gt;
&lt;li&gt;AC and DC voltage generated on the device form USB power. True AC voltage driving capability (up to 300VAC).&lt;/li&gt;
&lt;li&gt;32bit AVR SAMD21G18 microprocessor with plenty of memory and power
- Electronic settings for voltage level, frequency and AC/DC selection
- Electronic reading of actual voltage level
- One connector for communication and powering (USB-C)
- Optical isolation of the high-voltage electronics trough opto-couplers and PhotoMOS
- New polyphonic audio amplifier and speaker (it’s a synth!)
- Cartridge presence detection
- Feedback amplifier
- Super flat OLED Display
- Nice joystick and 2 buttons, 3 LEDs
- Reset button
- All files open source, designed on KiCAD&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;MSc Urs Gaudenz&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.gaudi.ch/OpenDrop/&#34;&gt;http://www.gaudi.ch/OpenDrop/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=TY97QfWY6J4&#34;&gt;https://www.youtube.com/watch?v=TY97QfWY6J4&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Anonymous&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Stytra</title>
      <link>https://open-neuroscience.com/en/post/stytra/</link>
      <pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/stytra/</guid>
      <description>&lt;p&gt;Stytra, a flexible, open-source software package, written in Python and designed to cover all the general requirements involved in larval zebrafish behavioral experiments.&lt;/p&gt;
&lt;p&gt;It provides timed stimulus presentation, interfacing with external devices and simultaneous real-time tracking of behavioral parameters such as position, orientation, tail and eye motion in both freely-swimming and head-restrained preparations.&lt;/p&gt;
&lt;p&gt;Stytra logs all recorded quantities, metadata, and code version in standardized formats to allow full provenance tracking, from data acquisition through analysis to publication.&lt;/p&gt;
&lt;p&gt;The package is modular and expandable for different experimental protocols and setups. We also provide complete documentation with examples for extending the package to new stimuli and hardware, as well as a schema and parts list for behavioural setups.&lt;/p&gt;
&lt;p&gt;The software can be used in the context of calcium imaging experiments by interfacing with other acquisition devices.&lt;/p&gt;
&lt;p&gt;Our aims are to enable more laboratories to easily implement behavioral experiments, as well as to provide a platform for sharing stimulus protocols that permits easy reproduction of experiments and straightforward validation.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Vilim Stih; Luigi Petrucco&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/portugueslab/stytra&#34;&gt;https://github.com/portugueslab/stytra&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Anonymous&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>An Open-source Anthropomorphic Robot Hand System: HRI Hand</title>
      <link>https://open-neuroscience.com/en/post/an_open-source_anthropomorphic_robot_hand_system_hri_hand/</link>
      <pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/an_open-source_anthropomorphic_robot_hand_system_hri_hand/</guid>
      <description>&lt;p&gt;We present an open-source anthropomorphic robot hand system called HRI hand. Our robot hand system was developed with a focus on the end-effector role of the collaborative robot manipulator. HRI hand is a research platform that can be built at a lower price (approximately $500, using only 3D printing) than commercial end-effectors. Moreover, it was designed as a two four-bar linkage for the under-actuated mechanism and provides pre-shaping motion similar to the human hand prior to touching an object. A URDF, python node, and rviz package is also provided to support the Robot Operating System (ROS). All hardware CAD design files and software source codes have been released and can be easily assembled and modified. The system proposed in this paper is developed with a five-finger structure, but each finger is modularized, so it can be developed with end-effectors of various shapes depending on the shape of the palm.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Hyeonjun Park; Donghan Kim&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/MrLacuqer/HRI-hand-firmware.git&#34;&gt;https://github.com/MrLacuqer/HRI-hand-firmware.git&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/c5Ry3tl9FVw&#34;&gt;https://youtu.be/c5Ry3tl9FVw&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Andre M Chagas&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Open Source Automated Western Blot Processor</title>
      <link>https://open-neuroscience.com/en/post/open_source_automated_western_blot_processor/</link>
      <pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/open_source_automated_western_blot_processor/</guid>
      <description>&lt;p&gt;Researchers in the biomedical area are always involved in methodologies comprising several processes that are repetitive and time-consuming; these researchers can take advantage of this time for other more important things.&lt;/p&gt;
&lt;p&gt;For many years, the trend for this type of problem has been automation. One of the routine methodologies used by researchers in broad areas of basic investigation is the Western blot technique.&lt;/p&gt;
&lt;p&gt;This method allows the detection through specific antibodies and the eventual quantification of the protein of interest in different biological lysates transferred onto a suitable membrane. This methodology involves several repetitive processes; one of them is the washing of blots after incubations with primary and secondary antibodies.&lt;/p&gt;
&lt;p&gt;The present device has been designed to automate this process at a low cost. Researchers must use several tools to carry out the same task at a much higher price, and more importantly, in a time-consuming process. Although it is designed for the Western blot, it can be optimized for other cyclic tasks.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Jorge Bravo-Martinez&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://dx.doi.org/10.17632/xcvckyc9mh.1&#34;&gt;http://dx.doi.org/10.17632/xcvckyc9mh.1&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Jorge Bravo-Martinez&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>OpenNeuro</title>
      <link>https://open-neuroscience.com/en/post/openneuro/</link>
      <pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/openneuro/</guid>
      <description>&lt;p&gt;A free and open platform for sharing MRI, MEG, EEG, iEEG, and ECoG data.&lt;/p&gt;
&lt;p&gt;With OpenNeuro, you can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Browse and explore public datasets and analyses from a wide range of global contributors. Our collection of public datasets continues to grow as more and more become BIDS compatible.&lt;/li&gt;
&lt;li&gt;Download and use public data to create new datasets and run your own analyses.&lt;/li&gt;
&lt;li&gt;Privately share your data so your colleagues can view and edit your work.&lt;/li&gt;
&lt;li&gt;Publish your dataset where anyone can view, download, and run analyses on it.&lt;/li&gt;
&lt;li&gt;Create snapshots of your datasets to ensure past analyses remain reproducible as your datasets grow and change. Publish any of your snapshots while you continue work on your original data behind the scenes.&lt;/li&gt;
&lt;li&gt;Explore your published OpenNeuro dataset using BrainLife&amp;rsquo;s computing network. Utilize their community driven apps to run a variety of analysis and processing software in the browser.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Russell A. Poldrack; Krzysztof Jacek Gorgolewski&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://openneuro.org/&#34;&gt;https://openneuro.org/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=FK_c1x1Pilk&#34;&gt;https://www.youtube.com/watch?v=FK_c1x1Pilk&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Elizabeth DuPre&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Small cost efficient 3D printed peristaltic pumps</title>
      <link>https://open-neuroscience.com/en/post/small_cost_efficient_3d_printed_peristaltic_pumps/</link>
      <pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/small_cost_efficient_3d_printed_peristaltic_pumps/</guid>
      <description>&lt;p&gt;The project overall aim is to provide cost efficient solution to drive microfluidics systems for e.g. cell culture and organ on a chip applications. Pumps, valves and other accessories are ofter expensive to buy or very expensive to custom made. The 8-channel FAST pump is a 3D printed pump that uses some off the shelf parts (steel pins and ball bearings) and is easily fabricated and assembled. A step by step protocol is published (&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2468067220300249)&#34;&gt;https://www.sciencedirect.com/science/article/pii/S2468067220300249)&lt;/a&gt;. The link to the picture is from this publication. The pump is far cheaper and smaller  than commercial pumps and still has at least as good as or better pump performance. The 8-channel pump is excellent for use in parallel cell culture applications.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Alexander Jönsson; Arianna Toppi; Martin Dufva&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2468067220300249&#34;&gt;https://www.sciencedirect.com/science/article/pii/S2468067220300249&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Martin Dufva&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Stackable titre plates for organ on a chip applications</title>
      <link>https://open-neuroscience.com/en/post/stackable_titre_plates_for_organ_on_a_chip_applications/</link>
      <pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/stackable_titre_plates_for_organ_on_a_chip_applications/</guid>
      <description>&lt;p&gt;Organ on a chip is typically difficult to achieve due to large technical challenges such as fabrication of chips and systems and biological challenges such as co-culture of cells.  In this project we have developed a system to stack 12 well plates inserts on top of each other where each plate holds a tissue. We illustrated this approach by creating an intestine model in the top plate, a blood vessel model in the  middle plate and a liver model in the lower plate. The respective insert contain specific cell types are developed independently and just before use, the plates are stacked on top of each other where in this case, the complete assembly models the first pass metabolism. The independent development  circumvent long term co-culture and medium incompatibilities.  The plates are 3D printed in a biocompatible resin and modified with a disc of gelatine where cell are cultured either on top (epithelial and endothelial cells) or inside (hepatocytes). Hence there is diffusional communication from intestine to the lived provided that studies compounds can penetrate the respective barrier. The simple to use system can be modified with any cell type including stem cell organoids and likely neuronal cells.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Martin Dufva; Morten Jepsen; Anja Boisen; Line Hagner Nielsen; Chiara Mazzoni; Andreas Willumsen&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/adbi.201900289&#34;&gt;https://onlinelibrary.wiley.com/doi/abs/10.1002/adbi.201900289&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Martin Dufva&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>cellfinder</title>
      <link>https://open-neuroscience.com/en/post/cellfinder/</link>
      <pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/cellfinder/</guid>
      <description>&lt;p&gt;cellfinder is software from the Margrie Lab at the Sainsbury Wellcome Centre for automated 3D cell detection and registration of whole-brain images (e.g. serial two-photon or lightsheet imaging).&lt;/p&gt;
&lt;p&gt;It’s a work in progress, but cellfinder can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detect labelled cells in 3D in whole-brain images (many hundreds of GB)&lt;/li&gt;
&lt;li&gt;Register the image to an atlas (such as the Allen Mouse Brain Atlas)&lt;/li&gt;
&lt;li&gt;Segment the brain based on the reference atlas&lt;/li&gt;
&lt;li&gt;Calculate the volume of each brain area, and the number of labelled cells within it&lt;/li&gt;
&lt;li&gt;Transform everything into standard space for analysis and visualisation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Adam Tyson&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/SainsburyWellcomeCentre/cellfinder&#34;&gt;https://github.com/SainsburyWellcomeCentre/cellfinder&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Adam Tyson&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>DeepLabCut</title>
      <link>https://open-neuroscience.com/en/post/deeplabcut/</link>
      <pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/deeplabcut/</guid>
      <description>&lt;p&gt;DeepLabCut™ is an efficient method for 3D markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results (i.e. you can match human labeling accuracy) with minimal training data (typically 50-200 frames). We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors.&lt;/p&gt;
&lt;p&gt;The package is open source, fast, robust, and can be used to compute 3D pose estimates. Please see the original paper and the latest work below.  This package is collaboratively developed by the Mathis Group &amp;amp; Mathis Lab at EPFL/Harvard.&lt;/p&gt;
&lt;p&gt;The code is freely available and easy to install in a few clicks with Anaconda (and pypi). Please see instructions on deeplabcut.org. We also provide a very easy to use GUI interface, and a step-by-step user guide!&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Mackenzie Mathis, Alexander Mathis &amp;amp; contributors&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://deeplabcut.org/&#34;&gt;http://deeplabcut.org/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC2HEbWpC_1v6i9RnDMy-dfA&#34;&gt;https://www.youtube.com/channel/UC2HEbWpC_1v6i9RnDMy-dfA&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Mackenzie Mathis&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>MNE-Python</title>
      <link>https://open-neuroscience.com/en/post/mne-python/</link>
      <pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/mne-python/</guid>
      <description>&lt;p&gt;MNE is a software package for processing electrophysiological signals primarily from magnetoencephalographic (MEG) and electroencephalographic (EEG) recordings, and more recently sEEG, ECoG and fNIRS. It provides a comprehensive solution for data preprocessing, forward modeling (with boundary element models), distributed source imaging, time–frequency analysis, non-parametric multivariate statistics, multivariate pattern analysis, and connectivity estimation. Importantly, this package allows all of these analyses to be applied in both sensor or source space. MNE is developed by an international team, with particular care for computational efficiency, code quality, and readability, as well as the common goal of facilitating reproducibility in neuroscience.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Alexandre Gramfort;Eric Larson;Denis Engemann;Daniel Strohmeier;Christian Brodbeck;Roman Goj;Mainak Jas;Teon Brooks;Lauri Parkkonen;Matti Hämäläinen;Jaakko Leppakangas;Jona Sassenhagen;Jean-Rémi King;Daniel McCloy;Marijn van Vliet;Clemens Brunner;Chris Holdgraf;Martin Luessi;Joan Massich;Guillaume Favelier;Andrew R. Dykstra;Mikolaj Magnuski;Stefan Appelhoff;Britta Westner;Richard Höchenberger;Robert Luke;Luke Bloy;Thomas Hartmann;Olaf Hauk;Adam Li&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://mne.tools&#34;&gt;https://mne.tools&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Anonymous&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Nilearn</title>
      <link>https://open-neuroscience.com/en/post/nilearn/</link>
      <pubDate>Sat, 11 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/nilearn/</guid>
      <description>&lt;p&gt;Nilearn is a Python module for fast and easy statistical learning on NeuroImaging data. It leverages the scikit-learn Python toolbox for multivariate statistics with applications such as predictive modelling, classification, decoding, or connectivity analysis.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/orgs/nilearn/people&#34;&gt;https://github.com/orgs/nilearn/people&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://nilearn.github.io/&#34;&gt;http://nilearn.github.io/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Anonymous&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>A Cartesian Coordinate Robot for Dispensing Fruit Fly Food</title>
      <link>https://open-neuroscience.com/en/post/a_cartesian_coordinate_robot_for_dispensing_fruit_fly_food/</link>
      <pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/a_cartesian_coordinate_robot_for_dispensing_fruit_fly_food/</guid>
      <description>&lt;p&gt;The fruit fly, Drosophila melanogaster, continues to be one of the most widely used model organisms in biomedical research.&lt;/p&gt;
&lt;p&gt;Though chosen for its ease of husbandry, maintaining large numbers of stocks of fruit flies, as done by many laboratories, is labour-intensive.&lt;/p&gt;
&lt;p&gt;One task which lends itself to automation is the production of the vials of food in which the flies are reared. Fly facilities typically have to generate several thousand vials of fly food each week to sustain their fly stocks.&lt;/p&gt;
&lt;p&gt;The system presented here combines a cartesian coordinate robot with a peristaltic pump. The design of the robot is based on an open hardware CNC (computer numerical control) machine, and uses belt and pulley actuators for the X and Y axes, and a leadscrew actuator for the Z axis.&lt;/p&gt;
&lt;p&gt;CNC motion and operation of the peristaltic pump are controlled by grbl (&lt;a href=&#34;https://github.com/gnea/grbl),&#34;&gt;https://github.com/gnea/grbl),&lt;/a&gt; an open source, embedded, G-code parser. Grbl is written in optimized C and runs directly on an Arduino. A Raspberry Pi is used to generate and stream G-code instructions to Grbl.&lt;/p&gt;
&lt;p&gt;A touch screen on the Raspberry Pi provides a graphical user interface to the system. Whilst the robot was built for the express purpose of filling vials of fly food, it could potentially be used for other liquid handling tasks in the laboratory.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Matt Wayland; Matthias Landgraf&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/WaylandM/fly-food-robot&#34;&gt;https://github.com/WaylandM/fly-food-robot&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.6084/m9.figshare.5175223.v1&#34;&gt;https://doi.org/10.6084/m9.figshare.5175223.v1&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matt Wayland&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Bonsai</title>
      <link>https://open-neuroscience.com/en/post/bonsai/</link>
      <pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/bonsai/</guid>
      <description>&lt;p&gt;Bonsai is a high-performance, easy to use, and flexible visual programming language for designing closed-loop neuroscience experiments combining physiology and behaviour data.&lt;/p&gt;
&lt;p&gt;Bonsai has allowed scientists with no previous programming experience to quickly develop their own experimental rigs and is also being increasingly used as a platform to integrate new open-source hardware and software from the experimental neuroscience community.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Gonçalo Lopes&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://bonsai-rx.org/&#34;&gt;https://bonsai-rx.org/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Gonçalo Lopes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Ethoscopes</title>
      <link>https://open-neuroscience.com/en/post/ethoscopes/</link>
      <pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/ethoscopes/</guid>
      <description>&lt;p&gt;Ethoscopes are machines for high-throughput analysis of behavior in Drosophila and other animals.&lt;/p&gt;
&lt;p&gt;Ethoscopes provide a software and hardware solution that is reproducible and easily scalable.&lt;/p&gt;
&lt;p&gt;They perform, in real-time, tracking and profiling of behavior by using a supervised machine learning algorithm, are able to deliver behaviorally triggered stimuli to flies in a feedback-loop mode, and are highly customizable and open source.&lt;/p&gt;
&lt;p&gt;Ethoscopes can be built easily by using 3D printing technology and rely on Raspberry Pi microcomputers and Arduino boards to provide affordable and flexible hardware.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Quentin Geissmann; Luis Garcia; Giorgio Gilestro&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://lab.gilest.ro/ethoscope&#34;&gt;http://lab.gilest.ro/ethoscope&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title&#34;&gt;https://www.youtube.com/watch?v=5oWGBUMJON8&amp;amp;feature=emb_title&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Giorgio Gilestro&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Bonvision</title>
      <link>https://open-neuroscience.com/en/post/bonvision/</link>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/bonvision/</guid>
      <description>&lt;p&gt;BonVision is an open-source closed-loop visual environment generator developed by the Saleem Lab and Solomon Lab at the UCL Institute of Behavioural Neuroscience in collaboration with NeuroGEARS.&lt;/p&gt;
&lt;p&gt;BonVision’s key features include:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Naturally closed-loop system based on reactive coding of the Bonsai framework
Handles 2D and 3D stimuli with equal ease
Visual environment generated independent of display configuration
Graphical programming language of the Bonsai framework
Can be used for Augmented Reality, Virtual Reality or 2D visual stimuli
Does not require the observer to be in a fixed position
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Bonvision&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://bonvision.github.io&#34;&gt;http://bonvision.github.io&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Poseidon</title>
      <link>https://open-neuroscience.com/en/post/poseidon/</link>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/poseidon/</guid>
      <description>&lt;p&gt;The Poseidon is an open-source syringe pump and microscope system. It uses 3D printed parts and common components that can be easily purchased. It can be used in microfluidics experiments or other applications. You can assemble it in a short-time for under $400. The system is modular and highly customizable. Examples of applications are: control the chemical environment of a bioreactor, purify proteins and precisely add reagents to chemical reactions over time.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Pachter Lab&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://pachterlab.github.io/poseidon&#34;&gt;https://pachterlab.github.io/poseidon&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Miguel Fernandes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>DeepLabStream</title>
      <link>https://open-neuroscience.com/en/post/deeplabstream/</link>
      <pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/deeplabstream/</guid>
      <description>&lt;p&gt;DeepLabStream is a python based multi-purpose tool that enables the realtime tracking of animals and manipulation of experiments. Our toolbox is adapted from the previously published DeepLabCut (Mathis et al., 2018) and expands on its core capabilities. DeepLabStreams core feature is the real-time analysis using any type of camera-based video stream (incl. multiple streams). Building onto that, we designed a full experimental closed-loop toolkit. It enables running experimental protocols that are dependent on a constant stream of bodypart positions and feedback activation of several input/output devices. It&amp;rsquo;s capabilities range from simple region of interest (ROI) based triggers to headdirection or behavior dependent stimulation.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Schwarz Neurocon Lab&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/SchwarzNeuroconLab/DeepLabStream&#34;&gt;https://github.com/SchwarzNeuroconLab/DeepLabStream&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>LED Zappelin&#39;</title>
      <link>https://open-neuroscience.com/en/post/led_zappelin/</link>
      <pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/led_zappelin/</guid>
      <description>&lt;p&gt;Two-photon (2P) microscopy is a cornerstone technique in neuroscience research. However, combining 2P imaging with spectrally arbitrary light stimulation can be challenging due to crosstalk between stimulation light and fluorescence detection. To overcome this limitation, we present a simple and low-cost electronic solution based on an ESP32 microcontroller and a TLC5947 LED driver to rapidly time-interleave stimulation and detection epochs during scans. Implemented for less than $100, our design can independently drive up to 24 arbitrary spectrum LEDs to meet user requirements. We demonstrate the utility of our stimulator for colour vision experiments on the in vivo tetrachromatic zebrafish retina and for optogenetic circuit mapping in Drosophila.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Maxime Zimmermann; Andre Maia Chagas; Philipp Bartel; Sinzi Pop, Lucia Pierto Godino; Tom Baden&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/BadenLab/LED-Zappelin&#34;&gt;https://github.com/BadenLab/LED-Zappelin&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Maxime Zimmermann&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>SpikeInterface</title>
      <link>https://open-neuroscience.com/en/post/spikeinterface/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/spikeinterface/</guid>
      <description>&lt;p&gt;SpikeInterface is a unified Python framework for spike sorting. With its high-level API, it is designed to be accessible and easy to use, allowing users to build full analysis pipelines for spike sorting (reading-writing (IO) / preprocessing / spike sorting / postprocessing / validation / curation / comparison / visualization) with a few lines of code.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Alessio Buccino*; Cole Hurwitz*; Samuel Garcia; Jeremy Magland; Josh Siegle; Matthias Hennig&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/SpikeInterface/spikeinterface&#34;&gt;https://github.com/SpikeInterface/spikeinterface&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=nWJGwFB7oII&#34;&gt;https://www.youtube.com/watch?v=nWJGwFB7oII&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Alessio Buccino&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Addgene&#39;s AAV Data Hub</title>
      <link>https://open-neuroscience.com/en/post/addgenes_aav_data_hub/</link>
      <pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/addgenes_aav_data_hub/</guid>
      <description>&lt;p&gt;AAV are versatile tools used by neuroscientists for expression and manipulation of neurons. Many scientists have benefited from the high-quality, ready-to-use AAV prep service from Addgene, a nonprofit plasmid repository. However, it can be challenging to determine which AAV tool and techniques are best to use for an experiment. Scientists also may have questions about how much virus to inject or which serotype or promoter should be used to target the desired neuron or brain region. To help scientists answer these questions, Addgene launched an open platform called the AAV Data Hub (&lt;a href=&#34;https://datahub.addgene.org/aav/&#34;&gt;https://datahub.addgene.org/aav/&lt;/a&gt;) which allows researchers to easily share practical experimental details with the scientific community (AAV used, in vivo model used, injection site, injection volumes, etc.). The goal of this platform is to help scientists find the best AAV tool for their experiments by reviewing combined data from a broad range of research labs. The AAV Data Hub launched in late 2019 and over 100 experiments have since been contributed to this project. The dataset includes details and images from experiments conducted in six different species and several different expression sites.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Addgene&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://datahub.addgene.org/aav/&#34;&gt;https://datahub.addgene.org/aav/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ZPKdr1RdtGI&amp;amp;feature=youtu.be&#34;&gt;https://www.youtube.com/watch?v=ZPKdr1RdtGI&amp;amp;feature=youtu.be&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Angela Abitua&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>FishCam</title>
      <link>https://open-neuroscience.com/en/post/fishcam/</link>
      <pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/fishcam/</guid>
      <description>&lt;p&gt;We describe the “FishCam”, a low-cost (500 USD) autonomous camera package to record videos and images underwater. The system is composed of easily accessible components and can be programmed to turn ON and OFF on customizable schedules. Its 8-megapixel camera module is capable of taking 3280 × 2464-pixel images and videos. An optional buzzer circuit inside the pressure housing allows synchronization of the video data from the FishCam with passive acoustic recorders. Ten FishCam deployments were performed along the east coast of Vancouver Island, British Columbia, Canada, from January to December 2019. Field tests demonstrate that the proposed system can record up to 212 h of video data over a period of at least 14 days. The FishCam data collected allowed us to identify fish species and observe species interactions and behaviors. The FishCam is an operational, easily-reproduced and inexpensive camera system that can help expand both the temporal and spatial coverage of underwater observations in ecological research. With its low cost and simple design, it has the potential to be integrated into educational and citizen science projects, and to facilitate learning the basics of electronics and programming.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Xavier Mouy&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2468067220300195&#34;&gt;https://www.sciencedirect.com/science/article/pii/S2468067220300195&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>neuTube</title>
      <link>https://open-neuroscience.com/en/post/neutube/</link>
      <pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/neutube/</guid>
      <description>&lt;p&gt;neuTube is an open source software for reconstructing neurons from fluorescence microscope images. It is easy to use and improves the efficiency of reconstructing neuron structures accurately. The framework combines 2D/3D visualization, semi-automated tracing algorithms, and flexible editing options that simplify the task of neuron reconstruction.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Ting Zhao&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.neutracing.com/&#34;&gt;https://www.neutracing.com/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Miguel Fernandes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>OpenTrons</title>
      <link>https://open-neuroscience.com/en/post/opentrons/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/opentrons/</guid>
      <description>&lt;p&gt;Today, biologists spend too much time pipetting by hand. We think biologists should have robots to do pipetting for them. People doing science should be free of tedious benchwork and repetitive stress injuries. They should be able to spend their time designing experiments and analyzing data.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s why we started Opentrons.&lt;/p&gt;
&lt;p&gt;We make robots for biologists. Our mission is to provide the scientific community with a common platform to easily share protocols and reproduce each other&amp;rsquo;s results. Our robots automate experiments that would otherwise be done by hand, allowing our community to spend more time pursuing answers to some of the 21st century’s most important questions&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Opentrons&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://opentrons.com/about&#34;&gt;https://opentrons.com/about&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCvMRmXIxnHs3AutkVhuqaQg&#34;&gt;https://www.youtube.com/channel/UCvMRmXIxnHs3AutkVhuqaQg&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Deep Cinac</title>
      <link>https://open-neuroscience.com/en/post/deep_cinac/</link>
      <pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/deep_cinac/</guid>
      <description>&lt;p&gt;Two-photon calcium imaging is now widely used to infer neuronal dynamics from changes in fluorescence of an indicator. However, state of the art computational tools are not optimized for the reliable detection of fluorescence transients from highly synchronous neurons located in densely packed regions such as the CA1 pyramidal layer of the hippocampus during early postnatal  stages  of  development.  Indeed,the  latest  analytical  tools  often  lack  proper benchmark  measurements.  To  meet  this  challenge,  we  first  developed  a  graphical  user interface allowing for a precise manual detection of all calcium transients from imaged neurons based on the visualization of the calcium imaging movie. Then, we analyzed the movies using a convolutional neural network with an attention process and a bidirectional long-short term memory network. This method is able to reach human performance and offers a better F1 score (harmonic mean of sensitivity and precision) than CaImAn to infer neural activity in the developingCA1 without any user intervention. It also enables automatically identifying activity originating from GABAergic neurons. Overall, DeepCINAC offers a simple, fast and flexible open-source toolbox for processing a wide variety of calcium imaging datasets while providing the tools to evaluate its performance.&lt;/p&gt;
&lt;p&gt;See full text at &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/803726v2.full.pdf&#34;&gt;https://www.biorxiv.org/content/10.1101/803726v2.full.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Julien Denis&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://gitlab.com/cossartlab/deepcinac&#34;&gt;https://gitlab.com/cossartlab/deepcinac&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>SciDraw</title>
      <link>https://open-neuroscience.com/en/post/scidraw/</link>
      <pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/scidraw/</guid>
      <description>&lt;p&gt;SciDraw is a free repository of high quality drawings of animals, scientific setups, and anything that might be useful for scientific presentations and posters.
We want this repository to be as open as possible, so do not require signing up to post a drawing. This however means that you won&amp;rsquo;t be able to edit your drawings after submission, so upload carefully!
The drawings on SciDraw are made by and for scientists. You are free to download, modify and use all drawings on the website.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Federico Claudi and Alex Harston&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://scidraw.io/&#34;&gt;https://scidraw.io/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>UC2</title>
      <link>https://open-neuroscience.com/en/post/uc2/</link>
      <pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/uc2/</guid>
      <description>&lt;p&gt;The open-source optical toolbox UC2 [YouSeeToo] simplifies the process of building optical setups, by combining 3D-printed cubes, each holding a specific component (e.g. lens, mirror) on a magnetic square-grid baseplate. The use of widely available consumables and 3D printing, together with documentation and software, offers an extremely low-cost and accessible alternative for both education and research areas. In order to reduce the entry barrier, we provide a fully comprehensive toolbox called TheBOX. A paper describing the scientific application in detail can be found 
&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2020.03.02.973073v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Benedict Diederich; René Lachmann; Barbora Marsikova&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://useetoo.org&#34;&gt;https://useetoo.org&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ey4uEFEG6MY&#34;&gt;https://www.youtube.com/watch?v=ey4uEFEG6MY&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Barbora Marsikova&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Head-Mounted Mesoscope</title>
      <link>https://open-neuroscience.com/en/post/head-mounted_mesoscope/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/head-mounted_mesoscope/</guid>
      <description>&lt;p&gt;The  advent  of  genetically encoded  calcium  indicators,  along  with  surgical  preparations  such  as thinned skulls or refractive index matched skulls, have enabled mesoscale cortical activity imaging in head-fixed mice. Such imaging studies have revealed complex patterns of coordinated activity across  the cortex during  spontaneous  behaviors,  goal-directed  behavior,  locomotion,  motor learning,and perceptual decision making. However, neural activity during unrestrained behavior significantly  differs from neural  activity in head-fixed  animals. Whole-cortex  imaging  in  freely behaving  mice  will  enable  the  study  of  neural  activity  in  a  larger,  more  complex  repertoire  of behaviors  not  possible  in  head-fixed  animals. Here  we present  the “Mesoscope,”  a  wide-field miniaturized, head-mounted fluorescence microscope compatible with transparent polymer skulls recently developed by our group. With afield of view of 8 mm x 10 mm and weighing less than 4 g, the Mesoscope can image most of the mouse dorsal cortex with resolution ranging from 39 to 56μm. Stroboscopic illumination with blue and green LEDs allows fort he measurement of both fluorescence changes  due  to calcium  activity  and  reflectance  signals to  capture hemodynamic changes. We have used the Mesoscope to successfully record mesoscale calcium activity across the dorsal cortex during sensory-evoked stimuli, open field behaviors, and social interactions.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Biosensing and Biorobotics Lab&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2020.05.25.114892v1.full.pdf&#34;&gt;https://www.biorxiv.org/content/10.1101/2020.05.25.114892v1.full.pdf&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Open Source Eye Tracking</title>
      <link>https://open-neuroscience.com/en/post/open_source_eye_tracking/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/open_source_eye_tracking/</guid>
      <description>&lt;p&gt;The purpose of this project is to convey a location in 3 dimensional space to a machine, hands free and in real time.&lt;/p&gt;
&lt;p&gt;Currently it is very difficult to control machines without making the user provide input with their hands. Additionally it can be very difficult to specify a location in space without a complex input device. This system provides a novel solution to this problem by allowing the user to specify a location simply by looking at it.&lt;/p&gt;
&lt;p&gt;Normally eyetracking solutions are prohibitively expensive and not open source, limiting their use for creators to integrate them into new projects. This solution is fully open source, easy to build and will provide a huge variety of options for makers interested in using this fascinating and powerful technology.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;John Evans&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://hackaday.io/project/153293-low-cost-open-source-eye-tracking&#34;&gt;https://hackaday.io/project/153293-low-cost-open-source-eye-tracking&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Open Source Syringe Pump Controller</title>
      <link>https://open-neuroscience.com/en/post/open_source_syringe_pump_controller/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/open_source_syringe_pump_controller/</guid>
      <description>&lt;p&gt;Syringe pumps are a necessary piece of laboratory equipment that are used for fluid delivery in behavioral neuroscience laboratories. Many experiments provide rodents and primates with fluid rewards such as juice, water, or liquid sucrose. Current commercialized syringe pumps are not customizable and do not have the ability to deliver multiple volumes of fluid based on different inputs to the pump. Additionally, many syringe pumps are expensive and cannot be used in experiments with paired neurophysiological recordings due to electrical noise. We developed an open source syringe pump controller using commonly available parts. The controller adjusts the acceleration and speed of the motor to deliver three different volumes of fluid reward within one common time epoch. This syringe pump controller is cost effective and has been successfully implemented in rodent behavioral experiments with paired neurophysiological recordings in the rat frontal cortex while rats lick for different volumes of liquid sucrose rewards. Our syringe pump controller will enable new experiments to address the potential confound of temporal information in studies of reward signaling by fluid magnitude.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Laubach Lab&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/LaubachLab/OpenSourceSyringePump&#34;&gt;https://github.com/LaubachLab/OpenSourceSyringePump&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Sample Rotator Mixer and Shaker</title>
      <link>https://open-neuroscience.com/en/post/sample_rotator_mixer_and_shaker/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/sample_rotator_mixer_and_shaker/</guid>
      <description>&lt;p&gt;An open-source 3-D printable laboratory sample rotator mixer is developed here in two variants that allow users to opt for the level of functionality, cost saving and associated complexity needed in their laboratories. First, a laboratory sample rotator is designed and demonstrated that can be used for tumbling as well as gentle mixing of samples in a variety of tube sizes by mixing them horizontally, vertically, or any position in between. Changing the mixing angle is fast and convenient and requires no tools. This device is battery powered and can be easily transported to operate in various locations in a lab including desktops, benches, clean hoods, chemical hoods, cold rooms, glove boxes, incubators or biological hoods. Second, an on-board Arduino-based microcontroller is incorporated that adds the functionality of a laboratory sample shaker. These devices can be customized both mechanically and functionally as the user can simply select the operation mode on the switch or alter the code to perform custom experiments. The open source laboratory sample rotator mixer can be built by non-specialists for under US$30 and adding shaking functionality can be done for under $20 more. Thus, these open source devices are technically superior to the proprietary commercial equipment available on the market while saving over 90% of the costs.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;MOST&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.appropedia.org/Open_Source_Laboratory_Sample_Rotator_Mixer_and_Shaker&#34;&gt;https://www.appropedia.org/Open_Source_Laboratory_Sample_Rotator_Mixer_and_Shaker&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Ta2ACV1oIjI&amp;amp;feature=emb_logo&#34;&gt;https://www.youtube.com/watch?v=Ta2ACV1oIjI&amp;amp;feature=emb_logo&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Automated Operant Conditioning</title>
      <link>https://open-neuroscience.com/en/post/automated_operant_conditioning/</link>
      <pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/automated_operant_conditioning/</guid>
      <description>&lt;p&gt;Operant conditioning (OC) is a classical paradigm and a standard technique used in experimental psychology in which animals learn to perform an action to achieve a reward. By using this paradigm, it is possible to extract learning curves and measure accurately reaction times (RTs). Both these measurements are proxy of cognitive capabilities and can be used to evaluate the effectiveness of therapeutic interventions in mouse models of disease. Here, we describe a fully 3D printable device that is able to perform OC on freely moving mice, while performing real-time tracking of the animal position. We successfully trained six mice, showing stereotyped learning curves that are highly reproducible across mice and reaching &amp;gt;70% of accuracy after 2 d of conditioning. Different products for OC are commercially available, though most of them do not provide customizable features and are relatively expensive. This data demonstrate that this system is a valuable alternative to available state-of-the-art commercial devices, representing a good balance between performance, cost, and versatility in its use.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Raffaele Mazziotti, Giulia Sagona, Leonardo Lupori, Virginia Martini and Tommaso Pizzorusso&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/raffaelemazziotti/oc_chamber&#34;&gt;https://github.com/raffaelemazziotti/oc_chamber&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Heuristic Spike Sorting Tuner (HSST), a framework to determine optimal parameter selection for a generic spike sorting algorithm</title>
      <link>https://open-neuroscience.com/en/post/heuristic_spike_sorting_tuner_hsst_a_framework_to_determine_optimal_parameter_selection_for_a_generic_spike_sorting_algorithm/</link>
      <pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/heuristic_spike_sorting_tuner_hsst_a_framework_to_determine_optimal_parameter_selection_for_a_generic_spike_sorting_algorithm/</guid>
      <description>&lt;p&gt;Extracellular microelectrodes frequently record neural activity from more than one neuron in the vicinity of the electrode. The process of labeling each recorded spike waveform with the identity of its source neuron is called spike sorting and is often approached from an abstracted statistical perspective. However, these approaches do not consider neurophysiological realities and may ignore important features that could improve the accuracy of these methods. Further, standard algorithms typically require selection of at least one free parameter, which can have significant effects on the quality of the output. We describe a Heuristic Spike Sorting Tuner (HSST) that determines the optimal choice of the free parameters for a given spike sorting algorithm based on the neurophysiological qualification of unit isolation and signal discrimination. A set of heuristic metrics are used to score the output of a spike sorting algorithm over a range of free parameters resulting in optimal sorting quality. We demonstrate that these metrics can be used to tune parameters in several spike sorting algorithms. The HSST algorithm shows robustness to variations in signal to noise ratio, number and relative size of units per channel. Moreover, the HSST algorithm is computationally efficient, operates unsupervised, and is parallelizable for batch processing.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;David A. Bjanes; Lee B. Fisher; Robert A. Gaunt; Douglas J. Weber&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/davidbjanes/hsst&#34;&gt;https://github.com/davidbjanes/hsst&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
David Bjanes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>VocalMat</title>
      <link>https://open-neuroscience.com/en/post/vocalmat/</link>
      <pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/vocalmat/</guid>
      <description>&lt;p&gt;Mice emit ultrasonic vocalizations (USV) to transmit socially-relevant information. To detect and classify these USVs, here we describe the development of VocalMat. VocalMat is a software that uses image-processing and differential geometry approaches to detect USVs in audio files, eliminating the need for user-defined parameter tuning. VocalMat also uses computational vision and machine learning methods to classify USVs into distinct categories. In a dataset of &amp;gt;4,000 USVs emitted by mice, VocalMat detected more than &amp;gt;98% of the USVs and accurately classified ≈86% of USVs when considering the most likely label out of 11 different USV types. We then used Diffusion Maps and Manifold Alignment to analyze the probability distribution of USV classification among different experimental groups, providing a robust method to quantify and qualify the vocal repertoire of mice. Thus, VocalMat allows accurate and highly quantitative analysis of USVs, opening the opportunity for detailed and high-throughput analysis of this behavior.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Antonio H. O. Fonseca, Gustavo M. Santana, Sergio Bampi, Marcelo O Dietrich&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.dietrich-lab.org/vocalmat&#34;&gt;https://www.dietrich-lab.org/vocalmat&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>BossDB</title>
      <link>https://open-neuroscience.com/en/post/bossdb/</link>
      <pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/bossdb/</guid>
      <description>&lt;p&gt;BossDB is a volumetric database that lives in the AWS cloud. Hundreds of terabytes of electron microscopy, light microscopy, and x-ray tomography data are available for free download and study.&lt;/p&gt;
&lt;p&gt;Have a project you want to share with the world for free? Get in touch!
&lt;a href=&#34;https://twitter.com/thebossdb&#34;&gt;https://twitter.com/thebossdb&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;JHU|APL&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://bossdb.org/&#34;&gt;https://bossdb.org/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Jordan Matelsky&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Neuroanatomy and Behaviour</title>
      <link>https://open-neuroscience.com/en/post/neuroanatomy_and_behaviour/</link>
      <pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/neuroanatomy_and_behaviour/</guid>
      <description>&lt;p&gt;Neuroanatomy and Behaviour (ISSN: 2652-1768) is a free open access journal for behavioural neuroscience and related fields. Powered by free open source software to eliminate costs and keep grant funds doing science. Scientist-run and non-profit.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Episteme Health Inc.&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://epistemehealth.com&#34;&gt;https://epistemehealth.com&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Shaun Khoo&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>3D Slicer</title>
      <link>https://open-neuroscience.com/en/post/3d_slicer/</link>
      <pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/3d_slicer/</guid>
      <description>&lt;p&gt;3D Slicer is a software for medical image informatics, image processing, and three-dimensional visualization. It’s extremely powerful and versatile with plenty of different options. It is a great tool for volume rendering, registration, interactive segmentation of images and even offers the possibility of running Python scripts thought an embedded Python interpreter.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Ron Kikinis&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Slicer/Slicer&#34;&gt;https://github.com/Slicer/Slicer&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Miguel Fernandes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Craniobot</title>
      <link>https://open-neuroscience.com/en/post/craniobot/</link>
      <pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/craniobot/</guid>
      <description>&lt;p&gt;The Craniobot is a cranial microsurgery platform that combines automated skull surface profiling with a computer numerical controlled (CNC) milling machine to perform a variety of cranial microsurgical procedures in mice. The Craniobot utilizes a low force contact sensor to profile the skull surface and uses this information to perform micrometer-scale precise milling operations within minutes. The procedure of removing the sub-millimeter thick mouse skull precisely without damaging the underlying brain can be technically challenging and often takes significant skill and practice. This can now be overcome using the Craniobot.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Mathew Rynes, Leila Ghanbari, Micheal Laroque, Greg Johnson, Daniel Sousa Schulman, Suhasa Kodandaramaiah&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.labmaker.org/products/craniobot&#34;&gt;https://www.labmaker.org/products/craniobot&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Labmaker</title>
      <link>https://open-neuroscience.com/en/post/labmaker/</link>
      <pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/labmaker/</guid>
      <description>&lt;p&gt;LabMaker is a maker and assembly service for OPEN SCIENCE instruments. OPEN SCIENCE initiatives provide part lists or &amp;ldquo;Bill Of Materials&amp;rdquo; (BOM) for openly available scientific instruments. LabMaker bridges the gap between the BOM and the ready-to-use instrument for those not wanting to build by themselves. LabMaker is based in Berlin, Germany and ships worldwide. Berlin, as a city, is not only amongst the frontrunners for the title &amp;ldquo;start-up capital of Europe&amp;rdquo;, but also home to a large diversity of companies rooted in traditional precision manufacturing.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Labmaker&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.labmaker.org/&#34;&gt;https://www.labmaker.org/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>MicroscoPy</title>
      <link>https://open-neuroscience.com/en/post/microscopy/</link>
      <pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/microscopy/</guid>
      <description>&lt;p&gt;An open-source, motorized, and modular microscope built using LEGO bricks, Arduino, Raspberry Pi and 3D printing. The microscope uses a Raspberry Pi mini-computer with an 8MP camera to capture images and videos. Stepper motors and the illumination are controlled using a circuit board comprising an Arduino microcontroller, six stepper motor drivers and a high-power LED driver. All functions can be controlled from a keyboard connected to the Raspberry Pi or a separate custom-built Arduino joystick connected to the mainboard. LEGO bricks are used to construct the main body of the microscope to achieve a modular and easy-to-assemble design concept.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Yuksel Temiz and IBM&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/IBM/MicroscoPy&#34;&gt;https://github.com/IBM/MicroscoPy&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-video&#34;&gt;Project Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=PBSYnk9T4o4&amp;amp;feature=youtu.be&#34;&gt;https://www.youtube.com/watch?v=PBSYnk9T4o4&amp;amp;feature=youtu.be&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Matias Andina&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Colaboratory</title>
      <link>https://open-neuroscience.com/en/post/colaboratory/</link>
      <pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/colaboratory/</guid>
      <description>&lt;p&gt;Colaboratory is a free Jupyter notebook environment that runs in the cloud. Your notebooks get stored on Google Drive. The great advantage is that you don’t have to install anything (however, for some features you need a Google account) on your system to use it. You can perform specific computations during data analysis with pre-installed Python libraries and gives you access to accelerated hardware for free (e.g. GPUs and TPUs).&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Google&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/notebooks/intro.ipynb&#34;&gt;https://colab.research.google.com/notebooks/intro.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Miguel Fernandes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Suite2P</title>
      <link>https://open-neuroscience.com/en/post/suite2p/</link>
      <pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate>
      <guid>https://open-neuroscience.com/en/post/suite2p/</guid>
      <description>&lt;p&gt;Suite2P is a very modular imaging processing pipeline written in Python which allows you to perform registration of raw data movies, automatic cell detection, extraction of calcium traces and infers spike times. It is a very fast and accurate tool and can work on standard workstations. It also includes a visualization graphical user interface (GUI) that facilitates analysis and manual curation of the cell detection algorithm.&lt;/p&gt;
&lt;h2 id=&#34;project-authors&#34;&gt;Project Author(s)&lt;/h2&gt;
&lt;p&gt;Carsen Stringer and Marius Pachitariu&lt;/p&gt;
&lt;h2 id=&#34;project-links&#34;&gt;Project Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://mouseland.github.io/suite2p/_build/html/index.html&#34;&gt;https://mouseland.github.io/suite2p/_build/html/index.html&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This post was automatically generated by
Miguel Fernandes&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
  </channel>
</rss>
